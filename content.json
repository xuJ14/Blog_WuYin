{"pages":[],"posts":[{"title":"Machine Learning Week02","text":"Multiple FeatureMultivariate Linear Regression $h_{\\theta}(x)=\\theta^{T}X$ $h_θ(x)=\\begin{bmatrix}θ_0&amp;θ_1&amp;…&amp;θ_n\\end{bmatrix}\\begin{bmatrix}x_0\\x_1\\⋮\\x_n\\end{bmatrix}=θ^Tx$ Gradient Descent for Multiple Variables Cost function: $$J(\\theta_0,\\theta_1…\\theta_n)=\\frac {1}{2m}\\sum \\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$ $$\\theta_j:=\\theta_j-\\alpha\\frac {\\partial} {\\partial \\theta_j}J(\\theta)=\\theta_j-\\alpha\\frac {1}{m}\\sum \\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\quad(for\\ j=0…n)$$ Feature Scaling Make sure features are on a similar scale 避免梯度下降一个维度值过大导致下降速率很慢 Get every feature into approximately a $-1\\le x_i\\le 1$ range 大约在这个范围附近就行，例如: $-2\\le x_i\\le 2$也可以 方法一：都除以最大值$x_{max}$ 均值归一化 Mean normalization： $$\\frac{x_i-\\mu_i}{s_{i}}$$ 使得均值为0 $\\mu$是训练集x某特征的均值，s可以是标准差，一般$s=max-min$即可 特征缩放不用太精确，只是为了让梯度下降更快而已 确定α 如果α过大，则可能出现 J 值增大或者不收敛 让α尽量小，使得每次迭代 J 值都在减小 自动收敛测试：可以设定一个值比如0.001，如果每一步J变化小于这个值即可认为收敛，但是选择合适的值很困难 所以可以采用：代价函数 J 随迭代步数变化曲线，可以帮助判断梯度下降算法是否收敛 多项式回归 Polynomial Regression 拟合复杂函数 将x进行平方、开根等处理。例如：$x_3=x_1^3$ 注意新特征要进行均值归一化使得各个特征之间差距不至于过大 Normal equation 正规方程 solve for θ analytically，θ的解析解法 $$J(\\theta_0,\\theta_1…\\theta_n)=\\frac {1}{2m}\\sum \\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$ ，令$\\frac {\\partial} {\\partial \\theta_j}J(\\theta)=0$(偏微分) $X=\\begin{bmatrix}x_0&amp;x_1&amp;…&amp;x_n\\end{bmatrix}$，x代表特征的列向量集合 则可以推出：$\\theta=(X^TX)^{-1}X^Ty$，即正规方程 matlab：pinv(X‘*X)*X’*y 用正规方程就不用变量归一化 优点：不用选择α；不用迭代 缺点：n较大时，算逆矩阵会比较慢，n&gt;10000，复杂度大约在$O(n^3)$ 正规方程的不可逆性 Normal Equation Non-invertibility pinv可以求伪逆，inv求逆 如果$X^TX$是不可逆的： 特征之间是相关的 特征多于样本数 解决方法：删除多余特征或正则化","link":"/Machine-Learning-Week02/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/hello-world/"},{"title":"This is my first blog!","text":"This is the first time for me to use Hexo to build my blog. My major is Engineering Physics and Financial Engineering. Hope to share my experiences with you! Thank you for following my blog!","link":"/This-is-my-first-bolg/"},{"title":"Machine Learning Week01","text":"Week 1 Supervised Learning 监督学习 分类问题 classification problem discrete output（0，1） features 特征无限多的时候可以用SVM支持向量机 回归问题 regression problem continuous output Unsupervised Learning 无监督学习 clustering algorithm 聚类算法 dataset（no label） find structure;根据数据内部关系分类 E.g. 网站分类；基因分类；social network analysis；market segmentation；Astronomical data analysis Cocktail party problem：混杂的声音中分离出两种声音 1[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x') *SVD: singular value decomposition 奇异值分解，求解线性方程 octave做原型，然后迁移到C或JAVA Model and Cost Function 模型和成本函数 线性回归算法 linear regression training set 训练集 m - 训练样本数 x - 输入变量 y - 输出变量 $(x^{i},y^{i})$ - i training example $\\theta$ - Parameters h - hypothesis，x映射到y的函数 ：$h_{\\theta}(x)=\\theta_0+\\theta_1x$；缩写即$h(x)$ 不一定都是线性方程 Cost Function 成本函数 度量函数拟合的程度 平方误差成本函数：适用于线性回归 Hypothesis: $$\\min \\limits_{\\theta_0\\theta_1}\\frac {1}{2m}\\sum \\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$ Cost function: $$J(\\theta_0,\\theta_1)=\\frac {1}{2m}\\sum \\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$ —– (Squared Error Function) contour plot: 轮廓图；等高线图 Parameter Learning Gradient descent 梯度下降算法 不断改变$\\theta$使得J函数变小，得到局部最优点local optimum，convergence收敛 $$\\theta_j:=\\theta_j-\\alpha\\frac {\\partial} {\\partial \\theta_j}J(\\theta_0,\\theta_j)\\quad(for\\ j=0\\ and\\ j=1)$$ $\\alpha$是学习速率 learning rate 注意编程中的赋值后会覆盖，所以要桥接一下 partial derivatives 偏导数；derivatives 导数 不需要调整α因为导数值越来越小 Batch Gradient Descent 批量梯度下降：每step都用到全部的训练样本 高等线性代数：normal equations method正规方程，大数据时梯度下降更好用","link":"/Machine-Learning-Week01/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Essay","slug":"Essay","link":"/tags/Essay/"}],"categories":[]}