<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Issues of Icarus</title>
    <url>/Issues-of-Icarus/</url>
    <content><![CDATA[<h1 id="upload">Upload</h1>
<ul>
<li>每次上传都需要密码 _config.yml中将仓库地址改为：git@github.com:
yourname/yourname.github.io.git</li>
</ul>
<h1 id="widgets">Widgets</h1>
<h1 id="posts">Posts</h1>
<h2 id="content">Content</h2>
<ul>
<li><p><strong>Image not showing up</strong></p>
<p>Image should be under source/gallery/. Use Hexo's tag:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">{% img /gallery/image.jpg "image title" %\}</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>Excerpt / Read more</p>
<p>Put a &lt;! -- more --&gt;tag in the post.</p>
<p>Or in the front-matter, "excerpt: the content of the
excerpt"</p></li>
<li><p><a href="https://www.icode9.com/content-4-828659.html">Image host
via github</a></p></li>
</ul>
<h2 id="formula">Formula</h2>
<ul>
<li><p>无法正确渲染</p>
<p><a href="https://blog.csdn.net/yexiaohhjk/article/details/82526604?utm_medium=distribute.pc_relevant_bbs_down.none-task-blog-baidujs-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_bbs_down.none-task-blog-baidujs-1.nonecase">完美解决方案</a></p>
<p>上述方案无法解决类似*$这样的问题，如何解决？？</p></li>
<li><p>hexo公式换行的问题？？</p></li>
<li><p>公式内的“\#”无法解析，或其他公式渲染问题，报"Template render
error"错误 尽量不要用#符号</p></li>
</ul>
]]></content>
      <tags>
        <tag>Issues</tag>
      </tags>
  </entry>
  <entry>
    <title>Citadel Central Regional Datathon 2021</title>
    <url>/Citadel-Central-Datathon-2021/</url>
    <content><![CDATA[<h1 id="whos-still-smoking">Who's still smoking?</h1>
<p>The goal is to use tobacco related data in order to discover and
analyze patterns associated with tobacco usage.</p>
<h2 id="prize">Prize</h2>
<p><a href="https://www.credential.net/4c9232c5-10b9-4f7e-a5ab-0d04a2f7a317">2nd
Place Winner of Central Regional Datathon 2021</a></p>
<h2 id="result">Result</h2>
<p><a href="https://github.com/mtanghu/Citadel-Central-Datathon-Fall21">code</a></p>
<div class="pdf-container" data-target="../pdf/Team_2_report.pdf" data-height="1000px"></div>
]]></content>
  </entry>
  <entry>
    <title>Machine Learning Week01</title>
    <url>/Machine-Learning-Week01/</url>
    <content><![CDATA[<h2 id="week-1">Week 1</h2>
<ol type="1">
<li><p><strong>Supervised Learning 监督学习</strong></p>
<ul>
<li>分类问题 classification problem
<ul>
<li>discrete output（0，1）</li>
<li>features</li>
<li>特征无限多的时候可以用SVM支持向量机</li>
</ul></li>
<li>回归问题 regression problem
<ul>
<li>continuous output</li>
</ul></li>
</ul></li>
<li><p><strong>Unsupervised Learning 无监督学习</strong></p>
<ul>
<li><p><strong>clustering algorithm 聚类算法</strong></p>
<ul>
<li><p>dataset（no label） find structure;根据数据内部关系分类</p></li>
<li><p>E.g. 网站分类；基因分类；social network analysis；market
segmentation；Astronomical data analysis</p></li>
<li><p><strong>Cocktail party
problem</strong>：混杂的声音中分离出两种声音</p>
<figure class="highlight matlab"><table><tbody><tr><td class="code"><pre><span class="line">[W,s,v] = svd((<span class="built_in">repmat</span>(sum(x.*x,<span class="number">1</span>),<span class="built_in">size</span>(x,<span class="number">1</span>),<span class="number">1</span>).*x)*x')</span><br></pre></td></tr></tbody></table></figure>
<p>*SVD: singular value decomposition 奇异值分解，求解线性方程</p>
<p>octave做原型，然后迁移到C或JAVA</p></li>
</ul></li>
</ul></li>
<li><p><strong>Model and Cost Function 模型和成本函数</strong></p>
<ul>
<li><p><strong>线性回归算法 linear regression</strong></p>
<ul>
<li><p>training set 训练集</p></li>
<li><p>m - 训练样本数</p>
<p>x - 输入变量</p>
<p>y - 输出变量</p>
<p><span class="math inline">\((x^{i},y^{i})\)</span> - i training
example</p>
<p><span class="math inline">\(\theta\)</span> - Parameters</p>
<p>h - hypothesis，x映射到y的函数 ：<span class="math inline">\(h_{\theta}(x)=\theta_0+\theta_1x\)</span>；缩写即<span class="math inline">\(h(x)\)</span></p>
<p>不一定都是线性方程</p></li>
</ul></li>
<li><p><strong>Cost Function 成本函数</strong></p>
<ul>
<li><p>度量函数拟合的程度</p></li>
<li><p>平方误差成本函数：适用于线性回归</p>
<p>Hypothesis: <span class="math display">\[\min
\limits_{\theta_0\theta_1}\frac {1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</span></p>
<p>Cost function: <span class="math display">\[J(\theta_0,\theta_1)=\frac {1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</span> ----- (Squared
Error Function)</p>
<p>contour plot: 轮廓图；等高线图</p></li>
</ul></li>
</ul></li>
<li><p><strong>Parameter Learning</strong></p>
<ul>
<li><p>Gradient descent 梯度下降算法</p>
<p>不断改变<span class="math inline">\(\theta\)</span>使得J函数变小，得到局部最优点local
optimum，convergence收敛</p>
<p><span class="math display">\[\theta_j:=\theta_j-\alpha\frac
{\partial} {\partial \theta_j}J(\theta_0,\theta_j)\quad(for\ j=0\ and\
j=1)\]</span></p>
<p><span class="math inline">\(\alpha\)</span>是学习速率 learning
rate</p>
<ul>
<li><p>注意编程中的赋值后会覆盖，所以要桥接一下</p></li>
<li><p>partial derivatives 偏导数；derivatives 导数</p></li>
<li><p>不需要调整α因为导数值越来越小</p></li>
<li><p>Batch Gradient Descent
批量梯度下降：每step都用到全部的训练样本</p></li>
<li><p>高等线性代数：normal equations
method正规方程，大数据时梯度下降更好用</p></li>
</ul></li>
</ul></li>
</ol>
<h1 id="matrices-and-vectors">Matrices and Vectors</h1>
<ol type="1">
<li>Matrix: rows*columns
<ul>
<li><span class="math inline">\(\mathbb{R} ^{2\times3}\)</span> , <span class="math inline">\(A_{ij}\)</span>,</li>
</ul></li>
<li>Vector: n*1 matrix
<ul>
<li><span class="math inline">\(\mathbb{R}^4\)</span></li>
</ul></li>
<li><span class="math inline">\(A\times B\ne B\times A\)</span></li>
<li>Associative交换律</li>
<li>Identity Matrix： <span class="math inline">\(I\)</span>, <span class="math inline">\(n\times n\)</span>
<ul>
<li><span class="math inline">\(\begin{bmatrix}1&amp;\cdots&amp;0\\\vdots&amp;\ddots&amp;\vdots\\0&amp;\cdots&amp;1\end{bmatrix}\)</span></li>
<li><span class="math inline">\(A\cdot I=I\cdot A\)</span></li>
<li>I = eye(2)</li>
</ul></li>
<li>Inverse 矩阵的逆：<span class="math inline">\(A(A^{-1})=A^{-1}A=I\)</span>
<ul>
<li>pinv（A）</li>
<li><span class="math inline">\(A_{m\times m}\)</span></li>
<li>0矩阵=奇异矩阵，没有逆矩阵的矩阵是奇异矩阵</li>
<li>高斯消元法求逆：
<ul>
<li>“某行乘以一个数后加到另一行”、“某两行互换位置”、“某行乘以某一个数”，这三种以行做运算的方法</li>
<li>行变换或列变换都可以</li>
<li>增广矩阵<span class="math inline">\(B=[A|I]=\begin{vmatrix} A_{11}
&amp; A_{12} &amp; A_{13}&amp;1&amp;0&amp;0 \\ A_{21} &amp;
A_{22}&amp;A_{23}&amp;0&amp;1&amp;0\\A_{31} &amp; A_{32} &amp;
A_{33}&amp;0&amp;0&amp;1 \end{vmatrix} \Rightarrow \begin{vmatrix}
1&amp;0&amp;0&amp;A_{11}^{'} &amp; A_{12}^{'} &amp;
A_{13}^{'} \\0&amp;1&amp;0&amp;A_{21}^{'} &amp; A_{22}^{'}
&amp; A_{23}^{'}\\0&amp;0&amp;1&amp;A_{31}^{'} &amp;
A_{32}^{'} &amp; A_{33}^{'} \end{vmatrix}\)</span></li>
</ul></li>
<li>待定系数法</li>
<li>伴随矩阵法$A^* <span class="math inline">\(，\)</span>A^{-1}=$
<ul>
<li>将矩阵A元素<span class="math inline">\(a_{ij}\)</span>所在的第i行j列元素划去后剩余元素按照原来顺序组成n-1阶矩阵所确定的行列式成为元素<span class="math inline">\(a_{ij}\)</span>的余子式，记为<span class="math inline">\(M_{ij}\)</span>，称<span class="math inline">\(A_{ij}=(-1)^{i+j}M_{ij}\)</span>为元素<span class="math inline">\(a_{ij}\)</span>的代数余子式</li>
<li><span class="math inline">\(A^*\)</span>的第i行j列元素为上面的<span class="math inline">\(A_{ij}\)</span></li>
</ul></li>
<li>LU分解法A=LU，<span class="math inline">\(A^{-1}=U^{-1}L^{-1}\)</span></li>
<li>SVD分解法</li>
<li>QR分解法</li>
</ul></li>
<li>Transpose 矩阵的转置：<span class="math inline">\(A^T\)</span></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week03</title>
    <url>/Machine-Learning-Week03/</url>
    <content><![CDATA[<h1 id="machine-learning">Machine Learning</h1>
<h2 id="week-03-classification-and-representation-overfitting">Week 03
Classification and Representation | Overfitting</h2>
<h3 id="classification-and-representation-分类和表征">1. Classification
and Representation 分类和表征</h3>
<ol type="1">
<li><p><strong>Logistic Regression算法应用于Classification</strong></p>
<ul>
<li><p><strong>binary classification problem </strong>二元分类问题</p>
<ul>
<li>函数值是离散的discrete</li>
</ul></li>
<li><p><strong>Hypothesis Representation</strong> 假设陈述</p>
<ul>
<li><p>因为需要<span class="math inline">\(y∈\{0,1\}\)</span>，所以要使得<span class="math inline">\(0\le h_\theta(x)\le1\)</span>，令<span class="math display">\[h_\theta(x)=g(\theta^Tx),\
z=\theta^Tx,\  g(x)=\frac {1}{1+e^{-z}}\]</span></p></li>
<li><p><span class="math inline">\(h_\theta(x)\)</span>表示输出为1的概率: <span class="math display">\[
h_\theta(x)=P(y=1|x;\theta)=1-P(y=0|x;\theta)
\]</span></p></li>
</ul></li>
</ul></li>
<li><p><strong>Decision Boundary</strong> 决策边界：分类的边界 <span class="math display">\[
h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2)
\]</span></p>
<ul>
<li><p>非线性(non-linear)决策边界</p>
<p>多项式是决策边界的属性，不是训练集的属性，训练集决定参数θ的值</p></li>
</ul></li>
</ol>
<h3 id="logistic-regression-model-逻辑回归模型">2. Logistic Regression
Model 逻辑回归模型</h3>
<ol type="1">
<li><p><strong>Cost Function</strong></p>
<ul>
<li><p>convex 凸函数，可以收敛到全局最小</p></li>
<li><p>不能使用linear regression的Cost function，因为这会使得logistic
function的成本函数变成波浪形，形成许多局部最优，就无法形成凸函数</p></li>
<li><p>因此替换成本函数为： <span class="math display">\[
J(\theta)=\frac {1}{m}\sum
\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)}) \\
Cost(h_\theta(x),y)=
\begin{cases}-log(h_\theta(x)) &amp; \quad if\ y=1 \\
-log(1-h_\theta(x)) &amp; \quad if\ y=0
\end{cases}
\]</span></p>
<blockquote>
<p><strong>The cost function in this way guarantees that J(θ) is convex
for logistic regression.</strong></p>
</blockquote></li>
<li><p>这里的Cost function等价于： <span class="math display">\[
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
\]</span></p></li>
<li><p>带入到代价函数 J(θ) 中</p></li>
<li><p>使用<strong>==最大似然估计法==</strong>可以得出这个代价函数，最大似然估计可以有效地为不同模型找到参数数据。</p></li>
<li><p>凸性是这个函数优秀地属性之一</p></li>
<li><p>接下来：为了拟合θ，就要最小化 J(θ)</p></li>
</ul></li>
<li><p><strong>Gradient Descent 梯度下降法</strong> <span class="math display">\[
\theta_j:=\theta_j-\alpha\frac {\partial} {\partial
\theta_j}J(\theta)=\theta_j-\alpha\frac {1}{m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\quad(for\ j=0...n)
\]</span></p>
<ul>
<li><p>选择学习速率：绘制 J(θ)
关于迭代次数的函数，使得每一次迭代函数值都在下降</p></li>
<li><p>可以使用向量化实现算法 <span class="math display">\[
\begin{array}{lc}
h=g(\theta^TX)\\
J(\theta)=\frac{1}{m}\cdot(-y^Tlog(h)-(1-y)^Tlog(1-h))\\
\text{注：这里X是$（n+1）\times1$维列向量}
\end{array}
\]</span> 最终得到： <span class="math display">\[
\theta:=\theta-\frac{\alpha}{m}X(g(\theta^TX)-\vec{y})
\]</span></p></li>
</ul></li>
</ol>
<ul>
<li>特征值缩放也同样适用该模型</li>
</ul>
<blockquote>
<p><strong>关于代价函数的最大似然估计法相关数学推导参见<a href="https://www.cnblogs.com/ranjiewen/p/5967496.html">这里</a></strong></p>
<p><strong>==统计学是机器学习的数学基础==</strong></p>
</blockquote>
<ol start="3" type="1">
<li><p>Optimization algorithm 优化算法</p>
<ul>
<li><p>例如：</p>
<ul>
<li><p>Conjugate gradient 共轭梯度法</p></li>
<li><p>BFGS变尺度法</p></li>
<li><p>L-BFGS限制变尺度法</p>
<p>优点：无需手动选择α（线性搜索法每一步都在改变α）；比梯度下降法收敛更快</p>
<p>缺点：算法更复杂、难理解</p></li>
</ul></li>
<li><p>octave中的优化算法表达：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">function [jVal, gradient] = costFunction(theta)</span><br><span class="line">  jVal = [...code to compute J(theta)...];</span><br><span class="line">  gradient = [...code to compute derivative of J(theta)...];</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">options = optimset('GradObj', 'on', 'MaxIter', 100);</span><br><span class="line">initialTheta = zeros(2,1);</span><br><span class="line">   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></tbody></table></figure></li>
</ul></li>
</ol>
<h3 id="multiclass-classification-多类别分类问题">3. Multiclass
Classification 多类别分类问题</h3>
<ol type="1">
<li><p><strong>one-vs-all 一对多分类算法</strong></p>
<p>划分为多个二分类问题，求多个h(x)。应用于预测集时，将x分类到h最大的那个集里。</p>
<p><span class="math inline">\(h^{(i)}_\theta(x)=P(y=i|x;\theta),\quad
prediction=\max\limits_{i}(h^{(i)}_\theta(x))\)</span></p></li>
</ol>
<h3 id="overfitting-过拟合">4. Overfitting 过拟合</h3>
<ol type="1">
<li><p>underfitting ——
高bias偏差（Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，即算法本身的拟合能力）</p>
<p>Overfitting ——
高variance方差（variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。反应预测的波动情况。）</p>
<blockquote>
<p>关于方差与偏差的意义参见<a href="https://blog.csdn.net/u012197749/article/details/79766317">这里</a></p>
</blockquote>
<ol start="2" type="1">
<li>解决过拟合：</li>
</ol>
<ul>
<li>减少特征的数量
<ul>
<li>手动选择保留的特征</li>
<li>模型选择算法model selection
algorithm，可以自动决定要保留的变量和要剔除的变量</li>
<li>缺点：剔除了一部分信息</li>
</ul></li>
<li>regularization 正则化
<ul>
<li>保留所有变量，减少变量θ的magnitude</li>
<li>在有很多变量时很有用</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li>正则化</li>
</ol>
<ul>
<li>在代价函数加一个正则化项，使得参数θ尽可能小，排除bias偏差项</li>
</ul>
<p><span class="math display">\[
\min\limits_\theta\frac{1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\sum\limits_{j=1}^n\theta_j^2
\]</span></p>
<ul>
<li><p>λ是正则化参数regularization parameter</p></li>
<li><p>应用于梯度下降法：（注：<span class="math inline">\(\theta_0\)</span>一定不要正则化！） <span class="math display">\[
\theta_j:=\theta_j-\alpha\frac {\partial} {\partial
\theta_j}J(\theta)=\theta_j-\alpha[\frac {1}{m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]\quad(for\
j=1...n)
\]</span></p></li>
<li><p>应用于正规方程normal equation： <span class="math display">\[
\theta=(X^TX+\lambda\cdot L)^{-1}X^Ty\\
where \quad L=\begin{bmatrix}0\\\
&amp;1\\\  &amp;\  &amp;1\\\  &amp;\  &amp;\  &amp;\ddots\\\  &amp;\  &amp;\  &amp;\  &amp;1\\\end{bmatrix}
\]</span></p>
<ul>
<li>Non-invertibility 不可逆性 suppose <span class="math inline">\(m\le
n\)</span>, 特征多于例子时，<span class="math inline">\(\theta=(X^TX)^{-1}X^Ty\)</span> 中，<span class="math inline">\(X^TX\)</span>是不可逆的（或奇异的），即为退化矩阵（degenerate）
但是加入λL
后，解决了上述问题，公式（9）括号内的矩阵是可逆的（前提是λ&gt;0）</li>
</ul></li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week02</title>
    <url>/Machine-Learning-Week02/</url>
    <content><![CDATA[<h1 id="multiple-feature">Multiple Feature</h1>
<h2 id="multivariate-linear-regression">Multivariate Linear
Regression</h2>
<ol type="1">
<li><p><span class="math inline">\(h_{\theta}(x)=\theta^{T}X\)</span></p>
<p><span class="math inline">\(h_θ(x)=\begin{bmatrix}θ_0&amp;θ_1&amp;...&amp;θ_n\end{bmatrix}\begin{bmatrix}x_0\\x_1\\⋮\\x_n\end{bmatrix}=θ^Tx\)</span></p></li>
<li><p>Gradient Descent for Multiple Variables</p>
<p>Cost function: <span class="math display">\[J(\theta_0,\theta_1...\theta_n)=\frac {1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</span></p>
<p><span class="math display">\[\theta_j:=\theta_j-\alpha\frac
{\partial} {\partial \theta_j}J(\theta)=\theta_j-\alpha\frac {1}{m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\quad(for\
j=0...n)\]</span></p></li>
<li><p>Feature Scaling</p>
<p>Make sure features are on a similar scale</p>
<p>避免梯度下降一个维度值过大导致下降速率很慢</p>
<ul>
<li>Get every feature into approximately a <span class="math inline">\(-1\le x_i\le 1\)</span> range</li>
<li>大约在这个范围附近就行，例如: <span class="math inline">\(-2\le
x_i\le 2\)</span>也可以</li>
<li>方法一：都除以最大值<span class="math inline">\(x_{max}\)</span></li>
<li>均值归一化 Mean normalization：
<ul>
<li><span class="math display">\[\frac{x_i-\mu_i}{s_{i}}\]</span></li>
<li>使得均值为0</li>
<li><span class="math inline">\(\mu\)</span>是训练集x某特征的均值，s可以是标准差，一般<span class="math inline">\(s=max-min\)</span>即可</li>
<li>特征缩放不用太精确，只是为了让梯度下降更快而已</li>
</ul></li>
</ul></li>
<li><p>确定α</p>
<ul>
<li>如果α过大，则可能出现 J 值增大或者不收敛</li>
<li>让α尽量小，使得每次迭代 J 值都在减小</li>
<li>自动收敛测试：可以设定一个值比如0.001，如果每一步J变化小于这个值即可认为收敛，但是选择合适的值很困难</li>
<li>所以可以采用：代价函数 J
随迭代步数变化曲线，可以帮助判断梯度下降算法是否收敛</li>
</ul></li>
<li><p>多项式回归 Polynomial Regression</p>
<ul>
<li>拟合复杂函数</li>
<li>将x进行平方、开根等处理。例如：<span class="math inline">\(x_3=x_1^3\)</span></li>
<li>注意新特征要进行均值归一化使得各个特征之间差距不至于过大</li>
</ul></li>
<li><p>Normal equation 正规方程</p>
<ul>
<li>solve for θ analytically，θ的解析解法</li>
<li><span class="math display">\[J(\theta_0,\theta_1...\theta_n)=\frac
{1}{2m}\sum \limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</span>
，令<span class="math inline">\(\frac {\partial} {\partial
\theta_j}J(\theta)=0\)</span>(偏微分)
<ul>
<li><span class="math inline">\(X=\begin{bmatrix}x_0&amp;x_1&amp;...&amp;x_n\end{bmatrix}\)</span>，x代表特征的列向量集合</li>
<li>则可以推出：<span class="math inline">\(\theta=(X^TX)^{-1}X^Ty\)</span>，即正规方程</li>
</ul></li>
<li>matlab：pinv(X‘*X)*X'*y</li>
<li>用正规方程就不用变量归一化</li>
<li>优点：不用选择α；不用迭代</li>
<li>缺点：n较大时，算逆矩阵会比较慢，n&gt;10000，复杂度大约在<span class="math inline">\(O(n^3)\)</span></li>
<li>正规方程的不可逆性 Normal Equation Non-invertibility
<ul>
<li>pinv可以求伪逆，inv求逆</li>
<li>如果<span class="math inline">\(X^TX\)</span>是不可逆的：
<ul>
<li>特征之间是相关的</li>
<li>特征多于样本数</li>
<li>解决方法：删除多余特征或正则化</li>
</ul></li>
</ul></li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week04</title>
    <url>/Machine-Learning-Week04/</url>
    <content><![CDATA[<h2 id="neural-networks-representation">Neural Networks：
Representation</h2>
<h3 id="non-linear-hypotheses">1. Non-linear Hypotheses</h3>
<p>特征多，有高次项</p>
<h3 id="neural-networks">2. Neural Networks</h3>
<ol type="1">
<li><p>Model representation</p>
<ul>
<li><p>bias unit偏置项（=1）</p></li>
<li><p>sigmoid activation function S型激励函数</p></li>
<li><p>input layer —— hidden layer —— output layer</p></li>
<li><p><span class="math display">\[ a_i^{(j)}= "activation"\
of\ unit\ i\ in\ layer\  j \]</span></p>
<p><span class="math inline">\(\Theta^{(j)}=\)</span> matrix of weights
controlling function mapping from layer j to layer j+1
，即参数矩阵（波矩阵、权重矩阵）</p>
<p>如果一个网络在 j 层有<span class="math inline">\(s_j\)</span>个单元，j+1层有<span class="math inline">\(s_{j+1}\)</span>个单元，那么<span class="math inline">\(\Theta^{(j)}\)</span>的矩阵维度是<span class="math inline">\(s_{j+1}\times(s_j+1)\)</span>，因为要加上bias
unit</p>
<p><img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/屏幕截图 2020-11-09 112621.png" alt="神经网络" style="zoom:50%;">
<span class="math display">\[
\begin{array}{r}
a_{1}^{(2)}=g\left(\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)}
x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)}
x_{3}\right)=g(z_1^{(2)}) \\
a_{2}^{(2)}=g\left(\Theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)}
x_{1}+\Theta_{22}^{(1)} x_{2}+\Theta_{23}^{(1)}
x_{3}\right)=g(z_2^{(2)})  \\
a_{3}^{(2)}=g\left(\Theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)}
x_{1}+\Theta_{32}^{(1)} x_{2}+\Theta_{33}^{(1)} x_{3}\right)
=g(z_3^{(2)}) \\
h_{\Theta}(x)=a_{1}^{(3)}=g\left(\Theta_{10}^{(2)}
a_{0}^{(2)}+\Theta_{11}^{(2)} a_{1}^{(2)}+\Theta_{12}^{(2)}
a_{2}^{(2)}+\Theta_{13}^{(2)} a_{3}^{(2)}\right)
\end{array}
\]</span></p></li>
<li><p>即前向传播（forward propagation）</p></li>
</ul></li>
</ol>
<h3 id="applications">3. Applications</h3>
<ol type="1">
<li>与运算（-30，20，20）</li>
<li>或运算（-10，20，20）</li>
<li>非运算（10，-20）</li>
<li>XNOR运算，同或运算（相同为1，否则为0）：两层神经网络</li>
<li>Multiclass Classification，如果4个分类器结果则例如[0,0,1,0]</li>
</ol>
<p>​</p>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week05</title>
    <url>/Machine-Learning-Week05/</url>
    <content><![CDATA[<h2 id="neural-networks-learning">Neural Networks Learning</h2>
<h3 id="cost-function-and-propagation">Cost Function and
Propagation</h3>
<ol type="1">
<li><p>Cost Function</p>
<ul>
<li><p>L 神经网络的层数</p></li>
<li><p><span class="math inline">\(s_l\)</span>第l层的神经元个数，不包括bias
unit</p></li>
<li><p>K输出层的神经元个数（即种类）</p></li>
<li><p>binary classification二元分类</p></li>
<li><p>Logistic regression的代价函数： <span class="math display">\[
J(\theta)=-\frac {1}{m}\sum \limits_{i=1}^m\bigg
[y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))\bigg
]+\frac{\lambda}{2m}\sum \limits_{j=1}^n\theta_j^2
\]</span></p></li>
<li><p>神经网络的代价函数： <span class="math display">\[
J(\Theta)=-\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[y_{k}^{(i)}
\log \left((h_{\Theta}(x^{(i)}))_{k}\right)+\left(1-y_{k}^{(i)}\right)
\log \left(1-(h_{\Theta}(x^{(i)}))_{k}\right)\right]+\frac{\lambda}{2 m}
\sum_{l=1}^{L-1} \sum_{i=1}^{s_l}
\sum_{j=1}^{s_{l+1}}(\theta_{j,i}^{(l)})^2
\]</span></p>
<ul>
<li>两个求和符号部分只是将输出层中每个单元的逻辑回归代价函数相加</li>
<li>三个求和符号部分只是将整个网络中所有单个θ的平方相加，其中i并不指代训练实例i</li>
</ul></li>
</ul></li>
<li><p>反向传播算法Backpropagation Algorithm</p>
<ul>
<li><p>梯度下降计算</p>
<ul>
<li><p>min J（θ）就需要计算：J（θ）；J（θ）关于各个θ的偏导</p></li>
<li><p>计算过程：先forward propagation；再反向</p></li>
<li><p><span class="math inline">\(\delta_j^{l}="error"\ of\
node\ j\ in\ layer\ l\)</span>用于改变activation激励值, Formally, <span class="math inline">\(\delta_j^{(l)}=\frac \partial {\partial
z_{j}^{(l)}} cost(j)\)</span>, 其中<span class="math inline">\(cost(i)=y^{(i)}log(h_\Theta(x^{(i)}))-(1-y^{(i)})log(1-h_\Theta(x^{(i)}))\)</span>,
求导后易得<span class="math inline">\(\delta_j^{(l)}=y_j^{(l)}-a_j^{(l)}\)</span>(为什么符号是相反的？答：这里cost错误，和前文代价函数符号相反，应该要变号)</p></li>
<li><p>计算过程：</p>
<ul>
<li><p>令<span class="math inline">\(\Delta_{i,j}^{l}:=0\)</span></p></li>
<li><p>For training example t =1 to m:</p>
<ol type="1">
<li><p>Set <span class="math inline">\(a^{(1)} :=
x^{(t)}\)</span></p></li>
<li><p>Perform forward propagation to compute <span class="math inline">\(a^{(l)}\ for\ l=2,3,…,L\)</span></p>
<p>(此处失效图片链接)img
(https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bYLgwteoEeaX9Qr89uJd1A_73f280ff78695f84ae512f19acfa29a3_Screenshot-2017-01-10-18.16.50.png?expiry=1606176000000&amp;hmac=9aVGT1io0l-sybFSrc1stejo_L0d7hzlNXbQIt47h2Y)</p></li>
<li><p>Using <span class="math inline">\(y^{(t)}\)</span>, compute$
^{(L)} = a^{(L)} - y^{(t)}$</p>
<p>Where L is our total number of layers and
a^{(L)}<em>a</em>(<em>L</em>) is the vector of outputs of the activation
units for the last layer. So our "error values" for the last layer are
simply the differences of our actual results in the last layer and the
correct outputs in y. To get the delta values of the layers before the
last layer, we can use an equation that steps us back from right to
left:</p></li>
<li><p>Compute$ ^{(L-1)}, <sup>{(L-2)},,</sup>{(2)}<span class="math inline">\(using\)</span>$ ^{(l)} = ((<sup>{(l)})</sup>T
^{(l+1)})&nbsp;.<em>&nbsp;a^{(l)}&nbsp;.</em>&nbsp;(1 - a^{(l)}) $$</p>
<p>The delta values of layer l are calculated by multiplying the delta
values in the next layer with the theta matrix of layer l. We then
element-wise multiply that with a function called g', or g-prime, which
is the derivative of the activation function g evaluated with the input
values given by <span class="math inline">\(z^{(l)}\)</span>.</p>
<p>The g-prime derivative terms can also be written out as:</p>
<p><span class="math inline">\(g′(z^{(l)})=a^{(l)} .∗
(1−a^{(l)})\)</span></p></li>
<li><p><span class="math inline">\(Δ_{i,j}^{(l)}:=Δ_{i,j}^{(l)}+a_j^{(l)}δ_i^{(l+1)}\)</span>,
or with vectorization, <span class="math inline">\(Δ^{(l)}:=Δ^{(l)}+δ^{(l+1)}(a^{(l)})^T\)</span></p>
<p>Hence we update our new <span class="math inline">\(\Delta\)</span>
matrix.</p>
<ul>
<li><p><span class="math inline">\(D_{i,
j}^{(l)}:=\frac{1}{m}\left(\Delta_{i, j}^{(l)}+\lambda \Theta_{i,
j}^{(l)}\right), \text { if } j \neq 0\)</span></p></li>
<li><p><span class="math inline">\(D_{i, j}^{(l)}:=\frac{1}{m}\Delta_{i,
j}^{(l)}, \text { if } j = 0\)</span></p>
<p>The capital-delta matrix D is used as an "accumulator" to add up our
values as we go along and eventually compute our partial derivative.
Thus we get$ J()= D_{ij}^{(l)}$</p></li>
</ul></li>
</ol></li>
</ul></li>
<li><p><a href="https://zhuanlan.zhihu.com/p/26765585">上述计算过程的中文推导</a></p></li>
</ul></li>
</ul></li>
</ol>
<h3 id="backpropagation-in-practice">Backpropagation in practice</h3>
<ol type="1">
<li><p>系数展开到向量：</p>
<ul>
<li><p>M(a,b)，既取a，也取b，从1开始</p></li>
<li><p>优化算法（如：fminunc）默认将参数整合到一个向量中</p>


   - 
   <figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">%代码过程</span><br><span class="line">thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]</span><br><span class="line">deltaVector = [ D1(:); D2(:); D3(:) ]</span><br><span class="line"></span><br><span class="line">Theta1 = reshape(thetaVector(1:110),10,11)</span><br><span class="line">Theta2 = reshape(thetaVector(111:220),10,11)</span><br><span class="line">Theta3 = reshape(thetaVector(221:231),1,11)</span><br></pre></td></tr></tbody></table></figure>

     </li>
<li><p>过程：(此处失效图片链接)img(https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/kdK7ubT2EeajLxLfjQiSjg_d35545b8d6b6940e8577b5a8d75c8657_Screenshot-2016-11-27-15.09.24.png?expiry=1606348800000&amp;hmac=bH794vb16zxSOiqZRj2Pe0PyuaYNbZ8tDQZlnGSoM18)</p></li>
</ul></li>
<li><p>梯度检验gradient checking</p>
<ul>
<li><p><span class="math inline">\(\frac{d}{d\Theta}J(\Theta)\approx
\frac{J(\Theta+\epsilon)+J(\Theta-\epsilon)}{2\epsilon}\)</span>，ε =
<span class="math inline">\(10^{-4}\)</span></p></li>
<li><p><span class="math display">\[
\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1,
\dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots,
\Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}
\]</span></p></li>
<li><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">epsilon = 1e-4;</span><br><span class="line">for i = 1:n,</span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(i) += epsilon;</span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(i) -= epsilon;</span><br><span class="line">  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)</span><br><span class="line">end;</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>Check: gradApprox ≈
deltaVector，只需要验证一次即可，否则这种方法会非常慢</p></li>
</ul></li>
<li><p>Random Initialization随机初始化</p>
<ul>
<li><p>Symmetry
breaking，因为如果设为一样的会使得梯度下降后参数也一样</p></li>
<li><p>将每一个<span class="math inline">\(\Theta_{ij}^{(l)}\)</span>设为在[-ε，ε]之间的随机数，但要同时设置，以防出现相同，例：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Theta1 = rand(10,11)*2*init_epsilon-init_epsilon;</span><br></pre></td></tr></tbody></table></figure>
<p>(Note: the epsilon used above is unrelated to the epsilon from
Gradient Checking)</p></li>
</ul></li>
<li><p>总体回顾</p>
<ul>
<li>一般默认隐藏层每层神经元数量一致</li>
<li>构建一个模型的过程：
<ol type="1">
<li>初始化模型
<ul>
<li>Number of input units = dimension of features
x^{(i)}<em>x</em>(<em>i</em>)</li>
<li>Number of output units = number of classes</li>
<li>Number of hidden units per layer = usually more the better (must
balance with cost of computation as it increases with more hidden
units)</li>
<li>Defaults: 1 hidden layer. If you have more than 1 hidden layer, then
it is recommended that you have the same number of units in every hidden
layer.</li>
</ul></li>
<li><strong>Training a Neural Network</strong>
<ol type="1">
<li>Randomly initialize the weights</li>
<li>Implement forward propagation to get <span class="math inline">\(h_\Theta(x^{(i)})\)</span> for any <span class="math inline">\(x^{(i)}\)</span></li>
<li>Implement the cost function</li>
<li>Implement backpropagation to compute partial derivatives</li>
<li>Use gradient checking to confirm that your backpropagation works.
Then disable gradient checking.</li>
<li>Use gradient descent or a built-in optimization function to minimize
the cost function with the weights in theta.</li>
<li>对每一个训练样例循环上述步骤（有可能得到局部最优，因为J（θ）是非凸函数）</li>
</ol></li>
</ol></li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week06</title>
    <url>/Machine-Learning-Week06/</url>
    <content><![CDATA[<h2 id="deciding-what-to-try-next">Deciding What to Try Next</h2>
<h3 id="evaluating-a-learning-algorithm">Evaluating a Learning
Algorithm</h3>
<ol type="1">
<li><p>误差大的改进方法：</p>
<ul>
<li>更多数据</li>
<li>更少特征集避免过拟合</li>
<li>更多特征</li>
<li>增加多项式特征</li>
<li>减少或增大正则化参数值</li>
</ul></li>
<li><p>评估假设函数Evaluating a Hypothesis</p>
<ul>
<li><p>如何判断过拟合？</p>
<ul>
<li><p>随机将数据分为两部分，一部分是训练集，一部分是预测集（30%）</p></li>
<li><p>学习训练集的参数θ（即最小化训练误差J ）</p></li>
<li><p>计算测试集误差</p>
<ul>
<li><p>线性回归中计算测试集的 J(θ)</p></li>
<li><p>逻辑回归中可以用误分类率来计算error：</p>
<p><span class="math inline">\(err(h_\theta(x),y)=\begin{cases}1,&amp;if\
h_\theta(x)\ge 0.5,y=0\  or\ if\ h_\theta(x)&lt;0.5, y=1\\0,
&amp;otherwise\end{cases}\)</span></p>
<p><span class="math inline">\(Test\
error=\frac{1}{m_{test}}\sum\limits_{i=1}^{m_{test}}err(h_\theta(x_{test}^{(i)},y^{(i)}))\)</span></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>模型选择问题：确定对于某组数据最合适的多项式是几次，怎样选用正确的特征来构造学习算法，或者需要正确选择算法中的正则化参数λ</p>
<ul>
<li>将数据分为三段：训练集（60%），交叉验证集（cross
validation）（20%），测试集（20%）</li>
<li>用不同的多项式模型得到θ，然后计算交叉验证集的误差，看看哪个模型中CV集的误差最小，进而选择那个多项式模型</li>
<li>最后计算测试集误差评价模型表现</li>
</ul></li>
</ol>
<h3 id="bias-vs.-variance">Bias vs. Variance</h3>
<ol type="1">
<li><p>分析bias和variance</p>
<ul>
<li>高偏差：欠拟合；高方差：过拟合</li>
<li>高偏差：
<ul>
<li><span class="math inline">\(J_{train}(\theta)\)</span>will be high,
the same as <span class="math inline">\(J_{CV}(\theta)\approx
J_{train}(\theta)\)</span></li>
</ul></li>
<li>高方差：
<ul>
<li><span class="math inline">\(J_{train}(\theta)\)</span>will be low,
<span class="math inline">\(J_{CV}(\theta)\gg
J_{train}(\theta)\)</span></li>
</ul></li>
<li><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/I4dRkz_pEeeHpAqQsW8qwg_bed7efdd48c13e8f75624c817fb39684_fixed.png?expiry=1606262400000&amp;hmac=zQgQniOf2GNzDd--YwZNZhUDjMOAn1JxYg1s_gG_d_I" title="fig:" alt="img"></li>
</ul></li>
<li><p>正则化与方差、偏差的关系</p>
<ul>
<li><p>正则化可以有效防止过拟合</p></li>
<li><p>选择λ的过程：</p>
<ul>
<li>Create a list of lambdas (i.e.
λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});</li>
<li>Create a set of models with different degrees or any other
variants.</li>
<li>Iterate through the λs and for each λ go through all the models to
learn some Θ.</li>
<li>Compute the cross validation error using the learned Θ (computed
with λ) on the$ J_{CV}()$ <strong>without</strong> regularization or λ =
0.</li>
<li>Select the best combo that produces the lowest error on the cross
validation set.</li>
<li>Using the best combo Θ and λ, apply it on$ J_{test}()$ to see if it
has a good generalization of the problem.</li>
</ul></li>
<li><p>注：训练时带λ，计算各个集的误差时不需要λ，即<span class="math inline">\(J_{train}(\theta)\)</span>、<span class="math inline">\(J_{CV}(\theta)\)</span>、<span class="math inline">\(J_{test}(\theta)\)</span>都不包含正则项 <span class="math display">\[
J_{train}(\theta)=\frac {1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
\]</span></p></li>
</ul></li>
<li><p>学习曲线（error-训练集的大小）</p>
<ul>
<li>高偏差
<ul>
<li><strong>Low training set size</strong>: causes <span class="math inline">\(J_{train}(\Theta)\)</span> to be low and <span class="math inline">\(J_{CV}(\Theta)\)</span> to be high.</li>
<li><strong>Large training set size</strong>: causes both <span class="math inline">\(J_{train}(\Theta)\)</span> and <span class="math inline">\(J_{CV}(\Theta)\)</span> to be high with <span class="math inline">\(J_{train}(\Theta)≈J_{CV}(\Theta)\)</span>.</li>
<li>高偏差，更多训练数据不会有很大帮助</li>
<li><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bpAOvt9uEeaQlg5FcsXQDA_ecad653e01ee824b231ff8b5df7208d9_2-am.png?expiry=1606262400000&amp;hmac=-xKhLZnoeJ_Ht3_X7XaX3Jae2F94vNrpbyPcMCExkPg" title="fig:" alt="img"></li>
</ul></li>
<li>高方差
<ul>
<li><strong>Low training set size</strong>: causes <span class="math inline">\(J_{train}(\Theta)\)</span> to be low and <span class="math inline">\(J_{CV}(\Theta)\)</span> to be high.</li>
<li><strong>Large training set size</strong>: <span class="math inline">\(J_{train}(\Theta)\)</span> increases with training
set size and <span class="math inline">\(J_{CV}(\Theta)\)</span>
continues to decrease without leveling off. Also, <span class="math inline">\(J_{train}(\Theta) &lt; J_{CV}(\Theta)\)</span> but
the difference between them remains significant.</li>
<li>高方差，更多训练数据可能有帮助</li>
<li><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/vqlG7t9uEeaizBK307J26A_3e3e9f42b5e3ce9e3466a0416c4368ee_ITu3antfEeam4BLcQYZr8Q_37fe6be97e7b0740d1871ba99d4c2ed9_300px-Learning1.png?expiry=1606262400000&amp;hmac=6kgO1n2qmCGikLwAE15RXslkLQYjwcHJ2utP_e9INDk" title="fig:" alt="img"></li>
</ul></li>
</ul></li>
<li><p>诊断法则如何判断</p>
<ul>
<li>更多训练数据——高方差，画学习曲线</li>
<li>更少特征集避免过拟合——高方差</li>
<li>更多特征——高偏差</li>
<li>增加多项式特征——高偏差</li>
<li>减少（高偏差）或增大正则化参数值（高方差）
<ul>
<li>正则化会保留所有的特征变量，但是会减小特征变量的数量级。正则化就是使用惩罚项，通过惩罚项，我们可以将一些参数的值变小。通常参数值越小，对应的函数也就越光滑，也就是更加简单的函数，因此不容易发生过拟合问题。</li>
</ul></li>
<li>神经网络很容易过拟合，正则化项非常有用
<ul>
<li>如何选择几层隐藏层：
<ul>
<li>通过交叉验证集，测试1，2，…，l个隐藏层的误差，选择表现最好的一个</li>
</ul></li>
</ul></li>
</ul></li>
</ol>
<h3 id="build-a-spam-classifier">Build a Spam Classifier</h3>
<ol type="1">
<li>做法：
<ul>
<li>遍历整个训练集，然后在中间选出出现次数最多的n个单词，n一般介于10000和50000之间，作为特征</li>
<li>列出可能的做法，讨论可行性，然后选择一个方向</li>
</ul></li>
<li>误差分析
<ul>
<li>推荐做法：
<ul>
<li>用简单算法快速实现，然后测试CV集（避免过早优化）</li>
<li>画学习曲线，决定是否需要更多数据、更多特征或其他</li>
<li>误差分析：手动检验CV集中的错误分类样本，发现系统性的错误分类特征，构造更好的特征
<ul>
<li>手动对错误的部分分类</li>
<li>发现特征</li>
<li>用数字来量化表现误差（在CV集上，不能在test集上）</li>
</ul></li>
</ul></li>
</ul></li>
<li>stem词干提取（porter stemmer）</li>
<li>skewed classes 偏斜类问题
<ul>
<li>癌症的比例非常非常低</li>
<li>查准率（precision）：预测为1的病人里，多少是真正得癌症的
<ul>
<li><span class="math inline">\(=\frac{True\ positives}{predicted\
positives}=\frac{True\ positives}{True\ pos+False\ pos}\)</span></li>
</ul></li>
<li>召回率Recall：实际得癌症的病人里，多少是真正预测得癌症的
<ul>
<li><span class="math inline">\(=\frac{True\ positives}{actual\
positives}=\frac{True\ positives}{True\ pos+False\ neg}\)</span></li>
</ul></li>
<li><img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/113quiz1q1.png"></li>
<li>假设现在想要预测y=1，当非常自信的情况下（修改<span class="math inline">\(h_\theta(x)\)</span>分类临界值threshold0.5为0.9）：高查准，低召回</li>
<li>假设想要避免错过癌症案例，（修改<span class="math inline">\(h_\theta(x)\)</span>分类临界值0.5为0.3）：高召回，低查准</li>
<li>绘制查准率-召回率曲线</li>
<li><span class="math inline">\(F_1\)</span>Score <span class="math inline">\(=\frac{2PR}{P+R}\)</span>:
<ul>
<li>评估选择算法或者不同临界值的量化标准</li>
</ul></li>
</ul></li>
<li>Data For Machine Learning
<ul>
<li>有大量训练数据可以显著提升算法表现，可能得到低方差、低偏差的结果，test误差和train误差也相近</li>
<li>多项式参数对大训练集没有帮助</li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week07</title>
    <url>/Machine-Learning-Week07/</url>
    <content><![CDATA[<h2 id="support-vector-machines-支持向量机">Support Vector Machines
支持向量机</h2>
<h3 id="large-margin-classification">Large Margin Classification</h3>
<ol type="1">
<li><p>Optimization Objective 优化目标</p>
<ul>
<li><p>在复杂非线性方程的学习上有优势。</p></li>
<li><figure>
<img src="C:\Users\Jun\AppData\Roaming\Typora\typora-user-images\image-20201206123628670.png" alt="image-20201206123628670">
<figcaption aria-hidden="true">image-20201206123628670</figcaption>
</figure></li>
<li><figure>
<img src="C:\Users\Jun\AppData\Roaming\Typora\typora-user-images\image-20201206124245802.png" alt="image-20201206124245802">
<figcaption aria-hidden="true">image-20201206124245802</figcaption>
</figure>
<p>注：实际上在训练代价函数时，分界线为-1，1</p></li>
</ul></li>
<li><p>Large Margin Intuition</p>
<ul>
<li><p>代价函数： <span class="math display">\[
\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)}
{\operatorname{cost}_{1}\left(\theta^{T}
x^{(i)}\right)}+\left(1-y^{(i)}\right)
\operatorname{cost}_{0}\left(\theta^{T}
x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}\\
if\ y=1, we\ want\ \theta^{T} x \geq 1\ (not\ just \left.\geq 0\right)\\
if\ y=0, we\ want\ \theta^{T} x \leq-1\ (not\ just&lt;0 )
\]</span></p></li>
<li><p>SVM也被叫做大间距分类器，因为它尽量用最大间距分类样本，使得SVM更具有鲁棒性</p></li>
<li><p>其中正则化参数C不适合设置得过大，因为这会使得一些异常值（outliner)
会使得分类向异常值产生比较大得偏差，这就类似与逻辑回归里过小的λ
。</p></li>
</ul></li>
<li><p>Mathematics Behind Large Margin Classification</p>
<ul>
<li><p>向量内积：<span class="math inline">\(\lVert u
\rVert\)</span>表明u的范数，即u向量的长度。<span class="math inline">\(u^Tv=p\cdot\lVert u \rVert\)</span>, p
是v在u上的投影长度</p></li>
<li><p>本图演示了为什么这种方式可以最小化θ<img src="C:\Users\Jun\AppData\Roaming\Typora\typora-user-images\image-20201206171424283.png" alt="image-20201206171424283"></p>
<p>注：这里决策边界和θ是具有垂直关系的</p></li>
<li><p>这里最小化θ范数等价于最大化所有的p，即最大化所有点到决策边界的margin。</p></li>
<li><p>即使<span class="math inline">\(\theta_0\ne0\)</span>推理过程也和上述类似</p></li>
</ul></li>
</ol>
<h3 id="kernels">Kernels</h3>
<ol type="1">
<li><p>非线性决策边界</p>
<ul>
<li><p>用相似度函数similarity
function代替高阶项，度量样本x和第一个标记的相似度，即核函数，例如：高斯核函数：<span class="math inline">\(f_1=similarity(x,l^{(1)})=exp(-\frac{\lVert
x-l^{(1)}
\rVert^2}{2\sigma^2})=exp(-\frac{\Sigma_{j=1}^{n}(x_j-l_j^{(i)})^2}{2\sigma^2})\)</span></p>
<p>==问：这里σ是什么，如何计算？==</p></li>
<li><p>通过用f替代特征，可以训练出复杂的非线性边界：例如：预测y=1，当<span class="math inline">\(\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3\ge0\)</span></p></li>
</ul></li>
<li><p>选择landmark标记点 l :</p>
<ul>
<li>Given一系列x，令<span class="math inline">\(l^1=x^1\)</span>, <span class="math inline">\(f_{m}^{(i)}=similarity(x^{(i)},l^{(m)})\)</span></li>
<li>将<span class="math inline">\(x^{(i)}\in
\mathbb{R}^{n+1}\)</span>，转变为<span class="math inline">\(f^{(i)}=\begin{bmatrix}f_0^{(i)}\\f_0^{(i)}\\
\vdots \\ f_m^{(i)} \end{bmatrix}\)</span>，<span class="math inline">\(f_0^{(i)}=1\)</span>。<span class="math inline">\(f\in \mathbb{R}^{m+1}\)</span></li>
</ul></li>
<li><p>Hypothesis：Given x ， compute features f</p>
<p>Predict "y=1" if <span class="math inline">\(\theta^Tf\ge0\)</span></p></li>
<li><p>代价函数： <span class="math display">\[
\min _{\theta} C \sum_{i=1}^{m} y^{(i)}
\operatorname{cost}_{1}\left(\theta^{T}
f^{(i)}\right)+\left(1-y^{(i)}\right)
\operatorname{cost}_{0}\left(\theta^{T} f^{(i)}\right)+\frac{1}{2}
\sum_{j=1}^{m } \theta_{j}^{2}
\]</span> 在SVM的实际应用中，最后一项通常会用<span class="math inline">\(\theta^TM\theta\)</span>，其中M和核函数有关，是为了提高计算效率</p></li>
<li><p>参数的选择：</p>
<ul>
<li><p>C(=<span class="math inline">\(\frac{1}{\lambda}\)</span>)</p>
<ul>
<li>Large C：Lower bias；High variance</li>
<li>Small C：Higher bias；Low variance</li>
</ul></li>
<li><p><span class="math inline">\(\sigma^2\)</span></p>
<ul>
<li><p>Large <span class="math inline">\(\sigma^2\)</span>: Fearures fi
vary more smoothly.</p>
<p>​ Higher bias; Lower variance</p></li>
<li><p>Small <span class="math inline">\(\sigma^2\)</span>: Features fi
vary less smoothly.</p>
<p>​ Lower bias, Higher variance.</p></li>
</ul></li>
<li><p><strong>方差（variance）：</strong>方差描述的是训练数据在不同迭代阶段的训练模型中，预测值的变化波动情况（或称之为离散情况）。</p></li>
<li><p><strong>偏差（bias）：</strong>偏差衡量了模型的预测值与实际值之间的偏离关系。</p></li>
<li><p>低偏差高方差：过拟合；</p></li>
<li><p>You want to train C and the parameters for the kernel function
using the training and cross-validation datasets.</p></li>
</ul></li>
</ol>
<h3 id="svms-in-practice">SVMs in Practice</h3>
<ol type="1">
<li><p>应用SVM</p>
<ul>
<li><p>选择C和σ</p>
<figure class="highlight matlab"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[C, sigma]</span> = <span class="title">dataset3Params</span><span class="params">(X, y, Xval, yval)</span></span></span><br><span class="line"><span class="comment">% 这里x是训练集，xval是验证集</span></span><br><span class="line">value = [<span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>];</span><br><span class="line"></span><br><span class="line">minC = <span class="number">0</span>;</span><br><span class="line">minSigma = <span class="number">0</span>;</span><br><span class="line"><span class="comment">% 最小值设为交叉验证集的用例数</span></span><br><span class="line">minError = <span class="built_in">size</span>(Xval,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">8</span>,</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="number">8</span>,</span><br><span class="line">        model= svmTrain(X, y, value(<span class="built_in">i</span>), @(x1, x2) gaussianKernel(x1, x2, value(<span class="built_in">j</span>)));</span><br><span class="line">        predictions = svmPredict(model,Xval);</span><br><span class="line">        error = <span class="built_in">mean</span>(double(predictions ~= yval));</span><br><span class="line">        <span class="keyword">if</span> minError &gt; error</span><br><span class="line">            minError = error;</span><br><span class="line">            minC = value(<span class="built_in">i</span>);</span><br><span class="line">            minSigma = value(<span class="built_in">j</span>);</span><br><span class="line">        <span class="keyword">end</span>;</span><br><span class="line">    <span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line">C = minC;</span><br><span class="line">sigma = minSigma;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></tbody></table></figure></li>
<li><p>选择核函数（相似度函数）</p>
<ul>
<li>线性核函数（=没有核函数）</li>
</ul></li>
<li><p><strong>需要对特征向量归一化</strong>，否则一个特征会占据主要位置</p></li>
<li><p>不是所有函数都能作为核函数，需要满足Mercer‘s
Theorem莫塞尔定理，来确保SVM优化包能够正确运行</p></li>
<li><p>其他核函数：</p>
<ul>
<li>多项式核函数</li>
<li>字符串核函数string kernel；</li>
<li>卡方核函数chi-square kernel</li>
<li>直方图交叉核函数histogram intersection kernel</li>
</ul></li>
<li><p>多类别分类</p>
<ul>
<li>svm包多数内置了</li>
<li>否则用one-vs-all
：训练得到K组θ的值，然后选择θx最大的那一组作为分类结果</li>
</ul></li>
<li><p>逻辑回归和SVM，什么时候用？</p>
<ul>
<li>n为特征数，m为训练集样本数。如果n相对于m来说比较大，使用逻辑回归或者没有核函数的SVM（linear
kernel）</li>
<li>n小，m中等大。使用高斯核函数</li>
<li>n小，m很大。运行很慢，逻辑回归或没有核函数的SVM</li>
<li>神经网络上面都适用，但有时可能会比SVM慢很多</li>
</ul></li>
<li><p>SVM属于凸优化问题，大多数包都会找到全局最优，无需担心局部最优。神经网络的局部最优问题不是非常大但也不小。</p></li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Read Papers</title>
    <url>/How-to-Read-Papers/</url>
    <content><![CDATA[<h1 id="how-to-read-papers">How to Read Papers?</h1>
<h2 id="resources">Resources</h2>
<ul>
<li>Arxiv</li>
<li>Blog Posts</li>
<li>Text Books</li>
</ul>
<h2 id="prepare">Prepare</h2>
<ul>
<li>列出待读论文，每一篇列一行，表示从0-100的阅读进度</li>
<li>如果有论文不是想要的，就删掉</li>
<li>如果有新的论文就加进来</li>
</ul>
<h2 id="methods">Methods</h2>
<ul>
<li>第一遍：只看标题，摘要，图表</li>
<li>第二遍：前言，结语，图表</li>
<li>第三遍：论文主体</li>
</ul>
]]></content>
      <tags>
        <tag>Learning Tips</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week08</title>
    <url>/Machine-Learning-Week08/</url>
    <content><![CDATA[<h1 id="unsupervised-learning-无监督学习">Unsupervised learning
无监督学习</h1>
<h2 id="clustering-聚类">Clustering 聚类</h2>
<p>无监督学习是不给分类标签y</p>
<h3 id="应用">1. 应用：</h3>
<ul>
<li>对消费者市场划分</li>
<li>社交网络分析</li>
<li>管理计算机</li>
<li>获取星系信息</li>
</ul>
<h3 id="k-means-algorithm-k均值算法">2. K-means algorithm K均值算法</h3>
<ul>
<li><p>随机选择两个点作为聚类中心(分两类)，然后进行下面两步</p>
<ul>
<li>簇分配，遍历每个点，和哪个cluster centroid更近</li>
<li>移动聚类中心到上面分配的簇的均值</li>
</ul></li>
<li><p>Input:</p>
<ul>
<li>K - number of clusters</li>
<li>training set</li>
</ul></li>
<li><p>algorithm</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"> repeat{</span><br><span class="line">	for i=1 to m</span><br><span class="line">		c_i := index (from 1 to K) of cluster centroid closest to x_i</span><br><span class="line">    for k=1 to K</span><br><span class="line">    	u_k := average (mean) of points assigned to cluster k</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>如果有中心没有被分配到任何点，可以删去这个分类，也可以reinitialize
the cluster centroid</p></li>
</ul>
<h3 id="k-means-for-non-sperated-clusters">3. K-means for non-sperated
clusters</h3>
<ul>
<li>根据身高体重分类T恤大小</li>
</ul>
<h3 id="optimization-objective">4. Optimization Objective</h3>
<ul>
<li><p><span class="math display">\[
\begin{array}{l}
J\left(c^{(1)}, \ldots, c^{(m)}, \mu_{1}, \ldots,
\mu_{K}\right)=\frac{1}{m}
\sum_{i=1}\left\|x^{(i)}-\mu_{c^{(i)}}\right\|^{2} \\
\min _{c^{(1)}, \ldots, c^{(m)},\mu_{1}, \ldots, \mu_{K}}
J\left(c^{(1)}, \ldots, c^{(m)}, \mu_{1}, \ldots, \mu_{K}\right)
\end{array}
\]</span></p></li>
<li><p>also called distortion of K-means algorithm</p></li>
<li><p>cost function 关于iteration的曲线</p></li>
</ul>
<h3 id="random-initialization">5. Random Initialization</h3>
<p>随机初始化聚类中心</p>
<p>随机选k个个例本身，作为中心，但是容易陷入Local
optima。解决方案：多次随机，选择cost function最小的</p>
<h3 id="choosing-the-number-of-clusters">6. Choosing the Number of
Clusters</h3>
<p>Elbow Method:</p>
<p>绘制J关于K的曲线，找到“Elbow”的节点，作为分类个数</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210331185002558.png" alt="image-20210331185002558">
<figcaption aria-hidden="true">image-20210331185002558</figcaption>
</figure>
<p>用途之一：later/downstream purpose</p>
<h2 id="motivation">Motivation</h2>
<h3 id="data-compression------dimensionality-reduction降维">1. Data
Compression ---- Dimensionality Reduction降维</h3>
<p>压缩数据，减少内存占用加快运算</p>
<h3 id="data-visualization">2. Data Visualization</h3>
<p>降维可以便于数据可视化</p>
<h2 id="principal-component-analysis主成分分析">Principal Component
Analysis主成分分析</h2>
<h3 id="problem-formulation">1. Problem formulation</h3>
<p>PCA：To find a surface onto which to project the data so as to
minimize the the projection error
(线性拟合中就是使得离某条直线的距离最小).</p>
<ul>
<li>先mean normalization（feature scaling）</li>
<li>Reduce from 2-dimension to 1-dimension: Find a direction (a vector
<span class="math inline">\(u_i\in \R^n\)</span>) onto which to project
the data so as to minimize the projection error.
该向量的正负没有关系。</li>
<li>Reduce from n-dimension to k-dimension: Find k vectors <span class="math inline">\(u^{(1)}\)</span>,<span class="math inline">\(u^{(2)}\)</span>, ... , <span class="math inline">\(u^{(k)}\)</span> onto which to project the data so
as to minimize the projection error.</li>
<li>注：PCA is not linear
regression。注意看下图区别，线性回归是和预期值的差最小，PCA是到直线距离最小</li>
</ul>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210331214953585.png" alt="image-20210331214953585">
<figcaption aria-hidden="true">image-20210331214953585</figcaption>
</figure>
<h3 id="pca-algorithm">2. PCA Algorithm</h3>
<p>PCA的目标：</p>
<ul>
<li>maximize variance perspective 最大投影方差</li>
<li>minimize error perspective 最小重构代价</li>
</ul>
<p>Step1: mean normalization.
根据数据(有不同的scales)，可能还需要feature
scaling。均值归一化是特征缩放的一种方法。（除以标准差）</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210331224012613.png" alt="image-20210331224012613">
<figcaption aria-hidden="true">image-20210331224012613</figcaption>
</figure>
<p>Step2: Reduce data from n-dimensions to k-dimensions：</p>
<ul>
<li><p>Compute "covariance matrix": <span class="math inline">\(\Sigma =
\frac{1}{m}\sum_{i=1}^{n}{(x^{(i)})(x^{(i)})^T}\)</span>，xi是n*1的列向量，symmetric
positive definite</p></li>
<li><p>Compute "eigenvectors" of matrix <span class="math inline">\(\Sigma\)</span>: [U, S, V] = svd(Sigma) ;或者
eig(Sigma)</p>
<p>svd: Singular value decomposition <a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解</a></p>
<p>其中U是我们需要的，
列向量为各个特征向量，压缩到k个向量只需要取前k个列向量（对称矩阵的特征向量是正交的）</p></li>
</ul>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210401221235384.png" alt="image-20210401221235384">
<figcaption aria-hidden="true">image-20210401221235384</figcaption>
</figure>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210401222501123.png" alt="image-20210401222501123">
<figcaption aria-hidden="true">image-20210401222501123</figcaption>
</figure>
<p>注：PCA算法中，没有x0=1这一项<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/v2-a00e6519ce24ea2b151c8bece4226d95_720w.jpg" alt="img"><img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/v2-ddbf215dce156564852941f7294c7431_720w.jpg" alt="img"></p>
<p><strong>疑问：</strong></p>
<ol type="1">
<li><p><strong>为什么不直接用X进行SVD，而要算协方差的SVD？</strong></p>
<p><strong>Answer：</strong><a href="https://www.coursera.org/learn/machine-learning/discussions/weeks/8/threads/kk41Fe5OEeqnzBL0ZMnGMw">协方差的U和X的V是相等的，其实都可以</a></p>
<p>理解：<span class="math inline">\(X=U'S'V'^T\)</span>,
<span class="math inline">\(X^TX=USV^T\)</span>，现要说明 <span class="math inline">\(V'=U\)</span>, 只需要 <span class="math inline">\(X^TX=V'S'^2V'^T=USV^T\)</span>，所以
<span class="math inline">\(V'=U\)</span>。</p></li>
<li><p><strong>为什么要取协方差矩阵SVD后的左奇异矩阵来压缩特征维度？</strong></p>
<p>首先易证一个SPD矩阵通过SVD得到的U和V是相等的。</p>
<p>其次，通常我们用XV=YU来表示坐标变换。</p>
<p><span class="math inline">\(\Sigma = X^TX\)</span>，又有 <span class="math inline">\(\Sigma = USV^T\)</span>，可以得到 <span class="math inline">\(\Sigma V =
US\)</span>，这里实际上是将covariance矩阵变化为对角线矩阵，即互不相关（其中U=V，即U为协方差矩阵的特征向量，S为其特征值）。那么这时，我们令XI=UZ，I是X坐标系下的正交基，即
<span class="math inline">\(Z=U^TX\)</span>。此时Z的协方差 <span class="math inline">\(Z^TZ=\Sigma\)</span>。</p></li>
</ol>
<h2 id="applying-pca">Applying PCA</h2>
<h3 id="reconstruction-from-compressed-representation">Reconstruction
from compressed representation</h3>
<p><span class="math display">\[
z=U_{reduce}^Tx
\]</span></p>
<p>可以得到： <span class="math display">\[
x_{approx}=U_{reduce}z
\]</span></p>
<h3 id="choosing-the-number-k-of-principal-component">Choosing the
number k of principal component</h3>
<ul>
<li>Average squared projection error：</li>
</ul>
<p><span class="math display">\[
\frac{1}{m}\Sigma_{i=1}^m\parallel x^{(i)}-x_{approx}^{(i)}\parallel ^2
\]</span></p>
<ul>
<li><p>Tptal variation in the data: <span class="math display">\[
\frac{1}{m}\Sigma_{i=1}^m\parallel x^{(i)}\parallel ^2
\]</span></p></li>
<li><p>Choose k to be smallest value so that <span class="math display">\[
\frac{\frac{1}{m}\Sigma_{i=1}^m\parallel
x^{(i)}-x_{approx}^{(i)}\parallel ^2}{\frac{1}{m}\Sigma_{i=1}^m\parallel
x^{(i)}\parallel ^2}\le0.01
\]</span> "99% of variance is
retained"。这个数值可以根据需求不同更换。</p></li>
<li><p>算法：</p>
<p>SVD函数返回的S矩阵为diagonal matrix。</p>
<p>For given k: k 递增，使得 <span class="math display">\[
1-\frac{\Sigma_{i=1}^{k}S_{ii}}{\Sigma_{i=1}^{n}S_{ii}}\le0.01
\]</span></p>
<p>选择最小的K满足上式。</p>
<p>根据性质：矩阵特征值之和等于主对角线元素之和，易证(6)式等价于(7)式。</p></li>
</ul>
<h3 id="advice-for-applying-pca">Advice for applying PCA</h3>
<ol type="1">
<li><p>Supervised learning speedup</p>
<p>压缩原始训练集数据的维度</p>
<p>Mapping <span class="math inline">\(x^{(i)}\rightarrow
z^{(i)}\)</span>should be defined by running PCA only on the trainning
set. This mapping can be applied as well to the examples <span class="math inline">\(x_{CV}^{(i)}\)</span> and <span class="math inline">\(x_{test}^{(i)}\)</span> in the cross validation
and test sets.</p></li>
<li><p>Compression</p>
<ul>
<li>Reduce memory/disk needed to store data</li>
<li>speed up learning algorithm</li>
</ul></li>
<li><p>Visualization</p></li>
<li><p>Bad use of PCA: To prevent overfitting</p>
<p>可能确实可以避免过拟合，但是不是解决过拟合的好方法。正确方法式加正则项(Use
regularization instead!)</p>
<p>For example: <img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210403175514854.png" alt="image-20210403175514854"></p></li>
<li><p>正确做法是先不用PCA，在原始数据跑一遍，如果不符合期望，再用PCA</p></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week09</title>
    <url>/Machine-Learning-Week09/</url>
    <content><![CDATA[<h1 id="anomaly-detection-异常检测">Anomaly Detection 异常检测</h1>
<h2 id="density-estimation">Density Estimation</h2>
<h3 id="problem-motivation">Problem Motivation</h3>
<p>异常检测主要用于unsupervised learning</p>
<p>Model：<span class="math inline">\(P(x_{test}&lt;\epsilon)\rightarrow
anomaly\)</span></p>
<p>​ <span class="math inline">\(P(x_{test}\ge \epsilon)\rightarrow
OK\)</span></p>
<p>也被用于检测账号被盗、生产、数据中心的电脑</p>
<h3 id="gaussian-distribution">Gaussian Distribution</h3>
<p>=Normal distribution</p>
<p>X ～ N (0, 1)，“distributed as”, 这里N的符号为"script N"</p>
<p>给定dataset，参数估计：via Maximum likelihood estimation
通常是m-1，对大数据集没影响 <span class="math display">\[
\mu = \frac{1}{m}\sum_{i=1}^mx^{(i)}\\
\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)^2
\]</span></p>
<h3 id="algorithm">Algorithm</h3>
<p>基本假设，各特征间相互独立 <span class="math display">\[
P(x)=\prod_{j=1}^np(x_j;\mu_j,\sigma_j^2)
\]</span> <img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210404220930233.png" alt="Anomaly detection algorithm"></p>
<h2 id="building-an-anomaly-detection-system">Building an Anomaly
Detection System</h2>
<h3 id="developing-and-evaluating-an-anomaly-detection-system">Developing
and Evaluating an Anomaly Detection System</h3>
<p>Training set: normal examples (可以有少量异常的混进来)</p>
<p>然后应用于cross calidation set 和 test set</p>
<ul>
<li><p>10000正常，20异常：</p>
<p>​ 训练集：6000正常</p>
<p>​ CV：2000正常，10异常</p>
<p>​ Test：2000正常，10异常</p></li>
</ul>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210405114004132.png" alt="Algorithm evaluation">
<figcaption aria-hidden="true">Algorithm evaluation</figcaption>
</figure>
<p>由于normal是0，所以预测0常常有更高的准确率，是skew的偏斜的。因此查准率accuracy不是很好的分析指标。</p>
<ul>
<li>选择ε时，可以试几个不同的，然后选择使得Cross
validation集中F1-score最大的，或者效果最好的。</li>
<li>有时还需要决定用什么特征</li>
</ul>
<h3 id="anomaly-detection-vs.-supervised-learning">Anomaly detection vs.
Supervised learning</h3>
<p>Anomaly detection:</p>
<ul>
<li>y=1非常少(0-20常见), y=0占比更大</li>
<li>有大量不同的异常类型，其他算法难于分辨各种类型</li>
<li>有未知异常类型</li>
</ul>
<p>supervised learning：</p>
<ul>
<li>大量的positive 和negative examples</li>
<li>有足够positive examples，并且未来的positive
example和训练集的相似</li>
<li>Spam</li>
</ul>
<h3 id="choosing-what-features-to-use">Choosing what features to
use</h3>
<ul>
<li>Non-gaussian feature</li>
</ul>
<p>绘制特征的直方图，看是否大致是个bell shaped
curve，如果不是，运用一些变换使得其看起来更加gaussian</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210405203939050.png" alt="take a log transformation">
<figcaption aria-hidden="true">take a log transformation</figcaption>
</figure>
<ul>
<li><p>Error analysis for anomaly detection</p>
<p>甄别失败时，new feature
可能有帮助，例如组合现有的特征形成新特征</p></li>
</ul>
<h2 id="multivariate-gaussian-distribution">Multivariate Gaussian
Distribution</h2>
<h3 id="multivariate-gaussian-distribution-1">Multivariate Gaussian
Distribution</h3>
<p>不同于之前的是，p(x)不再是各个特征概率的乘积。</p>
<p>Parameters: <span class="math inline">\(\mu \in \R^n\)</span>, <span class="math inline">\(\Sigma \in \R^{n\times n}\)</span> <span class="math display">\[
p(x;\mu,\Sigma)=
\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^2}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
\]</span> <span class="math inline">\(|\Sigma|\)</span>=determinant of
Σ</p>
<p><img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210405214331278.png" alt="examples"> <span class="math display">\[
\mu = \frac{1}{m}\sum_{i=1}^mx^{(i)}\\
\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T
\]</span></p>
<h3 id="anomaly-detection-using-the-multivariate-gaussian-distribution">Anomaly
Detection using the multivariate Gaussian distribution</h3>
<p>首先计算μ和Σ，然后计算p(x)，最后根据ε判断是否异常</p>
<ul>
<li><p>优点：自动捕捉特征间的相关性。m&gt;10n用起来才比较好，必须要m&gt;n，否则协方差矩阵不可逆。</p></li>
<li><p>缺点：计算量比较大。如果某个特征是其他的线性组合，那么协方差矩阵也不可逆，要注意避免。</p></li>
</ul>
<p>前述的模型：优点在于计算量小，但是要人工创造一些新特征来更好地分辨。m&lt;n的时候也能用。</p>
<p>若出现协方差矩阵不可逆：</p>
<ul>
<li>首先检查m, n大小</li>
<li>检查是否有redundant features，就是特征间是否线性独立</li>
</ul>
<h2 id="predicting-movie-ratings-recommendation-system">Predicting Movie
Ratings ——Recommendation System</h2>
<h3 id="problem-formulation">Problem Formulation</h3>
<p>r(i,j) 表示第i个电影，j用户是否打分；</p>
<p>y(i,j) 表示该用户对该电影的打分值 ；</p>
<h3 id="content-based-recommendations">Content Based
Recommendations</h3>
<p>设电影特征n个</p>
<p>For each user j, learn a parameter <span class="math inline">\(\theta^{(j)}\in\R^{n+1}\)</span>. Predict user j
as rating movie i with <span class="math inline">\((\theta^{(j)})^Tx^{(i)}\)</span> stars. （注意
<span class="math inline">\(x_0=1\)</span>）。用线性回归</p>
<p>To learn <span class="math inline">\(\theta^{(j)}\)</span>： <span class="math display">\[
\min_{\Theta^{(j)}}\frac{1}{2m^{(j)}}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2m^{(j)}}\sum_{k=1}^{n}(\theta_k^{(j)})^2
\]</span> Note: 这里 <span class="math inline">\(m^{(j)}\)</span>可以去掉，对结果没有任何影响</p>
<p>优化目标也可以为： <span class="math display">\[
J(\Theta^{(1)},...,\Theta^{(n_u)})=\min_{\Theta^{(1)},...,\Theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2
\]</span> Gradient descent update: <span class="math display">\[
\theta_{k}^{(j)}:=\theta_{k}^{(j)}-\alpha \sum_{i: r(i,
j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)
x_{k}^{(i)}\ (\text { for } k=0)
\]</span></p>
<p><span class="math display">\[
\theta_{k}^{(j)}:=\theta_{k}^{(j)}-\alpha\left(\sum_{i: r(i,
j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)
x_{k}^{(i)}+\lambda \theta_{k}^{(j)}\right)\ (\text { for } k \neq 0)
\]</span></p>
<p>k=0的时候不加正则项</p>
<h2 id="collaborative-filtering">Collaborative Filtering</h2>
<h3 id="collaborative-filtering-1">Collaborative Filtering</h3>
<ul>
<li><p>feature learning</p></li>
<li><p>Given <span class="math inline">\(\Theta^{(1)},...,\Theta^{(n_u)}\)</span>，to learn
<span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>;</p></li>
<li><p>算法类似7、8式</p></li>
<li><p><span class="math inline">\(用户的\Theta\rightarrow
电影特征x\rightarrow \Theta\rightarrow x\rightarrow...\)</span></p></li>
<li><p>每个用户都在帮助这个推荐系统更好地分类电影，从而更好地预测评分</p></li>
<li><p>算法：同时实现：</p>
<ol type="1">
<li><p>Given <span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>，estimate <span class="math inline">\(\theta^{(1)},...,\theta^{(n_u)}\)</span>： <span class="math display">\[
\min_{\Theta^{(j)}}\frac{1}{2m^{(j)}}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2m^{(j)}}\sum_{k=1}^{n}(\theta_k^{(j)})^2
\]</span></p></li>
<li><p>Given <span class="math inline">\(\theta^{(1)},...,\theta^{(n_u)}\)</span>，estimate
<span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>： <span class="math display">\[
\min _{x^{(1)}, \ldots, x^{\left(n_{m}\right)}} \frac{1}{2}
\sum_{i=1}^{n_{m}} \sum_{j: r(i,
j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i,
j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}}
\sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}
\]</span></p></li>
</ol>
<p>即：</p>
<p>Minimizing <span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>
and <span class="math inline">\(\theta^{(1)},...,\theta^{(n_u)}\)</span>
simultaneously： <span class="math display">\[
\min_{x^{(1)},...,x^{(n_m)}\\\theta^{(1)},...,\theta^{(n_u)}}J\left(x^{(1)},
\ldots, x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots,
\theta^{\left(n_{u}\right)}\right)
\]</span></p>
<p><span class="math display">\[
J\left(x^{(1)}, \ldots, x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots,
\theta^{\left(n_{u}\right)}\right)=\frac{1}{2} \sum_{(i, j): r(i,
j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i,
j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}}
\sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}+\frac{\lambda}{2}
\sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}
\]</span></p>
<blockquote>
<p>该算法下，无需设定 <span class="math inline">\(x_0=1\)</span>,
也没有<span class="math inline">\(\theta_0\)</span>，因为算法会自行选择参数。该算法下：<span class="math inline">\(\theta\in\R^n\)</span>，<span class="math inline">\(x\in\R^n\)</span>，n是特征数</p>
</blockquote></li>
<li><p>算法实现：</p>
<ol type="1">
<li>Initialize <span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>， <span class="math inline">\(\theta^{(1)},...,\theta^{(n_u)}\)</span> to small
random values</li>
<li>Minimize J using gradient descent (or an advanced optimization
algorithm)</li>
<li>For a user with parameters <span class="math inline">\(\theta\)</span> and a movie with (learned)
features x, predict a star rating of <span class="math inline">\(\theta^Tx\)</span></li>
</ol></li>
</ul>
<h2 id="low-rank-matrix-factorization">Low Rank Matrix
Factorization</h2>
<p>### Vectorization</p>
<ul>
<li><p>Given matrices X (each row containing features of a particular
movie) and Θ (each row containing the weights for those features for a
given user), then the full matrix Y of all predicted ratings of all
movies by all users is given simply by: <span class="math inline">\(Y =
X\Theta^T\)</span>.</p></li>
<li><p>Predicting how similar two movies i and j are can be done using
the distance between their respective feature vectors x. Specifically,
we are looking for a small value of <span class="math inline">\(||x^{(i)} - x^{(j)}||\)</span>.</p></li>
</ul>
<h3 id="mean-normalization">Mean Normalization</h3>
<p>If the ranking system for movies is used from the previous lectures,
then new users (who have watched no movies), will be assigned new movies
incorrectly. Specifically, they will be assigned θ with all components
equal to zero due to the minimization of the regularization term. That
is, we assume that the new user will rank all movies 0, which does not
seem intuitively correct. 新用户会被预测为0</p>
<p>We rectify this problem by normalizing the data relative to the mean.
First, we use a matrix Y to store the data from previous ratings, where
the ith row of Y is the ratings for the ith movie and the jth column
corresponds to the ratings for the jth user.</p>
<p>We can now define a vector <span class="math display">\[
\mu  = [\mu_1, \mu_2, \dots , \mu_{n_m}]
\]</span> such that <span class="math display">\[
\mu_i = \frac{\sum_{j:r(i,j)=1}{Y_{i,j}}}{\sum_{j}{r(i,j)}}
\]</span> Which is effectively the mean of the previous ratings for the
ith movie (where only movies that have been watched by users are
counted). We now can normalize the data by subtracting u, the mean
rating, from the actual ratings for each user (column in matrix Y):</p>
<p>As an example, consider the following matrix Y and mean ratings μ:
<span class="math display">\[
Y=\left[\begin{array}{cccc}
5 &amp; 5 &amp; 0 &amp; 0 \\
4 &amp; ? &amp; ? &amp; 0 \\
0 &amp; 0 &amp; 5 &amp; 4 \\
0 &amp; 0 &amp; 5 &amp; 0
\end{array}\right], \quad \mu=\left[\begin{array}{c}
2.5 \\
2 \\
2.25 \\
1.25
\end{array}\right]
\]</span> The resulting Y′ vector is: <span class="math display">\[
Y^{\prime}=\left[\begin{array}{cccc}
2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 \\
2 &amp; ? &amp; ? &amp; -2 \\
-2 .25 &amp; -2.25 &amp; 3.75 &amp; 1.25 \\
-1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25
\end{array}\right]
\]</span> Now we must slightly modify the linear regression prediction
to include the mean normalization term: <span class="math display">\[
(\theta^{(j)})^T x^{(i)} + \mu_i
\]</span> Now, for a new user, the initial predicted values will be
equal to the μ term instead of simply being initialized to zero, which
is more accurate.</p>
<blockquote>
<p>Note:
一般的均值归一化要除以range，即max-min，但是这里不需要，因为都有统一的scale</p>
</blockquote>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week10</title>
    <url>/Machine-Learning-Week10/</url>
    <content><![CDATA[<h1 id="large-scale-machine-learning">Large Scale Machine Learning</h1>
<h2 id="gradient-descent-with-large-datasets">Gradient Descent with
Large Datasets</h2>
<h3 id="learning-with-large-datasets">Learning with large datasets</h3>
<p>如何知道小m（size of dataset）也能产生很好的结果？绘制learning
curve（error关于m的曲线）</p>
<p>左图为high variance，说明增加m有助于结果变好；右图为high
bias，说明增加m无助于结果变好</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210409182113679.png" alt="左：high variance；右: high bias">
<figcaption aria-hidden="true">左：high variance；右: high
bias</figcaption>
</figure>
<h3 id="stochastic-gradient-descent-随机梯度下降">Stochastic gradient
descent 随机梯度下降</h3>
<p>Batch gradient descent:
前述传统的梯度下降算法。batch意味着每次都要用到所有m个例子</p>
<p>Stochastic gradient descent：每次iteration用1个例子</p>
<ol type="1">
<li><p>Randomly shuffle dataset</p></li>
<li><p>Repeat：</p>
<p>for i =1,..., m { <span class="math inline">\(\theta_j=\theta_j-\alpha(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\)</span>
}</p>
<blockquote>
<p>Note: 和Batch gradient
descent不同的是，无需每一步都要计算全部的training example</p>
</blockquote></li>
</ol>
<p>并不一定会达到global minimum，最后可能在全局最优附近徘徊。</p>
<p>上述过程重复的次数取决于数据集的大小，一般为10次，若数据集特别大，则1次。</p>
<h3 id="mini-batch-gradient-descent">Mini-Batch gradient descent</h3>
<p>每次iteration用b（mini-batch size）个例子，常用2-100</p>
<p>第i个例子，直到第i+b-1个例子</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210415223239627.png" alt="mini-batch gradient descent">
<figcaption aria-hidden="true">mini-batch gradient descent</figcaption>
</figure>
<p>只有当have a good vetorized implementation
，mini-batch才能比stochastic表现得更好。（并行运算）</p>
<h3 id="stochastic-gradient-descent-convergence">Stochastic gradient
descent convergence</h3>
<p>收敛性：</p>
<ul>
<li><figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210416223947732.png" alt="Algorithm">
<figcaption aria-hidden="true">Algorithm</figcaption>
</figure></li>
<li><p>更小的学习速率学习曲线更平滑，有可能会得到slightly
better结果。（cost关于no. of iteration 的曲线）</p></li>
<li><p>增加1000这个数值，会使得曲线更加平滑</p></li>
<li><p>如果学习曲线在上升，就要用更小的α</p></li>
<li><p>如果学习曲线几乎在一个水平上，那么就需要增加特征等</p></li>
<li><p>关于学习速率的选择：</p>
<ul>
<li>Learning rate α is typically held constant 在随机梯度下降中</li>
<li>Can slowly decrease α over time if we want θ to converge.
（例如：<span class="math inline">\(\alpha=\frac{const1}{iteration
Number+const2}\)</span>）
<ul>
<li>但是这里，两个参数的调整tune会很费时间，所以很少采用这种方法</li>
</ul></li>
</ul></li>
</ul>
<h2 id="advanced-topics">Advanced Topics</h2>
<p>### Online Learning</p>
<ul>
<li><p>连续的数据流：用户进入离开</p></li>
<li><p><span class="math inline">\(p(y=1|x;\theta)\)</span>其中x表示价格，可以用logistic
regression或者神经网络</p></li>
<li><p>这里考虑逻辑回归：</p>
<p>Repeat forever {</p>
<p>​ Get (x,y) corresponding to user.</p>
<p>​ Update θ using (x,y):</p>
<p>​ <span class="math inline">\(\theta_j:=\theta_j-\alpha(h_{\theta}(x)-y)x_j\)</span>
(j=0, ..., n)</p>
<p>}</p></li>
<li><p>这个算法可以用于学习用户喜好</p>
<ul>
<li>Choosing special offers to show user</li>
<li>Customized selection of news articles</li>
<li>product recommendation/search</li>
</ul></li>
</ul>
<h3 id="map-reduce-and-data-parallelism">Map reduce and data
parallelism</h3>
<p>Map reduce 可以处理更多数据。思想来自于Jeffrey Dean和Sanjay
Ghemawat。summation over the training set。即并行处理</p>
<ul>
<li>神经网络也同样可以并行计算前向和后向propagation</li>
<li>有时只需要向量化实现算法，不需要多核并行</li>
<li>Hadoop是开源map reduce</li>
<li>由于并行的latency，速度会少于N倍 的单核心</li>
</ul>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/Reading-List/</url>
    <content><![CDATA[<h1 id="machine-learning">1. Machine Learning</h1>
<h2 id="step-1">Step 1</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="">
Hands-On Machine learning with Scikit-Learn and Tensorflow</li>
<li><input type="checkbox" disabled="">
Python Machine Learning. Sebastian Raschka</li>
<li><input type="checkbox" disabled="">
Introduction to Statistical Learning with R</li>
<li><input type="checkbox" disabled="">
【参考书】机器学习. 周志华</li>
</ul>
<h2 id="step-2">Step 2</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="">
训练平台：Kaggle，天池大数据竞赛</li>
<li><input type="checkbox" disabled="">
Scikit-learn: machine learning in Python</li>
</ul>
<h2 id="step-3">Step 3</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="">
Deeplearning.ai —— Coursera</li>
<li><input type="checkbox" disabled="">
Deep learning - by Ian GoodFellow</li>
</ul>
<h2 id="step-4">Step 4</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="">
Elements of Statistical Learning</li>
<li><input type="checkbox" disabled="">
统计学习基础</li>
<li><input type="checkbox" disabled="">
订阅arxiv</li>
<li><input type="checkbox" disabled="">
关注顶会：ICML/NIPS/KDD</li>
</ul>
<h1 id="financial-engineering">2. Financial Engineering</h1>
<p><a href="https://www.zhihu.com/question/302531501/answer/534780501">金融工程专业需要哪些数学基础？
- 经管之家的回答 - 知乎</a></p>
]]></content>
      <tags>
        <tag>Learning Tips</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>This is my first blog!</title>
    <url>/This-is-my-first-bolg/</url>
    <content><![CDATA[<p>This is the first time for me to use Hexo to build my blog.</p>
<p>My major is <strong>Engineering Physics</strong> and
<strong>Financial Engineering</strong>.</p>
<p>Hope to share my experiences with you!</p>
<p><strong>Thank you</strong> for following my blog!</p>
]]></content>
      <tags>
        <tag>Essay</tag>
      </tags>
  </entry>
</search>
