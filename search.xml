<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Caculus Review</title>
    <url>/Caculus-Review/</url>
    <content><![CDATA[<h1 id="calculus-微积分复习">Calculus 微积分复习：</h1>
<p>Text Book: 《高等微积分教程》上下册.清华大学出版社</p>
<h2 id="基础知识">基础知识</h2>
<h3 id="第一章-实数列">第一章 实数列</h3>
<blockquote>
<p><strong>Definition 1 </strong>
数集A的上界、下界；有界的、无界的；上确界sup、下确界inf</p>
</blockquote>
<blockquote>
<p><strong>Definition 2</strong> 数列<span class="math inline">\(\{a_n\}\)</span>，如果 <span class="math inline">\(\forall\epsilon&gt;0\)</span>,<span class="math inline">\(\exists N\in \mathbb{N}\)</span>,使得当<span class="math inline">\(n&gt;N\)</span>时，就有<span class="math inline">\(|a_n-A|&lt;\epsilon\)</span>,则称数列<span class="math inline">\(\{a_n\}\)</span>有极限A,也称<span class="math inline">\(\{a_n\}\)</span>收敛于A，记为$_{n }{a_n}=A $</p>
</blockquote>
<ol type="1">
<li><p>收敛数列的性质：</p>
<blockquote>
<p>若数列收敛，则极限唯一</p>
</blockquote>
<blockquote>
<p>在一个收敛数列中任意添加或删去有限项，或者改变有限项的值，不会改变该数列的收敛性与极限值。</p>
</blockquote>
<blockquote>
<p>若数列收敛于A，则它的任何子列都收敛于A</p>
</blockquote>
<blockquote>
<p>收敛数列一定有界</p>
</blockquote>
<blockquote>
<p><strong>极限的保序性：</strong>若a数列收敛于A，b数列收敛于B，若A&gt;B，则存在N，使得当n&gt;N时，就有<span class="math inline">\(a_n&gt;b_n\)</span>。</p>
</blockquote>
<blockquote>
<p>极限的四则运算：</p>
</blockquote>
<blockquote>
<p><strong>夹逼原理</strong>（求极限）</p>
</blockquote></li>
<li><p>单调数列</p>
<blockquote>
<p><strong>单调收敛定理：</strong>(1)单调递增且有上界的数列必收敛。(2)单调递减且有下界的数列必收敛。`</p>
</blockquote>
<blockquote>
<p><strong>Stolz定理：</strong>(1)设数列{<span class="math inline">\(b_n\)</span>}严格单调递增且<span class="math inline">\(b_n\rightarrow +\infty\)</span>。如果<span class="math inline">\(\lim_{n\rightarrow \infty} \frac
{a_n-a_{n-1}}{b_n-b{n-1}}=A\)</span>，则<span class="math inline">\(\lim_{n\rightarrow\infty}\frac{a_n}{b_n}=A\)</span>；(2)设数列{<span class="math inline">\(b_n\)</span>}严格单调递减且<span class="math inline">\(\lim_{n\rightarrow\infty}b_n=\lim_{n\rightarrow\infty}a_n=0\)</span>。如果<span class="math inline">\(\lim_{n\rightarrow \infty} \frac
{a_n-a_{n-1}}{b_n-b{n-1}}=A\)</span>，则<span class="math inline">\(\lim_{n\rightarrow\infty}\frac{a_n}{b_n}=A\)</span>；</p>
</blockquote>
<blockquote>
<p><strong>欧拉常数：</strong><span class="math inline">\(\lim_{n\rightarrow\infty}(1+\frac{1}{2}+...+\frac{1}{n}-lnn)=\gamma\)</span>，γ=0.577...为欧拉常数</p>
</blockquote>
<blockquote>
<p><strong>柯西收敛原理：</strong>数列<span class="math inline">\(\{a_n\}\)</span>，如果 <span class="math inline">\(\forall\epsilon&gt;0\)</span>,<span class="math inline">\(\exists N\in \mathbb{N}\)</span>,使得对所有的<span class="math inline">\(n,m&gt;N\)</span>，都有<span class="math inline">\(|a_n-a_m|&lt;\epsilon\)</span>，称该数列为柯西列。数列收敛的充分必要条件是其为柯西列。</p>
</blockquote></li>
</ol>
<h3 id="第二章-函数">第二章 函数</h3>
<blockquote>
<p><strong>Definition：</strong>对于每个x，都有唯一的y与之对应，记为y=f(x)，把这个对应规则称为一个从X到Y的映射，记为<span class="math inline">\(f:X\rightarrow
Y\)</span>。X称之为定义域，Y称为值域</p>
</blockquote>
<blockquote>
<p><strong>邻域；空心邻域</strong></p>
</blockquote>
<blockquote>
<p><strong>函数极限的保序性</strong></p>
</blockquote>
<blockquote>
<p><strong>夹逼原理</strong>：若<span class="math inline">\(f(x)\le
g(x)\le h(x),x\in U(x_0,\rho)\)</span>，如果<span class="math inline">\(\lim_{x\rightarrow x_0}f(x)=\lim_{x\rightarrow
x_0}h(x)=A\)</span>，则<span class="math inline">\(\lim_{x\rightarrow
x_0}g(x)=A\)</span>。如果</p>
</blockquote>
<blockquote>
<p><strong>复合函数极限</strong></p>
</blockquote>
<blockquote>
<p><strong>函数极限的柯西收敛原理</strong></p>
</blockquote>
<blockquote>
<p>(1)f(x)是g(x)的<strong>高阶无穷小量</strong>：<span class="math inline">\(\lim_{x\rightarrow
x_0}\frac{f(x)}{g(x)}=0\)</span>，记作 <span class="math inline">\(f(x)=o(g(x))\ (x\rightarrow x_0)\)</span></p>
<p>(2)f(x)是g(x)的<strong>同阶无穷小量</strong>：<span class="math inline">\(\lim_{x\rightarrow x_0}\frac{f(x)}{g(x)}=c,\ c\ne
0\)</span></p>
<p>(3)f(x)是g(x)的<strong>等价无穷小量</strong>：<span class="math inline">\(\lim_{x\rightarrow
x_0}\frac{f(x)}{g(x)}=1\)</span>，记作 <span class="math inline">\(f(x)\sim o(g(x))\ (x\rightarrow x_0)\)</span></p>
<p>(4)f(x)是<strong>k阶无穷小量</strong>：<span class="math inline">\(\lim_{x\rightarrow
x_0}\frac{f(x)}{(x-x_0)^k}=c\)</span></p>
<p>Note: 无穷大量的定义同前三条。</p>
</blockquote>
<blockquote>
<p><strong>无穷小量间的等价关系：</strong></p>
<p>(1)<span class="math inline">\(sinx\sim x\)</span>，<span class="math inline">\(tanx\sim x\)</span></p>
<p>(2)<span class="math inline">\(1-cosx\sim \frac{1}{2}x^2\)</span></p>
<p>(3)<span class="math inline">\(ln(1+x)\sim x\)</span></p>
<p>(4)<span class="math inline">\(e^x-1\sim x, a^x-1\sim
xlna(a&gt;0)\)</span></p>
<p>(5)<span class="math inline">\((1+x)^n-1\sim ax\)</span></p>
</blockquote>
<blockquote>
<p>若 <span class="math inline">\(\lim_{x\rightarrow
x_0}f(x)=f(x_0)\)</span>，则称f在点 <span class="math inline">\(x_0\)</span>处连续。</p>
</blockquote>
<blockquote>
<p>若<span class="math inline">\(\lim_{x\rightarrow
x_0^+}f(x)=f(x_0)\)</span>，则称f在点 <span class="math inline">\(x_0\)</span>处右连续。</p>
</blockquote>
<blockquote>
<p>若<span class="math inline">\(\lim_{x\rightarrow
x_0^-}f(x)=f(x_0)\)</span>，则称f在点 <span class="math inline">\(x_0\)</span>处右连续。</p>
</blockquote>
<blockquote>
<p>f在点 <span class="math inline">\(x_0\)</span>处连续的充分必要条件是f在点 <span class="math inline">\(x_0\)</span>处既右连续又左连续。</p>
</blockquote>
<blockquote>
<p>f在点 <span class="math inline">\(x_0\)</span>处不连续，则称间断。三种情形：</p>
<p>1）可去间断点：极限存在，但在该点处没有定义或者<span class="math inline">\(\lim_{x\rightarrow x_0}f(x)\ne
f(x_0)\)</span>，即可以令该点的值等于极限，变为一个连续点</p>
<p>2）第一类间断点：左右极限都存在，但二者不相等。也包括可去间断点。</p>
<p>3）第二类间断点：至少有一个单侧极限不存在。</p>
</blockquote>
<blockquote>
<p>介值定理：设 <span class="math inline">\(f\in
C[a,b]\)</span>（f在闭区间上连续），<span class="math inline">\(f(a)\ne
f(b)\)</span>，则对介于f(a)与f(b)之间的每个数c，都存在 <span class="math inline">\(\xi\in (a,b)\)</span>，使 <span class="math inline">\(f(\xi)=c\)</span></p>
</blockquote>
<blockquote>
<p>闭区间上连续函数的有界性定理与最大最小值定理</p>
</blockquote>
<h3 id="第三章-函数的导数">第三章 函数的导数</h3>
<blockquote>
<p>导数的定义：设函数 <span class="math inline">\(f\)</span>在 <span class="math inline">\(x_0\)</span>点的某个领域内定义，如果 <span class="math inline">\(\lim_{\Delta x\to 0}\frac{f(x_0)+\Delta
x)-f(x_0)}{\Delta x}\)</span>存在，则称 <span class="math inline">\(f\)</span>在 <span class="math inline">\(x_0\)</span>点可导，称这个极限值为 <span class="math inline">\(f\)</span>在 <span class="math inline">\(x_0\)</span>点的导数，记作 <span class="math inline">\(f'(x_0)\)</span>。</p>
</blockquote>
<blockquote>
<p>可导则连续</p>
</blockquote>
<blockquote>
<p>右导数</p>
<p>左导数</p>
<p>导数存在的充分必要条件是左导数和右导数存在且相等</p>
</blockquote>
<blockquote>
<p>微分的定义：设函数 <span class="math inline">\(f\)</span>在 <span class="math inline">\(x_0\)</span>点的某个领域内定义，如果当自变量的增量
<span class="math inline">\(\Delta
x\)</span>充分小时，相应的函数值的增量 <span class="math inline">\(\Delta f(x_0)=f(x_0+\Delta
x)-f(x_0)\)</span>可以表示为：<span class="math inline">\(\Delta
f(x_0)=a\Delta x+o(\Delta x)(\Delta x \to \ 0)\)</span>，其中 <span class="math inline">\(a\)</span>为常数，则称 <span class="math inline">\(f\)</span>在 <span class="math inline">\(x_0\)</span>点可微，并称 <span class="math inline">\(df(x_0)=a\cdot\Delta x\)</span>为<span class="math inline">\(f\)</span>在 <span class="math inline">\(x_0\)</span>点处的微分。</p>
</blockquote>
<blockquote>
<p>可微的充要条件是可导</p>
</blockquote>
<blockquote>
<p>求导法则：</p>
<ul>
<li>导数的四则运算</li>
<li>复合函数求导数的链式法则：<span class="math inline">\(h'(x_0)=f'(\varphi(x_0))\varphi'(x_0)\)</span></li>
<li>反函数求导法则：设 <span class="math inline">\(f\)</span>在 <span class="math inline">\((a,b)\)</span>内严格单调且连续，<span class="math inline">\(x_0\in (a,b)\)</span>，<span class="math inline">\(f'(x_0)\ne 0\)</span>，则反函数 <span class="math inline">\(x=f^{-1}(y)\)</span>在 <span class="math inline">\(y_0=f(x_0)\)</span>处可导，并且 <span class="math inline">\((f^{-1})'(y_0)=\frac{1}{f'(x_0)}\)</span>。</li>
</ul>
</blockquote>
<blockquote>
<p>常见函数的导数</p>
</blockquote>
<figure>
<img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20210531112117849.png" alt="常见函数的导数">
<figcaption aria-hidden="true">常见函数的导数</figcaption>
</figure>
<blockquote>
<p>隐函数求导：对于 <span class="math inline">\(F(x,y)=0\)</span>方程两边同时对x求导，解出 <span class="math inline">\(y'\)</span>即可。</p>
</blockquote>
<blockquote>
<p>由参数方程形成的函数的求导方法：<span class="math inline">\(\frac{dy}{dx}=\frac{dy}{dt}\frac{dt}{dx}=\frac{dy}{dt}\frac{1}{\frac{dx}{dt}}=\frac{\psi'(t)}{\varphi'(t)}=\frac{y'(t)}{x'(t)}\)</span></p>
</blockquote>
<blockquote>
<p>高阶导数：若 <span class="math inline">\(f\)</span>在 <span class="math inline">\((a,b)\)</span>上n阶可导，则称 <span class="math inline">\(f\)</span>在 <span class="math inline">\((a,b)\)</span>上n阶可导；而当 <span class="math inline">\(f^{(n)}(x)在（a,b）\)</span>上连续时，称 <span class="math inline">\(f\)</span>在 <span class="math inline">\((a,b)\)</span>上为n阶连续可导（连续可微）。</p>
<ul>
<li><span class="math inline">\((f\cdot
g)^{(n)}(x)=\sum^{n}_{k=0}C_{n}^{k}f^{(k)}(x)g^{(n-k)}(x)\)</span>，莱布尼茨公式</li>
</ul>
</blockquote>
<h3 id="第四章-导数应用">第四章 导数应用</h3>
<blockquote>
<p>局部极值的定义：设函数<span class="math inline">\(f\)</span>在点
<span class="math inline">\(x_0\)</span>的某个邻域中有定义，如果$&gt;0
$使得当 <span class="math inline">\(|x-x_0|&lt;\rho\)</span>时，有 <span class="math inline">\(f(x)\ge f(x_0)\  (f(x)\le
f(x_0))\)</span>，则称<span class="math inline">\(f\)</span>在点 <span class="math inline">\(x_0\)</span>处取得（局部）极小值（极大值）</p>
</blockquote>
<blockquote>
<p>费马定理：设 <span class="math inline">\(x_0\)</span>是函数 <span class="math inline">\(f\)</span>的一个极值点，如果 <span class="math inline">\(f'(x_0)\)</span>存在，则 <span class="math inline">\(f'(x_0)=0\)</span></p>
</blockquote>
<blockquote>
<p>罗尔定理：设函数<span class="math inline">\(f\)</span>在闭区间<span class="math inline">\([a,b]\)</span>上连续，在开区间<span class="math inline">\((a,b)\)</span>内可导。如果<span class="math inline">\(f(a)=f(b)\)</span>，则存在<span class="math inline">\(\epsilon \in (a,b)\)</span>使得<span class="math inline">\(f'(\epsilon)=0\)</span>。</p>
</blockquote>
<blockquote>
<p>柯西中值定理：设函数f,
g都在闭区间[a,b]上连续，在开区间(a,b)内可导，并且在(a,b)中 <span class="math inline">\(g'(x)\ne 0\)</span>，则存在 <span class="math inline">\(\epsilon\in (a,b)\)</span>，使得：<span class="math inline">\(\frac{f'(\epsilon)}{g'(\epsilon)}=\frac{f(b)-f(a)}{g(b)-g(a)}\)</span></p>
</blockquote>
<blockquote>
<p>拉格朗日中值定理：是柯西中值定理中 <span class="math inline">\(g(x)=x\)</span>的一种特殊形式。</p>
<p>几何意义：必存在一点使得曲线在该点处的切线与函数两端点的连线平行</p>
<p>拉格朗日中值公式： <span class="math inline">\(f(x)-f(x_0)=f'(\epsilon)(x-x_0)，\epsilon\in
(x_0,x)\)</span>，(可用于证明不等式)</p>
</blockquote>
<blockquote>
<p>洛必达法则： 分子分母同时求导</p>
</blockquote>
<blockquote>
<p>泰勒公式：设函数 <span class="math inline">\(f\)</span>在点 <span class="math inline">\(x_0\)</span>处有n阶导数，则当 <span class="math inline">\(x\rightarrow x_0\)</span>时，<span class="math inline">\(f(x)=\sum_{k=0}^{n}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k+o((x-x_0)^n)\)</span></p>
<p>其中，<span class="math inline">\(R_n(x)=o((x-x_0)^n)\)</span>，称为皮亚诺余项形式</p>
</blockquote>
<blockquote>
<p>设函数 <span class="math inline">\(f\)</span>在区间 <span class="math inline">\([a,b]\)</span>上n+1阶可导，<span class="math inline">\(x_0\)</span>与<span class="math inline">\(x\)</span>为<span class="math inline">\([a,b]\)</span>中任意两点，<span class="math inline">\(P_n\)</span>为 <span class="math inline">\(f\)</span>在 <span class="math inline">\(x_0\)</span> 处的 n 阶泰勒多项式，则存在 <span class="math inline">\(\epsilon\)</span> 介于 <span class="math inline">\(x_0\)</span> 与 <span class="math inline">\(x\)</span> 之间，使得： <span class="math display">\[
R_n(x)=f(x)-P_n(x)=\frac{f^{(n+1)}(\epsilon)}{(n+1)!}(x-x_0)^{n+1}
\]</span></p>
</blockquote>
<blockquote>
<p>设设函数 <span class="math inline">\(f\)</span>在点 <span class="math inline">\(x_0\)</span>处有n阶导数，并且存在 n 阶多项式 <span class="math inline">\(Q_n(x)\)</span>，使得： <span class="math display">\[
f(x)= Q_n(x)+O((x-x_0)^N)(x\rightarrow x_0)
\]</span> 则<span class="math inline">\(Q_n(x)\)</span>即为 <span class="math inline">\(f\)</span>在点 <span class="math inline">\(x_0\)</span>处有n阶泰勒多项式<span class="math inline">\(P_n(x)\)</span></p>
</blockquote>
<ul>
<li><p>难以求导的可以化为几个简单函数的复合，分别泰勒展开，然后带入，忽略高阶项</p></li>
<li><p>泰勒公式可以用于求近似值、无穷小量阶的确定与求不定型极限以及有关函数机器导数的等式与不等式证明等问题中的应用。</p></li>
</ul>
]]></content>
      <tags>
        <tag>Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title>Issues of Icarus</title>
    <url>/Issues-of-Icarus/</url>
    <content><![CDATA[<h1 id="upload">Upload</h1>
<ul>
<li><p>每次上传都需要密码 _config.yml中将仓库地址改为:</p>
<p>git@github.com: yourname/yourname.github.io.git</p></li>
</ul>
<h1 id="widgets">Widgets</h1>
<h1 id="posts">Posts</h1>
<h2 id="content">Content</h2>
<ul>
<li><p><strong>Image not showing up</strong></p>
<p>Image should be under source/gallery/. Use Hexo's tag:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">{% img /gallery/image.jpg "image title" %\}</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>Excerpt / Read more</p>
<p>Put a &lt;! -- more --&gt;tag in the post.</p>
<p>Or in the front-matter, "excerpt: the content of the
excerpt"</p></li>
<li><p><a href="https://www.icode9.com/content-4-828659.html">Image host
via github</a></p></li>
</ul>
<h2 id="formula">Formula</h2>
<ul>
<li><p>无法正确渲染</p>
<p><a href="https://blog.csdn.net/yexiaohhjk/article/details/82526604?utm_medium=distribute.pc_relevant_bbs_down.none-task-blog-baidujs-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_bbs_down.none-task-blog-baidujs-1.nonecase">完美解决方案</a></p>
<p>上述方案无法解决类似*$这样的问题，如何解决？？</p></li>
<li><p>hexo公式换行的问题？？</p></li>
<li><p>公式内的“\#”无法解析，或其他公式渲染问题，报"Template render
error"错误 尽量不要用#符号</p></li>
</ul>
]]></content>
      <tags>
        <tag>Issues</tag>
      </tags>
  </entry>
  <entry>
    <title>Citadel Central Regional Datathon 2021</title>
    <url>/Citadel-Central-Datathon-2021/</url>
    <content><![CDATA[<h1 id="whos-still-smoking">Who's still smoking?</h1>
<p>The goal is to use tobacco related data in order to discover and
analyze patterns associated with tobacco usage.</p>
<h2 id="prize">Prize</h2>
<p><a href="https://www.credential.net/4c9232c5-10b9-4f7e-a5ab-0d04a2f7a317">2nd
Place Winner of Central Regional Datathon 2021</a></p>
<h2 id="result">Result</h2>
<p><a href="https://github.com/mtanghu/Citadel-Central-Datathon-Fall21">code</a></p>
<div class="pdf-container" data-target="../pdf/Team_2_report.pdf" data-height="1000px"></div>
]]></content>
      <tags>
        <tag>Project</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week01</title>
    <url>/Machine-Learning-Week01/</url>
    <content><![CDATA[<h2 id="week-1">Week 1</h2>
<ol type="1">
<li><p><strong>Supervised Learning 监督学习</strong></p>
<ul>
<li>分类问题 classification problem
<ul>
<li>discrete output（0，1）</li>
<li>features</li>
<li>特征无限多的时候可以用SVM支持向量机</li>
</ul></li>
<li>回归问题 regression problem
<ul>
<li>continuous output</li>
</ul></li>
</ul></li>
<li><p><strong>Unsupervised Learning 无监督学习</strong></p>
<ul>
<li><p><strong>clustering algorithm 聚类算法</strong></p>
<ul>
<li><p>dataset（no label） find structure;根据数据内部关系分类</p></li>
<li><p>E.g. 网站分类；基因分类；social network analysis；market
segmentation；Astronomical data analysis</p></li>
<li><p><strong>Cocktail party
problem</strong>：混杂的声音中分离出两种声音</p>
<figure class="highlight matlab"><table><tbody><tr><td class="code"><pre><span class="line">[W,s,v] = svd((<span class="built_in">repmat</span>(sum(x.*x,<span class="number">1</span>),<span class="built_in">size</span>(x,<span class="number">1</span>),<span class="number">1</span>).*x)*x')</span><br></pre></td></tr></tbody></table></figure>
<p>*SVD: singular value decomposition 奇异值分解，求解线性方程</p>
<p>octave做原型，然后迁移到C或JAVA</p></li>
</ul></li>
</ul></li>
<li><p><strong>Model and Cost Function 模型和成本函数</strong></p>
<ul>
<li><p><strong>线性回归算法 linear regression</strong></p>
<ul>
<li><p>training set 训练集</p></li>
<li><p>m - 训练样本数</p>
<p>x - 输入变量</p>
<p>y - 输出变量</p>
<p><span class="math inline">\((x^{i},y^{i})\)</span> - i training
example</p>
<p><span class="math inline">\(\theta\)</span> - Parameters</p>
<p>h - hypothesis，x映射到y的函数 ：<span class="math inline">\(h_{\theta}(x)=\theta_0+\theta_1x\)</span>；缩写即<span class="math inline">\(h(x)\)</span></p>
<p>不一定都是线性方程</p></li>
</ul></li>
<li><p><strong>Cost Function 成本函数</strong></p>
<ul>
<li><p>度量函数拟合的程度</p></li>
<li><p>平方误差成本函数：适用于线性回归</p>
<p>Hypothesis: <span class="math display">\[\min
\limits_{\theta_0\theta_1}\frac {1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</span></p>
<p>Cost function: <span class="math display">\[J(\theta_0,\theta_1)=\frac {1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</span> ----- (Squared
Error Function)</p>
<p>contour plot: 轮廓图；等高线图</p></li>
</ul></li>
</ul></li>
<li><p><strong>Parameter Learning</strong></p>
<ul>
<li><p>Gradient descent 梯度下降算法</p>
<p>不断改变<span class="math inline">\(\theta\)</span>使得J函数变小，得到局部最优点local
optimum，convergence收敛</p>
<p><span class="math display">\[\theta_j:=\theta_j-\alpha\frac
{\partial} {\partial \theta_j}J(\theta_0,\theta_j)\quad(for\ j=0\ and\
j=1)\]</span></p>
<p><span class="math inline">\(\alpha\)</span>是学习速率 learning
rate</p>
<ul>
<li><p>注意编程中的赋值后会覆盖，所以要桥接一下</p></li>
<li><p>partial derivatives 偏导数；derivatives 导数</p></li>
<li><p>不需要调整α因为导数值越来越小</p></li>
<li><p>Batch Gradient Descent
批量梯度下降：每step都用到全部的训练样本</p></li>
<li><p>高等线性代数：normal equations
method正规方程，大数据时梯度下降更好用</p></li>
</ul></li>
</ul></li>
</ol>
<h1 id="matrices-and-vectors">Matrices and Vectors</h1>
<ol type="1">
<li>Matrix: rows*columns
<ul>
<li><span class="math inline">\(\mathbb{R} ^{2\times3}\)</span> , <span class="math inline">\(A_{ij}\)</span>,</li>
</ul></li>
<li>Vector: n*1 matrix
<ul>
<li><span class="math inline">\(\mathbb{R}^4\)</span></li>
</ul></li>
<li><span class="math inline">\(A\times B\ne B\times A\)</span></li>
<li>Associative交换律</li>
<li>Identity Matrix： <span class="math inline">\(I\)</span>, <span class="math inline">\(n\times n\)</span>
<ul>
<li><span class="math inline">\(\begin{bmatrix}1&amp;\cdots&amp;0\\\vdots&amp;\ddots&amp;\vdots\\0&amp;\cdots&amp;1\end{bmatrix}\)</span></li>
<li><span class="math inline">\(A\cdot I=I\cdot A\)</span></li>
<li>I = eye(2)</li>
</ul></li>
<li>Inverse 矩阵的逆：<span class="math inline">\(A(A^{-1})=A^{-1}A=I\)</span>
<ul>
<li>pinv（A）</li>
<li><span class="math inline">\(A_{m\times m}\)</span></li>
<li>0矩阵=奇异矩阵，没有逆矩阵的矩阵是奇异矩阵</li>
<li>高斯消元法求逆：
<ul>
<li>“某行乘以一个数后加到另一行”、“某两行互换位置”、“某行乘以某一个数”，这三种以行做运算的方法</li>
<li>行变换或列变换都可以</li>
<li>增广矩阵<span class="math inline">\(B=[A|I]=\begin{vmatrix} A_{11}
&amp; A_{12} &amp; A_{13}&amp;1&amp;0&amp;0 \\ A_{21} &amp;
A_{22}&amp;A_{23}&amp;0&amp;1&amp;0\\A_{31} &amp; A_{32} &amp;
A_{33}&amp;0&amp;0&amp;1 \end{vmatrix} \Rightarrow \begin{vmatrix}
1&amp;0&amp;0&amp;A_{11}^{'} &amp; A_{12}^{'} &amp;
A_{13}^{'} \\0&amp;1&amp;0&amp;A_{21}^{'} &amp; A_{22}^{'}
&amp; A_{23}^{'}\\0&amp;0&amp;1&amp;A_{31}^{'} &amp;
A_{32}^{'} &amp; A_{33}^{'} \end{vmatrix}\)</span></li>
</ul></li>
<li>待定系数法</li>
<li>伴随矩阵法$A^* <span class="math inline">\(，\)</span>A^{-1}=$
<ul>
<li>将矩阵A元素<span class="math inline">\(a_{ij}\)</span>所在的第i行j列元素划去后剩余元素按照原来顺序组成n-1阶矩阵所确定的行列式成为元素<span class="math inline">\(a_{ij}\)</span>的余子式，记为<span class="math inline">\(M_{ij}\)</span>，称<span class="math inline">\(A_{ij}=(-1)^{i+j}M_{ij}\)</span>为元素<span class="math inline">\(a_{ij}\)</span>的代数余子式</li>
<li><span class="math inline">\(A^*\)</span>的第i行j列元素为上面的<span class="math inline">\(A_{ij}\)</span></li>
</ul></li>
<li>LU分解法A=LU，<span class="math inline">\(A^{-1}=U^{-1}L^{-1}\)</span></li>
<li>SVD分解法</li>
<li>QR分解法</li>
</ul></li>
<li>Transpose 矩阵的转置：<span class="math inline">\(A^T\)</span></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week02</title>
    <url>/Machine-Learning-Week02/</url>
    <content><![CDATA[<h1 id="multiple-feature">Multiple Feature</h1>
<h2 id="multivariate-linear-regression">Multivariate Linear
Regression</h2>
<ol type="1">
<li><p><span class="math inline">\(h_{\theta}(x)=\theta^{T}X\)</span></p>
<p><span class="math inline">\(h_θ(x)=\begin{bmatrix}θ_0&amp;θ_1&amp;...&amp;θ_n\end{bmatrix}\begin{bmatrix}x_0\\x_1\\⋮\\x_n\end{bmatrix}=θ^Tx\)</span></p></li>
<li><p>Gradient Descent for Multiple Variables</p>
<p>Cost function: <span class="math display">\[J(\theta_0,\theta_1...\theta_n)=\frac {1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</span></p>
<p><span class="math display">\[\theta_j:=\theta_j-\alpha\frac
{\partial} {\partial \theta_j}J(\theta)=\theta_j-\alpha\frac {1}{m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\quad(for\
j=0...n)\]</span></p></li>
<li><p>Feature Scaling</p>
<p>Make sure features are on a similar scale</p>
<p>避免梯度下降一个维度值过大导致下降速率很慢</p>
<ul>
<li>Get every feature into approximately a <span class="math inline">\(-1\le x_i\le 1\)</span> range</li>
<li>大约在这个范围附近就行，例如: <span class="math inline">\(-2\le
x_i\le 2\)</span>也可以</li>
<li>方法一：都除以最大值<span class="math inline">\(x_{max}\)</span></li>
<li>均值归一化 Mean normalization：
<ul>
<li><span class="math display">\[\frac{x_i-\mu_i}{s_{i}}\]</span></li>
<li>使得均值为0</li>
<li><span class="math inline">\(\mu\)</span>是训练集x某特征的均值，s可以是标准差，一般<span class="math inline">\(s=max-min\)</span>即可</li>
<li>特征缩放不用太精确，只是为了让梯度下降更快而已</li>
</ul></li>
</ul></li>
<li><p>确定α</p>
<ul>
<li>如果α过大，则可能出现 J 值增大或者不收敛</li>
<li>让α尽量小，使得每次迭代 J 值都在减小</li>
<li>自动收敛测试：可以设定一个值比如0.001，如果每一步J变化小于这个值即可认为收敛，但是选择合适的值很困难</li>
<li>所以可以采用：代价函数 J
随迭代步数变化曲线，可以帮助判断梯度下降算法是否收敛</li>
</ul></li>
<li><p>多项式回归 Polynomial Regression</p>
<ul>
<li>拟合复杂函数</li>
<li>将x进行平方、开根等处理。例如：<span class="math inline">\(x_3=x_1^3\)</span></li>
<li>注意新特征要进行均值归一化使得各个特征之间差距不至于过大</li>
</ul></li>
<li><p>Normal equation 正规方程</p>
<ul>
<li>solve for θ analytically，θ的解析解法</li>
<li><span class="math display">\[J(\theta_0,\theta_1...\theta_n)=\frac
{1}{2m}\sum \limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</span>
，令<span class="math inline">\(\frac {\partial} {\partial
\theta_j}J(\theta)=0\)</span>(偏微分)
<ul>
<li><span class="math inline">\(X=\begin{bmatrix}x_0&amp;x_1&amp;...&amp;x_n\end{bmatrix}\)</span>，x代表特征的列向量集合</li>
<li>则可以推出：<span class="math inline">\(\theta=(X^TX)^{-1}X^Ty\)</span>，即正规方程</li>
</ul></li>
<li>matlab：pinv(X‘*X)*X'*y</li>
<li>用正规方程就不用变量归一化</li>
<li>优点：不用选择α；不用迭代</li>
<li>缺点：n较大时，算逆矩阵会比较慢，n&gt;10000，复杂度大约在<span class="math inline">\(O(n^3)\)</span></li>
<li>正规方程的不可逆性 Normal Equation Non-invertibility
<ul>
<li>pinv可以求伪逆，inv求逆</li>
<li>如果<span class="math inline">\(X^TX\)</span>是不可逆的：
<ul>
<li>特征之间是相关的</li>
<li>特征多于样本数</li>
<li>解决方法：删除多余特征或正则化</li>
</ul></li>
</ul></li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week03</title>
    <url>/Machine-Learning-Week03/</url>
    <content><![CDATA[<h1 id="machine-learning">Machine Learning</h1>
<h2 id="week-03-classification-and-representation-overfitting">Week 03
Classification and Representation | Overfitting</h2>
<h3 id="classification-and-representation-分类和表征">1. Classification
and Representation 分类和表征</h3>
<ol type="1">
<li><p><strong>Logistic Regression算法应用于Classification</strong></p>
<ul>
<li><p><strong>binary classification problem </strong>二元分类问题</p>
<ul>
<li>函数值是离散的discrete</li>
</ul></li>
<li><p><strong>Hypothesis Representation</strong> 假设陈述</p>
<ul>
<li><p>因为需要<span class="math inline">\(y∈\{0,1\}\)</span>，所以要使得<span class="math inline">\(0\le h_\theta(x)\le1\)</span>，令<span class="math display">\[h_\theta(x)=g(\theta^Tx),\
z=\theta^Tx,\  g(x)=\frac {1}{1+e^{-z}}\]</span></p></li>
<li><p><span class="math inline">\(h_\theta(x)\)</span>表示输出为1的概率: <span class="math display">\[
h_\theta(x)=P(y=1|x;\theta)=1-P(y=0|x;\theta)
\]</span></p></li>
</ul></li>
</ul></li>
<li><p><strong>Decision Boundary</strong> 决策边界：分类的边界 <span class="math display">\[
h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2)
\]</span></p>
<ul>
<li><p>非线性(non-linear)决策边界</p>
<p>多项式是决策边界的属性，不是训练集的属性，训练集决定参数θ的值</p></li>
</ul></li>
</ol>
<h3 id="logistic-regression-model-逻辑回归模型">2. Logistic Regression
Model 逻辑回归模型</h3>
<ol type="1">
<li><p><strong>Cost Function</strong></p>
<ul>
<li><p>convex 凸函数，可以收敛到全局最小</p></li>
<li><p>不能使用linear regression的Cost function，因为这会使得logistic
function的成本函数变成波浪形，形成许多局部最优，就无法形成凸函数</p></li>
<li><p>因此替换成本函数为： <span class="math display">\[
J(\theta)=\frac {1}{m}\sum
\limits_{i=1}^mCost(h_\theta(x^{(i)}),y^{(i)}) \\
Cost(h_\theta(x),y)=
\begin{cases}-log(h_\theta(x)) &amp; \quad if\ y=1 \\
-log(1-h_\theta(x)) &amp; \quad if\ y=0
\end{cases}
\]</span></p>
<blockquote>
<p><strong>The cost function in this way guarantees that J(θ) is convex
for logistic regression.</strong></p>
</blockquote></li>
<li><p>这里的Cost function等价于： <span class="math display">\[
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
\]</span></p></li>
<li><p>带入到代价函数 J(θ) 中</p></li>
<li><p>使用<strong>==最大似然估计法==</strong>可以得出这个代价函数，最大似然估计可以有效地为不同模型找到参数数据。</p></li>
<li><p>凸性是这个函数优秀地属性之一</p></li>
<li><p>接下来：为了拟合θ，就要最小化 J(θ)</p></li>
</ul></li>
<li><p><strong>Gradient Descent 梯度下降法</strong> <span class="math display">\[
\theta_j:=\theta_j-\alpha\frac {\partial} {\partial
\theta_j}J(\theta)=\theta_j-\alpha\frac {1}{m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\quad(for\ j=0...n)
\]</span></p>
<ul>
<li><p>选择学习速率：绘制 J(θ)
关于迭代次数的函数，使得每一次迭代函数值都在下降</p></li>
<li><p>可以使用向量化实现算法 <span class="math display">\[
\begin{array}{lc}
h=g(\theta^TX)\\
J(\theta)=\frac{1}{m}\cdot(-y^Tlog(h)-(1-y)^Tlog(1-h))\\
\text{注：这里X是$（n+1）\times1$维列向量}
\end{array}
\]</span> 最终得到： <span class="math display">\[
\theta:=\theta-\frac{\alpha}{m}X(g(\theta^TX)-\vec{y})
\]</span></p></li>
</ul></li>
</ol>
<ul>
<li>特征值缩放也同样适用该模型</li>
</ul>
<blockquote>
<p><strong>关于代价函数的最大似然估计法相关数学推导参见<a href="https://www.cnblogs.com/ranjiewen/p/5967496.html">这里</a></strong></p>
<p><strong>==统计学是机器学习的数学基础==</strong></p>
</blockquote>
<ol start="3" type="1">
<li><p>Optimization algorithm 优化算法</p>
<ul>
<li><p>例如：</p>
<ul>
<li><p>Conjugate gradient 共轭梯度法</p></li>
<li><p>BFGS变尺度法</p></li>
<li><p>L-BFGS限制变尺度法</p>
<p>优点：无需手动选择α（线性搜索法每一步都在改变α）；比梯度下降法收敛更快</p>
<p>缺点：算法更复杂、难理解</p></li>
</ul></li>
<li><p>octave中的优化算法表达：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">function [jVal, gradient] = costFunction(theta)</span><br><span class="line">  jVal = [...code to compute J(theta)...];</span><br><span class="line">  gradient = [...code to compute derivative of J(theta)...];</span><br><span class="line">end</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">options = optimset('GradObj', 'on', 'MaxIter', 100);</span><br><span class="line">initialTheta = zeros(2,1);</span><br><span class="line">   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);</span><br></pre></td></tr></tbody></table></figure></li>
</ul></li>
</ol>
<h3 id="multiclass-classification-多类别分类问题">3. Multiclass
Classification 多类别分类问题</h3>
<ol type="1">
<li><p><strong>one-vs-all 一对多分类算法</strong></p>
<p>划分为多个二分类问题，求多个h(x)。应用于预测集时，将x分类到h最大的那个集里。</p>
<p><span class="math inline">\(h^{(i)}_\theta(x)=P(y=i|x;\theta),\quad
prediction=\max\limits_{i}(h^{(i)}_\theta(x))\)</span></p></li>
</ol>
<h3 id="overfitting-过拟合">4. Overfitting 过拟合</h3>
<ol type="1">
<li><p>underfitting ——
高bias偏差（Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，即算法本身的拟合能力）</p>
<p>Overfitting ——
高variance方差（variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。反应预测的波动情况。）</p>
<blockquote>
<p>关于方差与偏差的意义参见<a href="https://blog.csdn.net/u012197749/article/details/79766317">这里</a></p>
</blockquote>
<ol start="2" type="1">
<li>解决过拟合：</li>
</ol>
<ul>
<li>减少特征的数量
<ul>
<li>手动选择保留的特征</li>
<li>模型选择算法model selection
algorithm，可以自动决定要保留的变量和要剔除的变量</li>
<li>缺点：剔除了一部分信息</li>
</ul></li>
<li>regularization 正则化
<ul>
<li>保留所有变量，减少变量θ的magnitude</li>
<li>在有很多变量时很有用</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li>正则化</li>
</ol>
<ul>
<li>在代价函数加一个正则化项，使得参数θ尽可能小，排除bias偏差项</li>
</ul>
<p><span class="math display">\[
\min\limits_\theta\frac{1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\sum\limits_{j=1}^n\theta_j^2
\]</span></p>
<ul>
<li><p>λ是正则化参数regularization parameter</p></li>
<li><p>应用于梯度下降法：（注：<span class="math inline">\(\theta_0\)</span>一定不要正则化！） <span class="math display">\[
\theta_j:=\theta_j-\alpha\frac {\partial} {\partial
\theta_j}J(\theta)=\theta_j-\alpha[\frac {1}{m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]\quad(for\
j=1...n)
\]</span></p></li>
<li><p>应用于正规方程normal equation： <span class="math display">\[
\theta=(X^TX+\lambda\cdot L)^{-1}X^Ty\\
where \quad L=\begin{bmatrix}0\\\
&amp;1\\\  &amp;\  &amp;1\\\  &amp;\  &amp;\  &amp;\ddots\\\  &amp;\  &amp;\  &amp;\  &amp;1\\\end{bmatrix}
\]</span></p>
<ul>
<li>Non-invertibility 不可逆性 suppose <span class="math inline">\(m\le
n\)</span>, 特征多于例子时，<span class="math inline">\(\theta=(X^TX)^{-1}X^Ty\)</span> 中，<span class="math inline">\(X^TX\)</span>是不可逆的（或奇异的），即为退化矩阵（degenerate）
但是加入λL
后，解决了上述问题，公式（9）括号内的矩阵是可逆的（前提是λ&gt;0）</li>
</ul></li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Read Papers</title>
    <url>/How-to-Read-Papers/</url>
    <content><![CDATA[<h1 id="how-to-read-papers">How to Read Papers?</h1>
<h2 id="resources">Resources</h2>
<ul>
<li>Arxiv</li>
<li>Blog Posts</li>
<li>Text Books</li>
</ul>
<h2 id="prepare">Prepare</h2>
<ul>
<li>列出待读论文，每一篇列一行，表示从0-100的阅读进度</li>
<li>如果有论文不是想要的，就删掉</li>
<li>如果有新的论文就加进来</li>
</ul>
<h2 id="methods">Methods</h2>
<ul>
<li>第一遍：只看标题，摘要，图表</li>
<li>第二遍：前言，结语，图表</li>
<li>第三遍：论文主体</li>
</ul>
]]></content>
      <tags>
        <tag>Learning Tips</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week04</title>
    <url>/Machine-Learning-Week04/</url>
    <content><![CDATA[<h2 id="neural-networks-representation">Neural Networks：
Representation</h2>
<h3 id="non-linear-hypotheses">1. Non-linear Hypotheses</h3>
<p>特征多，有高次项</p>
<h3 id="neural-networks">2. Neural Networks</h3>
<ol type="1">
<li><p>Model representation</p>
<ul>
<li><p>bias unit偏置项（=1）</p></li>
<li><p>sigmoid activation function S型激励函数</p></li>
<li><p>input layer —— hidden layer —— output layer</p></li>
<li><p><span class="math display">\[ a_i^{(j)}= "activation"\
of\ unit\ i\ in\ layer\  j \]</span></p>
<p><span class="math inline">\(\Theta^{(j)}=\)</span> matrix of weights
controlling function mapping from layer j to layer j+1
，即参数矩阵（波矩阵、权重矩阵）</p>
<p>如果一个网络在 j 层有<span class="math inline">\(s_j\)</span>个单元，j+1层有<span class="math inline">\(s_{j+1}\)</span>个单元，那么<span class="math inline">\(\Theta^{(j)}\)</span>的矩阵维度是<span class="math inline">\(s_{j+1}\times(s_j+1)\)</span>，因为要加上bias
unit</p>
<p><img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/屏幕截图 2020-11-09 112621.png" alt="神经网络" style="zoom:50%;">
<span class="math display">\[
\begin{array}{r}
a_{1}^{(2)}=g\left(\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)}
x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)}
x_{3}\right)=g(z_1^{(2)}) \\
a_{2}^{(2)}=g\left(\Theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)}
x_{1}+\Theta_{22}^{(1)} x_{2}+\Theta_{23}^{(1)}
x_{3}\right)=g(z_2^{(2)})  \\
a_{3}^{(2)}=g\left(\Theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)}
x_{1}+\Theta_{32}^{(1)} x_{2}+\Theta_{33}^{(1)} x_{3}\right)
=g(z_3^{(2)}) \\
h_{\Theta}(x)=a_{1}^{(3)}=g\left(\Theta_{10}^{(2)}
a_{0}^{(2)}+\Theta_{11}^{(2)} a_{1}^{(2)}+\Theta_{12}^{(2)}
a_{2}^{(2)}+\Theta_{13}^{(2)} a_{3}^{(2)}\right)
\end{array}
\]</span></p></li>
<li><p>即前向传播（forward propagation）</p></li>
</ul></li>
</ol>
<h3 id="applications">3. Applications</h3>
<ol type="1">
<li>与运算（-30，20，20）</li>
<li>或运算（-10，20，20）</li>
<li>非运算（10，-20）</li>
<li>XNOR运算，同或运算（相同为1，否则为0）：两层神经网络</li>
<li>Multiclass Classification，如果4个分类器结果则例如[0,0,1,0]</li>
</ol>
<p>​</p>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Win a Data Science Competition</title>
    <url>/How-to-Win-a-Data-Science-Competition-Learn-from-Top-Kagglers/</url>
    <content><![CDATA[<h1 id="how-to-win-a-data-science-competition-learn-from-top-kagglers">How
to Win a Data Science Competition: Learn from Top Kagglers</h1>
<p>linear model：适合高维稀疏线性可分空间</p>
<p>decision tree: 难以捕捉线性相关性，分成boxes</p>
<p>k-NN：</p>
<p>NN：非线性边界</p>
<h2 id="tips">Tips</h2>
<h3 id="feature-preprocessing">1. feature preprocessing</h3>
<ul>
<li><p>类别数据：</p>
<ul>
<li>one hot encoding</li>
<li>随机森林不用transform类别数据</li>
</ul></li>
<li><p>numeric features：</p></li>
<li><p>scale</p>
<ul>
<li>tree model不用变形数据</li>
<li>其他可能要变
<ul>
<li>scale可能影响梯度下降有效性</li>
<li>minmaxscaling</li>
<li>standardscaler</li>
<li>有时可以适当放缩feature，来看模型有没有变好，这样使得那个feature更重要</li>
</ul></li>
</ul></li>
<li><p>outlier：</p>
<ul>
<li>可以选择上下界（按照百分比）
<ul>
<li>winsorization：对financial data</li>
<li>可以看直方图</li>
</ul></li>
</ul></li>
<li><p>rank</p>
<ul>
<li>线性模型、knn、nn对于这个处理方式有效</li>
<li>scipy.stats.rankdata</li>
</ul></li>
<li><p>对于non-tree based models：</p>
<ul>
<li>log transform</li>
<li>开根号：np.sqrt(x+2/3)</li>
<li>减少异常值，增大0附近差异</li>
<li>可以混合不同模型（基于不同的处理方法）</li>
</ul></li>
<li><p>feature generation：</p>
<ul>
<li>加减乘除</li>
<li>取小数部分</li>
<li>要理解数据</li>
</ul></li>
<li><p>categorical features/ ordinal features</p>
<ul>
<li><p>label encoding</p>
<ul>
<li><p>对tree友好</p></li>
<li><p>要转化为数字</p>
<ul>
<li><p>按字母排序：sklearn.preprocessing.LabelEncode</p></li>
<li><p>按出现顺序：pandas.factorize</p></li>
<li><p>转化为出现频率作为特征（常用于tree）</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">encoding = titanic.groupby('Embarked').size()</span><br><span class="line">encoding = encoding/len(titanic)</span><br><span class="line">titanic['enc'] = titanic.Embarked.map(encoding)</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>rank</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">from scipy.stats import rankdata</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>one hot encoding:常用于non-tree</p>
<ul>
<li>pandas.get_dummies</li>
<li>sklearn.preprocessing.OneHotEncoder</li>
<li>可能对tree不友好</li>
<li>需要稀疏矩阵（word）</li>
<li>组合各种类别，形成多种类：如等级加性别</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Datetime</p>
<ul>
<li>周期</li>
<li>时间点</li>
<li>时间跨度（距离过去，距离未来）</li>
</ul></li>
<li><p>坐标coordination</p>
<ul>
<li>单坐标
<ul>
<li>区域中心</li>
<li>特殊位置</li>
</ul></li>
<li>统计值
<ul>
<li>区域统计值</li>
<li>距离</li>
</ul></li>
<li>旋转坐标可能有奇效</li>
</ul></li>
<li><p>missing values</p>
<ul>
<li>fillna
<ul>
<li>-999，-1
<ul>
<li>注意在后面均一化时会有重大影响</li>
<li>feature generation前 fillna可能有问题</li>
</ul></li>
<li>中位数、均值</li>
<li>reconstruct value
<ul>
<li>时间序列：estimation</li>
<li>其他的很难估计</li>
</ul></li>
</ul></li>
<li>添加一个isNull的feature，神经网络和tree</li>
<li>可以把outlier as missing values</li>
<li>xgboost可以处理nan</li>
</ul></li>
<li><p>text</p>
<ul>
<li><p>bag of words</p></li>
<li><p>sklearn.feature_extraction.text.CountVectorizer</p></li>
<li><p>TF-iDF</p>
<ul>
<li><p>term frequency</p></li>
<li><p>inverse document frequency</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf = <span class="number">1</span>/x.<span class="built_in">sum</span>(axis=<span class="number">1</span>)[:,<span class="literal">None</span>]</span><br><span class="line">x = x*tf</span><br><span class="line">idf = np.log(x.shape[<span class="number">0</span>]/(x&gt;<span class="number">0</span>).<span class="built_in">sum</span>(<span class="number">0</span>))</span><br><span class="line">x = x*idf</span><br><span class="line"><span class="comment"># sklearn.feature_extraction.text.TfidfVectorizer</span></span><br></pre></td></tr></tbody></table></figure>
<p>可以降低高频词比重</p></li>
</ul></li>
<li><p>N-grams</p>
<ul>
<li>n个字符组合</li>
<li>sklearn.feature_extraction.text.CountVectorizer: Ngram_range,
analyzer</li>
</ul></li>
<li><p>lowercase</p></li>
<li><p>lemmatization: car = cars转换为基本形式</p></li>
<li><p>stemming: 取词干</p></li>
<li><p>stopwords：</p>
<ul>
<li>NLTK: natural language toolkit library</li>
<li>sklearn.feature_extraction.text.CountVectorizer:
max_df(按频率删除)</li>
</ul></li>
<li><p>embedding(word2vec)</p>
<ul>
<li>邻近的词向量相近：向量相加减, 几百个维度</li>
<li>Word2vec, Glove, FastText</li>
<li>Doc2vec</li>
<li>pretrained model</li>
</ul></li>
</ul></li>
<li><p>image</p>
<ul>
<li>CNN</li>
<li>预先训练好的模型微调fine-tuning再训练对小数据集有时候很有效
<ul>
<li>keras等库有</li>
</ul></li>
<li>image augmentation：
<ul>
<li>旋转图像，增加样本</li>
<li>加噪音</li>
</ul></li>
</ul></li>
</ul>
<h3 id="eda">2. EDA</h3>
<ul>
<li>查看数据逻辑，检查是否有error，探究error原因</li>
<li>了解数据如何被采集，可以有效设置validation scheme</li>
<li>plt.hist(x)</li>
<li>plt.plot(x)</li>
<li>plt.scatter(len(x), x, c=y)</li>
<li>x.describe</li>
<li>look at pairs/groups</li>
<li>检查train test是否分布相同</li>
<li>根据eda结果生成新feature</li>
<li>pd.scatter_matrix(df)</li>
<li>plt.matshow()</li>
<li>sort_values</li>
<li>检查重复的特征或常数特征（drop_duplicate）</li>
<li>traintest[f] = traintest[f].factorize()</li>
<li>.select_dtypes(include=['object'])可以选择不同类型columns</li>
</ul>
<h3 id="validation-and-overfitting">3. Validation and overfitting</h3>
<ul>
<li><p>validation：</p>
<ul>
<li><p>holdout: sklearn.model_selection.ShuffleSplit</p></li>
<li><p>k fold: sklearn.model_selection.Kfold</p></li>
<li><p>leave one out: sklearn.model_selection.LeaveOneOut</p></li>
</ul></li>
<li><p>Stratification: 分层，保留相同分布</p></li>
<li><p>Data splitting：</p>
<ul>
<li>random: row independent</li>
<li>time-based splits
<ul>
<li>moving window validation:
sklearn.model_selection.TimeSeriesSplit</li>
</ul></li>
<li>by id</li>
<li>combined
<ul>
<li>https://medium.com/<span class="citation" data-cites="soumyachess1496/cross-validation-in-time-series-566ae4981ce4">@soumyachess1496/cross-validation-in-time-series-566ae4981ce4</span></li>
</ul></li>
<li>有时需要看test set和train的分布差异</li>
</ul></li>
<li><p>data leak:</p>
<ul>
<li>time series</li>
</ul></li>
</ul>
<h3 id="metrics-evaluation">4. metrics evaluation</h3>
<ul>
<li><p>regression metrics：</p>
<ul>
<li><p>可以改loss function：</p></li>
<li><figure>
<img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20220623215750485.png" alt="image-20220623215750485">
<figcaption aria-hidden="true">image-20220623215750485</figcaption>
</figure></li>
<li><p>MSE： 常数的最优估计是均值</p></li>
<li><p>RMSE：root mean square error</p></li>
<li><p>R-squared</p></li>
<li><p>MAE: mean average error: not sensetive than mse to outlier</p>
<ul>
<li>常数的最优估计是median
<ul>
<li>对outlier友好</li>
</ul></li>
<li>XGBoost不能用，因为二阶导为0</li>
<li>LightGBM可以用</li>
<li>类似huber loss：特别是当error比较大</li>
</ul></li>
<li><p>MSPE：</p>
<ul>
<li>常数的最优估计是weighted target mean</li>
</ul></li>
<li><p>MAPE:</p>
<ul>
<li>常数最优估计：weighted target median</li>
<li>outlier会有很高权重，很少用</li>
</ul></li>
<li><p>RMSLE：log space</p></li>
</ul></li>
<li><p>classification metric：</p>
<ul>
<li>accuracy</li>
<li>log loss
<ul>
<li>binary</li>
<li>multi loss</li>
</ul></li>
<li>AUC：area under curve，包括order</li>
<li>ROC</li>
<li>cohen‘s Kappa</li>
<li>confusion matrix</li>
<li>weighted error</li>
<li>quadratic and linear weighted kappa：多用于医学</li>
</ul></li>
<li><p>optimization</p>
<ul>
<li><p>有的模型不能用一些loss function来优化：</p>
<ul>
<li><p>XGBoost不能用MSPE</p></li>
<li><p>custom loss for XGBoost:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logregobj</span>(<span class="params">preds,dtrains</span>):</span><br><span class="line">    labels = dtrain.get_label()</span><br><span class="line">    preds = <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-preds))<span class="comment"># 有时要处理pred</span></span><br><span class="line">    grad = preds - labels</span><br><span class="line">    hess = preds * (<span class="number">1.0</span>-preds)</span><br></pre></td></tr></tbody></table></figure></li>
</ul></li>
<li><p>early stopping: 防止overfitting</p></li>
<li><p>变换target：比如指数、log等</p></li>
<li><p>calibrate prediction</p>
<ul>
<li>plat scaling: fit logistic regression to predictions(stacking)</li>
<li>isotonic regression: fit isotonic regression to predictions</li>
<li>stacking: fit XGBoost or neural net to predictions</li>
</ul></li>
<li><p>AUC(ROC) optimization:</p>
<ul>
<li><p>pointwise loss: <span class="math inline">\(\min \Sigma
L_{point}(\hat y_i;y_i)\)</span></p></li>
<li><p>pairwise loss:</p>
<ul>
<li><p><span class="math inline">\(\min \Sigma L_{pair}(\hat y_i,\hat
y_j:y_i,y_j)\)</span></p></li>
<li></li>
<li><p><span class="math display">\[
\operatorname{Loss}=-\frac{1}{N_{0} N_{2}} \sum_{j: y_{j}=1}^{N_{1}}
\sum_{i: y_{i}=0}^{N_{0}} \log
\left(\operatorname{prob}\left(\hat{y}_{j}-\hat{y}_{i}\right)\right)
\]</span></p></li>
</ul></li>
<li><p>xgboost, lightboom可行</p></li>
</ul></li>
<li><p>quadratic weighted Kappa:</p>
<ul>
<li><p>optimize MSE</p></li>
<li><p><span class="math display">\[
\begin{aligned}
\operatorname{Kappa}(y, \hat{y}) &amp; \approx 1-\frac{\frac{1}{N}
\sum_{i=1}^{N}\left(\hat{y}_{i}-y_{i}\right)^{2}}{\text { hard to deal
with part }} \\
&amp;=1-\frac{\operatorname{MSE}(y, \hat{y})}{\text { hard to deal with
part }}
\end{aligned}
\]</span></p></li>
</ul></li>
<li><p>optimize thresholds</p></li>
</ul></li>
<li><p>mean encoding: 有很多分类特征</p>
<ul>
<li><img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20220624111932610.png" title="fig:" alt="image-20220624111932610"></li>
</ul></li>
<li><p>StratifiedKFold</p></li>
<li><p>分析树可能有新发现</p></li>
</ul>
<h3 id="hyper-parameter-tuning">5. hyper parameter tuning</h3>
<ul>
<li>libraries:
<ul>
<li>hyperopt</li>
<li>scikit-optimize</li>
<li>spearmint</li>
<li>gpyopt</li>
<li>robo</li>
<li>smac3</li>
</ul></li>
<li>GBDT
<ul>
<li><p>绿色提升fit，红色降低fit</p></li>
<li><p>min_child_weight很重要，increase
model变保守（0，5，15，300……）</p></li>
<li><figure>
<img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20220624114638662.png" alt="image-20220624114638662">
<figcaption aria-hidden="true">image-20220624114638662</figcaption>
</figure></li>
<li><p>sklearn.randomforest/ExtraTrees</p>
<ul>
<li>N_estimators(higher the better)</li>
<li><img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20220624115132530.png" title="fig:" alt="image-20220624115132530"></li>
</ul></li>
</ul></li>
<li>neural networks
<ul>
<li><img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20220624120041719.png" title="fig:" alt="image-20220624120041719"></li>
</ul></li>
<li>可以将数据存储为HDF5或MPI格式，或cpickle</li>
</ul>
<h3 id="pipeline">6. pipeline</h3>
<ul>
<li>EDA:
<ul>
<li>histgram</li>
<li>feature vs target</li>
<li>univariate predictability metrics（IV，R，auc）</li>
<li>bining numerical features and correlation matrices</li>
</ul></li>
<li>Decide the CV strategy</li>
<li>feature engineering:</li>
<li>Modeling
<ul>
<li><figure>
<img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20220624160045769.png" alt="image-20220624160045769">
<figcaption aria-hidden="true">image-20220624160045769</figcaption>
</figure></li>
<li><p>看文献</p></li>
</ul></li>
<li>Ensemble</li>
</ul>
<h3 id="feaure">7. feaure</h3>
<ul>
<li>bray-curtis metric</li>
<li>KNN features</li>
<li>matrix factorization: 利用矩阵分解、加减乘除来增加、减少feature</li>
<li>feature interactions:
<ul>
<li>f1 and f2: 组合形成one hot矩阵：先各自one hot，再pairwise相乘</li>
<li>积分、微分</li>
<li>可以用random forest来选择feature importance</li>
<li>分析树的结构创造新特征：
<ul>
<li>xgboost：predict（pred_leaf=True）</li>
<li>sklearn: apply()</li>
</ul></li>
</ul></li>
<li>tSNE
<ul>
<li>常用于EDA</li>
<li>great tool for visualization</li>
<li>但是要调参</li>
<li>dstill.pub 学习网站</li>
<li>library: tsne, sklearn(慢)</li>
<li>结果可能不容易理解</li>
</ul></li>
</ul>
<h3 id="ensemble">8. Ensemble</h3>
<ul>
<li>bagging：
<ul>
<li>sklearn: baggingClassifier, BaggingRegressor</li>
</ul></li>
<li>boosting:
<ul>
<li>weight based boosting: 关注错的更厉害的
<ul>
<li>adaboost</li>
</ul></li>
<li>residual based boosting
<ul>
<li>learning rate</li>
<li>num of estimator</li>
<li>models：
<ul>
<li>dart：只用之前一部分模型的预测结果</li>
<li>fully gradient based</li>
<li>xgboost</li>
<li>lightgbm</li>
<li>catboost</li>
<li>sklearn GBM</li>
</ul></li>
</ul></li>
</ul></li>
<li>stacking：
<ul>
<li><p>用所有模型的predict输入到新模型<br>
</p></li>
<li><p>注意time</p></li>
<li><p>stacknet</p>
<ul>
<li>可以用Kfold形式训练各个模型</li>
<li>可以把原始feature加入stacking</li>
</ul></li>
<li><figure>
<img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20220624193839550.png" alt="image-20220624193839550">
<figcaption aria-hidden="true">image-20220624193839550</figcaption>
</figure></li>
<li><figure>
<img src="https://raw.githubusercontent.com/xuJ14/ImageHost/main/image-20220624194034207.png" alt="image-20220624194034207">
<figcaption aria-hidden="true">image-20220624194034207</figcaption>
</figure></li>
<li><p>小心data leakage</p></li>
<li><p>stacknet：可用从说明文档中学习到各个模型哪些参数重要</p></li>
<li><p>xcessiv</p></li>
<li><p>stacked ensembles from h2o</p></li>
</ul></li>
<li>catboost
<ul>
<li>快，预制了很多函数</li>
<li>overfiting detector</li>
<li></li>
</ul></li>
</ul>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week05</title>
    <url>/Machine-Learning-Week05/</url>
    <content><![CDATA[<h2 id="neural-networks-learning">Neural Networks Learning</h2>
<h3 id="cost-function-and-propagation">Cost Function and
Propagation</h3>
<ol type="1">
<li><p>Cost Function</p>
<ul>
<li><p>L 神经网络的层数</p></li>
<li><p><span class="math inline">\(s_l\)</span>第l层的神经元个数，不包括bias
unit</p></li>
<li><p>K输出层的神经元个数（即种类）</p></li>
<li><p>binary classification二元分类</p></li>
<li><p>Logistic regression的代价函数： <span class="math display">\[
J(\theta)=-\frac {1}{m}\sum \limits_{i=1}^m\bigg
[y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))\bigg
]+\frac{\lambda}{2m}\sum \limits_{j=1}^n\theta_j^2
\]</span></p></li>
<li><p>神经网络的代价函数： <span class="math display">\[
J(\Theta)=-\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[y_{k}^{(i)}
\log \left((h_{\Theta}(x^{(i)}))_{k}\right)+\left(1-y_{k}^{(i)}\right)
\log \left(1-(h_{\Theta}(x^{(i)}))_{k}\right)\right]+\frac{\lambda}{2 m}
\sum_{l=1}^{L-1} \sum_{i=1}^{s_l}
\sum_{j=1}^{s_{l+1}}(\theta_{j,i}^{(l)})^2
\]</span></p>
<ul>
<li>两个求和符号部分只是将输出层中每个单元的逻辑回归代价函数相加</li>
<li>三个求和符号部分只是将整个网络中所有单个θ的平方相加，其中i并不指代训练实例i</li>
</ul></li>
</ul></li>
<li><p>反向传播算法Backpropagation Algorithm</p>
<ul>
<li><p>梯度下降计算</p>
<ul>
<li><p>min J（θ）就需要计算：J（θ）；J（θ）关于各个θ的偏导</p></li>
<li><p>计算过程：先forward propagation；再反向</p></li>
<li><p><span class="math inline">\(\delta_j^{l}="error"\ of\
node\ j\ in\ layer\ l\)</span>用于改变activation激励值, Formally, <span class="math inline">\(\delta_j^{(l)}=\frac \partial {\partial
z_{j}^{(l)}} cost(j)\)</span>, 其中<span class="math inline">\(cost(i)=y^{(i)}log(h_\Theta(x^{(i)}))-(1-y^{(i)})log(1-h_\Theta(x^{(i)}))\)</span>,
求导后易得<span class="math inline">\(\delta_j^{(l)}=y_j^{(l)}-a_j^{(l)}\)</span>(为什么符号是相反的？答：这里cost错误，和前文代价函数符号相反，应该要变号)</p></li>
<li><p>计算过程：</p>
<ul>
<li><p>令<span class="math inline">\(\Delta_{i,j}^{l}:=0\)</span></p></li>
<li><p>For training example t =1 to m:</p>
<ol type="1">
<li><p>Set <span class="math inline">\(a^{(1)} :=
x^{(t)}\)</span></p></li>
<li><p>Perform forward propagation to compute <span class="math inline">\(a^{(l)}\ for\ l=2,3,…,L\)</span></p>
<p>(此处失效图片链接)img
(https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bYLgwteoEeaX9Qr89uJd1A_73f280ff78695f84ae512f19acfa29a3_Screenshot-2017-01-10-18.16.50.png?expiry=1606176000000&amp;hmac=9aVGT1io0l-sybFSrc1stejo_L0d7hzlNXbQIt47h2Y)</p></li>
<li><p>Using <span class="math inline">\(y^{(t)}\)</span>, compute$
^{(L)} = a^{(L)} - y^{(t)}$</p>
<p>Where L is our total number of layers and
a^{(L)}<em>a</em>(<em>L</em>) is the vector of outputs of the activation
units for the last layer. So our "error values" for the last layer are
simply the differences of our actual results in the last layer and the
correct outputs in y. To get the delta values of the layers before the
last layer, we can use an equation that steps us back from right to
left:</p></li>
<li><p>Compute$ ^{(L-1)}, <sup>{(L-2)},,</sup>{(2)}<span class="math inline">\(using\)</span>$ ^{(l)} = ((<sup>{(l)})</sup>T
^{(l+1)})&nbsp;.<em>&nbsp;a^{(l)}&nbsp;.</em>&nbsp;(1 - a^{(l)}) $$</p>
<p>The delta values of layer l are calculated by multiplying the delta
values in the next layer with the theta matrix of layer l. We then
element-wise multiply that with a function called g', or g-prime, which
is the derivative of the activation function g evaluated with the input
values given by <span class="math inline">\(z^{(l)}\)</span>.</p>
<p>The g-prime derivative terms can also be written out as:</p>
<p><span class="math inline">\(g′(z^{(l)})=a^{(l)} .∗
(1−a^{(l)})\)</span></p></li>
<li><p><span class="math inline">\(Δ_{i,j}^{(l)}:=Δ_{i,j}^{(l)}+a_j^{(l)}δ_i^{(l+1)}\)</span>,
or with vectorization, <span class="math inline">\(Δ^{(l)}:=Δ^{(l)}+δ^{(l+1)}(a^{(l)})^T\)</span></p>
<p>Hence we update our new <span class="math inline">\(\Delta\)</span>
matrix.</p>
<ul>
<li><p><span class="math inline">\(D_{i,
j}^{(l)}:=\frac{1}{m}\left(\Delta_{i, j}^{(l)}+\lambda \Theta_{i,
j}^{(l)}\right), \text { if } j \neq 0\)</span></p></li>
<li><p><span class="math inline">\(D_{i, j}^{(l)}:=\frac{1}{m}\Delta_{i,
j}^{(l)}, \text { if } j = 0\)</span></p>
<p>The capital-delta matrix D is used as an "accumulator" to add up our
values as we go along and eventually compute our partial derivative.
Thus we get$ J()= D_{ij}^{(l)}$</p></li>
</ul></li>
</ol></li>
</ul></li>
<li><p><a href="https://zhuanlan.zhihu.com/p/26765585">上述计算过程的中文推导</a></p></li>
</ul></li>
</ul></li>
</ol>
<h3 id="backpropagation-in-practice">Backpropagation in practice</h3>
<ol type="1">
<li><p>系数展开到向量：</p>
<ul>
<li><p>M(a,b)，既取a，也取b，从1开始</p></li>
<li><p>优化算法（如：fminunc）默认将参数整合到一个向量中</p>


   - 
   <figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">%代码过程</span><br><span class="line">thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]</span><br><span class="line">deltaVector = [ D1(:); D2(:); D3(:) ]</span><br><span class="line"></span><br><span class="line">Theta1 = reshape(thetaVector(1:110),10,11)</span><br><span class="line">Theta2 = reshape(thetaVector(111:220),10,11)</span><br><span class="line">Theta3 = reshape(thetaVector(221:231),1,11)</span><br></pre></td></tr></tbody></table></figure>

     </li>
<li><p>过程：(此处失效图片链接)img(https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/kdK7ubT2EeajLxLfjQiSjg_d35545b8d6b6940e8577b5a8d75c8657_Screenshot-2016-11-27-15.09.24.png?expiry=1606348800000&amp;hmac=bH794vb16zxSOiqZRj2Pe0PyuaYNbZ8tDQZlnGSoM18)</p></li>
</ul></li>
<li><p>梯度检验gradient checking</p>
<ul>
<li><p><span class="math inline">\(\frac{d}{d\Theta}J(\Theta)\approx
\frac{J(\Theta+\epsilon)+J(\Theta-\epsilon)}{2\epsilon}\)</span>，ε =
<span class="math inline">\(10^{-4}\)</span></p></li>
<li><p><span class="math display">\[
\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1,
\dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots,
\Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}
\]</span></p></li>
<li><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">epsilon = 1e-4;</span><br><span class="line">for i = 1:n,</span><br><span class="line">  thetaPlus = theta;</span><br><span class="line">  thetaPlus(i) += epsilon;</span><br><span class="line">  thetaMinus = theta;</span><br><span class="line">  thetaMinus(i) -= epsilon;</span><br><span class="line">  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)</span><br><span class="line">end;</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>Check: gradApprox ≈
deltaVector，只需要验证一次即可，否则这种方法会非常慢</p></li>
</ul></li>
<li><p>Random Initialization随机初始化</p>
<ul>
<li><p>Symmetry
breaking，因为如果设为一样的会使得梯度下降后参数也一样</p></li>
<li><p>将每一个<span class="math inline">\(\Theta_{ij}^{(l)}\)</span>设为在[-ε，ε]之间的随机数，但要同时设置，以防出现相同，例：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">Theta1 = rand(10,11)*2*init_epsilon-init_epsilon;</span><br></pre></td></tr></tbody></table></figure>
<p>(Note: the epsilon used above is unrelated to the epsilon from
Gradient Checking)</p></li>
</ul></li>
<li><p>总体回顾</p>
<ul>
<li>一般默认隐藏层每层神经元数量一致</li>
<li>构建一个模型的过程：
<ol type="1">
<li>初始化模型
<ul>
<li>Number of input units = dimension of features
x^{(i)}<em>x</em>(<em>i</em>)</li>
<li>Number of output units = number of classes</li>
<li>Number of hidden units per layer = usually more the better (must
balance with cost of computation as it increases with more hidden
units)</li>
<li>Defaults: 1 hidden layer. If you have more than 1 hidden layer, then
it is recommended that you have the same number of units in every hidden
layer.</li>
</ul></li>
<li><strong>Training a Neural Network</strong>
<ol type="1">
<li>Randomly initialize the weights</li>
<li>Implement forward propagation to get <span class="math inline">\(h_\Theta(x^{(i)})\)</span> for any <span class="math inline">\(x^{(i)}\)</span></li>
<li>Implement the cost function</li>
<li>Implement backpropagation to compute partial derivatives</li>
<li>Use gradient checking to confirm that your backpropagation works.
Then disable gradient checking.</li>
<li>Use gradient descent or a built-in optimization function to minimize
the cost function with the weights in theta.</li>
<li>对每一个训练样例循环上述步骤（有可能得到局部最优，因为J（θ）是非凸函数）</li>
</ol></li>
</ol></li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week07</title>
    <url>/Machine-Learning-Week07/</url>
    <content><![CDATA[<h2 id="support-vector-machines-支持向量机">Support Vector Machines
支持向量机</h2>
<h3 id="large-margin-classification">Large Margin Classification</h3>
<ol type="1">
<li><p>Optimization Objective 优化目标</p>
<ul>
<li><p>在复杂非线性方程的学习上有优势。</p></li>
<li><figure>
<img src="C:\Users\Jun\AppData\Roaming\Typora\typora-user-images\image-20201206123628670.png" alt="image-20201206123628670">
<figcaption aria-hidden="true">image-20201206123628670</figcaption>
</figure></li>
<li><figure>
<img src="C:\Users\Jun\AppData\Roaming\Typora\typora-user-images\image-20201206124245802.png" alt="image-20201206124245802">
<figcaption aria-hidden="true">image-20201206124245802</figcaption>
</figure>
<p>注：实际上在训练代价函数时，分界线为-1，1</p></li>
</ul></li>
<li><p>Large Margin Intuition</p>
<ul>
<li><p>代价函数： <span class="math display">\[
\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)}
{\operatorname{cost}_{1}\left(\theta^{T}
x^{(i)}\right)}+\left(1-y^{(i)}\right)
\operatorname{cost}_{0}\left(\theta^{T}
x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}\\
if\ y=1, we\ want\ \theta^{T} x \geq 1\ (not\ just \left.\geq 0\right)\\
if\ y=0, we\ want\ \theta^{T} x \leq-1\ (not\ just&lt;0 )
\]</span></p></li>
<li><p>SVM也被叫做大间距分类器，因为它尽量用最大间距分类样本，使得SVM更具有鲁棒性</p></li>
<li><p>其中正则化参数C不适合设置得过大，因为这会使得一些异常值（outliner)
会使得分类向异常值产生比较大得偏差，这就类似与逻辑回归里过小的λ
。</p></li>
</ul></li>
<li><p>Mathematics Behind Large Margin Classification</p>
<ul>
<li><p>向量内积：<span class="math inline">\(\lVert u
\rVert\)</span>表明u的范数，即u向量的长度。<span class="math inline">\(u^Tv=p\cdot\lVert u \rVert\)</span>, p
是v在u上的投影长度</p></li>
<li><p>本图演示了为什么这种方式可以最小化θ<img src="C:\Users\Jun\AppData\Roaming\Typora\typora-user-images\image-20201206171424283.png" alt="image-20201206171424283"></p>
<p>注：这里决策边界和θ是具有垂直关系的</p></li>
<li><p>这里最小化θ范数等价于最大化所有的p，即最大化所有点到决策边界的margin。</p></li>
<li><p>即使<span class="math inline">\(\theta_0\ne0\)</span>推理过程也和上述类似</p></li>
</ul></li>
</ol>
<h3 id="kernels">Kernels</h3>
<ol type="1">
<li><p>非线性决策边界</p>
<ul>
<li><p>用相似度函数similarity
function代替高阶项，度量样本x和第一个标记的相似度，即核函数，例如：高斯核函数：<span class="math inline">\(f_1=similarity(x,l^{(1)})=exp(-\frac{\lVert
x-l^{(1)}
\rVert^2}{2\sigma^2})=exp(-\frac{\Sigma_{j=1}^{n}(x_j-l_j^{(i)})^2}{2\sigma^2})\)</span></p>
<p>==问：这里σ是什么，如何计算？==</p></li>
<li><p>通过用f替代特征，可以训练出复杂的非线性边界：例如：预测y=1，当<span class="math inline">\(\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3\ge0\)</span></p></li>
</ul></li>
<li><p>选择landmark标记点 l :</p>
<ul>
<li>Given一系列x，令<span class="math inline">\(l^1=x^1\)</span>, <span class="math inline">\(f_{m}^{(i)}=similarity(x^{(i)},l^{(m)})\)</span></li>
<li>将<span class="math inline">\(x^{(i)}\in
\mathbb{R}^{n+1}\)</span>，转变为<span class="math inline">\(f^{(i)}=\begin{bmatrix}f_0^{(i)}\\f_0^{(i)}\\
\vdots \\ f_m^{(i)} \end{bmatrix}\)</span>，<span class="math inline">\(f_0^{(i)}=1\)</span>。<span class="math inline">\(f\in \mathbb{R}^{m+1}\)</span></li>
</ul></li>
<li><p>Hypothesis：Given x ， compute features f</p>
<p>Predict "y=1" if <span class="math inline">\(\theta^Tf\ge0\)</span></p></li>
<li><p>代价函数： <span class="math display">\[
\min _{\theta} C \sum_{i=1}^{m} y^{(i)}
\operatorname{cost}_{1}\left(\theta^{T}
f^{(i)}\right)+\left(1-y^{(i)}\right)
\operatorname{cost}_{0}\left(\theta^{T} f^{(i)}\right)+\frac{1}{2}
\sum_{j=1}^{m } \theta_{j}^{2}
\]</span> 在SVM的实际应用中，最后一项通常会用<span class="math inline">\(\theta^TM\theta\)</span>，其中M和核函数有关，是为了提高计算效率</p></li>
<li><p>参数的选择：</p>
<ul>
<li><p>C(=<span class="math inline">\(\frac{1}{\lambda}\)</span>)</p>
<ul>
<li>Large C：Lower bias；High variance</li>
<li>Small C：Higher bias；Low variance</li>
</ul></li>
<li><p><span class="math inline">\(\sigma^2\)</span></p>
<ul>
<li><p>Large <span class="math inline">\(\sigma^2\)</span>: Fearures fi
vary more smoothly.</p>
<p>​ Higher bias; Lower variance</p></li>
<li><p>Small <span class="math inline">\(\sigma^2\)</span>: Features fi
vary less smoothly.</p>
<p>​ Lower bias, Higher variance.</p></li>
</ul></li>
<li><p><strong>方差（variance）：</strong>方差描述的是训练数据在不同迭代阶段的训练模型中，预测值的变化波动情况（或称之为离散情况）。</p></li>
<li><p><strong>偏差（bias）：</strong>偏差衡量了模型的预测值与实际值之间的偏离关系。</p></li>
<li><p>低偏差高方差：过拟合；</p></li>
<li><p>You want to train C and the parameters for the kernel function
using the training and cross-validation datasets.</p></li>
</ul></li>
</ol>
<h3 id="svms-in-practice">SVMs in Practice</h3>
<ol type="1">
<li><p>应用SVM</p>
<ul>
<li><p>选择C和σ</p>
<figure class="highlight matlab"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[C, sigma]</span> = <span class="title">dataset3Params</span><span class="params">(X, y, Xval, yval)</span></span></span><br><span class="line"><span class="comment">% 这里x是训练集，xval是验证集</span></span><br><span class="line">value = [<span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>];</span><br><span class="line"></span><br><span class="line">minC = <span class="number">0</span>;</span><br><span class="line">minSigma = <span class="number">0</span>;</span><br><span class="line"><span class="comment">% 最小值设为交叉验证集的用例数</span></span><br><span class="line">minError = <span class="built_in">size</span>(Xval,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">8</span>,</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="number">8</span>,</span><br><span class="line">        model= svmTrain(X, y, value(<span class="built_in">i</span>), @(x1, x2) gaussianKernel(x1, x2, value(<span class="built_in">j</span>)));</span><br><span class="line">        predictions = svmPredict(model,Xval);</span><br><span class="line">        error = <span class="built_in">mean</span>(double(predictions ~= yval));</span><br><span class="line">        <span class="keyword">if</span> minError &gt; error</span><br><span class="line">            minError = error;</span><br><span class="line">            minC = value(<span class="built_in">i</span>);</span><br><span class="line">            minSigma = value(<span class="built_in">j</span>);</span><br><span class="line">        <span class="keyword">end</span>;</span><br><span class="line">    <span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line">C = minC;</span><br><span class="line">sigma = minSigma;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></tbody></table></figure></li>
<li><p>选择核函数（相似度函数）</p>
<ul>
<li>线性核函数（=没有核函数）</li>
</ul></li>
<li><p><strong>需要对特征向量归一化</strong>，否则一个特征会占据主要位置</p></li>
<li><p>不是所有函数都能作为核函数，需要满足Mercer‘s
Theorem莫塞尔定理，来确保SVM优化包能够正确运行</p></li>
<li><p>其他核函数：</p>
<ul>
<li>多项式核函数</li>
<li>字符串核函数string kernel；</li>
<li>卡方核函数chi-square kernel</li>
<li>直方图交叉核函数histogram intersection kernel</li>
</ul></li>
<li><p>多类别分类</p>
<ul>
<li>svm包多数内置了</li>
<li>否则用one-vs-all
：训练得到K组θ的值，然后选择θx最大的那一组作为分类结果</li>
</ul></li>
<li><p>逻辑回归和SVM，什么时候用？</p>
<ul>
<li>n为特征数，m为训练集样本数。如果n相对于m来说比较大，使用逻辑回归或者没有核函数的SVM（linear
kernel）</li>
<li>n小，m中等大。使用高斯核函数</li>
<li>n小，m很大。运行很慢，逻辑回归或没有核函数的SVM</li>
<li>神经网络上面都适用，但有时可能会比SVM慢很多</li>
</ul></li>
<li><p>SVM属于凸优化问题，大多数包都会找到全局最优，无需担心局部最优。神经网络的局部最优问题不是非常大但也不小。</p></li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week06</title>
    <url>/Machine-Learning-Week06/</url>
    <content><![CDATA[<h2 id="deciding-what-to-try-next">Deciding What to Try Next</h2>
<h3 id="evaluating-a-learning-algorithm">Evaluating a Learning
Algorithm</h3>
<ol type="1">
<li><p>误差大的改进方法：</p>
<ul>
<li>更多数据</li>
<li>更少特征集避免过拟合</li>
<li>更多特征</li>
<li>增加多项式特征</li>
<li>减少或增大正则化参数值</li>
</ul></li>
<li><p>评估假设函数Evaluating a Hypothesis</p>
<ul>
<li><p>如何判断过拟合？</p>
<ul>
<li><p>随机将数据分为两部分，一部分是训练集，一部分是预测集（30%）</p></li>
<li><p>学习训练集的参数θ（即最小化训练误差J ）</p></li>
<li><p>计算测试集误差</p>
<ul>
<li><p>线性回归中计算测试集的 J(θ)</p></li>
<li><p>逻辑回归中可以用误分类率来计算error：</p>
<p><span class="math inline">\(err(h_\theta(x),y)=\begin{cases}1,&amp;if\
h_\theta(x)\ge 0.5,y=0\  or\ if\ h_\theta(x)&lt;0.5, y=1\\0,
&amp;otherwise\end{cases}\)</span></p>
<p><span class="math inline">\(Test\
error=\frac{1}{m_{test}}\sum\limits_{i=1}^{m_{test}}err(h_\theta(x_{test}^{(i)},y^{(i)}))\)</span></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>模型选择问题：确定对于某组数据最合适的多项式是几次，怎样选用正确的特征来构造学习算法，或者需要正确选择算法中的正则化参数λ</p>
<ul>
<li>将数据分为三段：训练集（60%），交叉验证集（cross
validation）（20%），测试集（20%）</li>
<li>用不同的多项式模型得到θ，然后计算交叉验证集的误差，看看哪个模型中CV集的误差最小，进而选择那个多项式模型</li>
<li>最后计算测试集误差评价模型表现</li>
</ul></li>
</ol>
<h3 id="bias-vs.-variance">Bias vs. Variance</h3>
<ol type="1">
<li><p>分析bias和variance</p>
<ul>
<li>高偏差：欠拟合；高方差：过拟合</li>
<li>高偏差：
<ul>
<li><span class="math inline">\(J_{train}(\theta)\)</span>will be high,
the same as <span class="math inline">\(J_{CV}(\theta)\approx
J_{train}(\theta)\)</span></li>
</ul></li>
<li>高方差：
<ul>
<li><span class="math inline">\(J_{train}(\theta)\)</span>will be low,
<span class="math inline">\(J_{CV}(\theta)\gg
J_{train}(\theta)\)</span></li>
</ul></li>
<li><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/I4dRkz_pEeeHpAqQsW8qwg_bed7efdd48c13e8f75624c817fb39684_fixed.png?expiry=1606262400000&amp;hmac=zQgQniOf2GNzDd--YwZNZhUDjMOAn1JxYg1s_gG_d_I" title="fig:" alt="img"></li>
</ul></li>
<li><p>正则化与方差、偏差的关系</p>
<ul>
<li><p>正则化可以有效防止过拟合</p></li>
<li><p>选择λ的过程：</p>
<ul>
<li>Create a list of lambdas (i.e.
λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});</li>
<li>Create a set of models with different degrees or any other
variants.</li>
<li>Iterate through the λs and for each λ go through all the models to
learn some Θ.</li>
<li>Compute the cross validation error using the learned Θ (computed
with λ) on the$ J_{CV}()$ <strong>without</strong> regularization or λ =
0.</li>
<li>Select the best combo that produces the lowest error on the cross
validation set.</li>
<li>Using the best combo Θ and λ, apply it on$ J_{test}()$ to see if it
has a good generalization of the problem.</li>
</ul></li>
<li><p>注：训练时带λ，计算各个集的误差时不需要λ，即<span class="math inline">\(J_{train}(\theta)\)</span>、<span class="math inline">\(J_{CV}(\theta)\)</span>、<span class="math inline">\(J_{test}(\theta)\)</span>都不包含正则项 <span class="math display">\[
J_{train}(\theta)=\frac {1}{2m}\sum
\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
\]</span></p></li>
</ul></li>
<li><p>学习曲线（error-训练集的大小）</p>
<ul>
<li>高偏差
<ul>
<li><strong>Low training set size</strong>: causes <span class="math inline">\(J_{train}(\Theta)\)</span> to be low and <span class="math inline">\(J_{CV}(\Theta)\)</span> to be high.</li>
<li><strong>Large training set size</strong>: causes both <span class="math inline">\(J_{train}(\Theta)\)</span> and <span class="math inline">\(J_{CV}(\Theta)\)</span> to be high with <span class="math inline">\(J_{train}(\Theta)≈J_{CV}(\Theta)\)</span>.</li>
<li>高偏差，更多训练数据不会有很大帮助</li>
<li><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bpAOvt9uEeaQlg5FcsXQDA_ecad653e01ee824b231ff8b5df7208d9_2-am.png?expiry=1606262400000&amp;hmac=-xKhLZnoeJ_Ht3_X7XaX3Jae2F94vNrpbyPcMCExkPg" title="fig:" alt="img"></li>
</ul></li>
<li>高方差
<ul>
<li><strong>Low training set size</strong>: causes <span class="math inline">\(J_{train}(\Theta)\)</span> to be low and <span class="math inline">\(J_{CV}(\Theta)\)</span> to be high.</li>
<li><strong>Large training set size</strong>: <span class="math inline">\(J_{train}(\Theta)\)</span> increases with training
set size and <span class="math inline">\(J_{CV}(\Theta)\)</span>
continues to decrease without leveling off. Also, <span class="math inline">\(J_{train}(\Theta) &lt; J_{CV}(\Theta)\)</span> but
the difference between them remains significant.</li>
<li>高方差，更多训练数据可能有帮助</li>
<li><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/vqlG7t9uEeaizBK307J26A_3e3e9f42b5e3ce9e3466a0416c4368ee_ITu3antfEeam4BLcQYZr8Q_37fe6be97e7b0740d1871ba99d4c2ed9_300px-Learning1.png?expiry=1606262400000&amp;hmac=6kgO1n2qmCGikLwAE15RXslkLQYjwcHJ2utP_e9INDk" title="fig:" alt="img"></li>
</ul></li>
</ul></li>
<li><p>诊断法则如何判断</p>
<ul>
<li>更多训练数据——高方差，画学习曲线</li>
<li>更少特征集避免过拟合——高方差</li>
<li>更多特征——高偏差</li>
<li>增加多项式特征——高偏差</li>
<li>减少（高偏差）或增大正则化参数值（高方差）
<ul>
<li>正则化会保留所有的特征变量，但是会减小特征变量的数量级。正则化就是使用惩罚项，通过惩罚项，我们可以将一些参数的值变小。通常参数值越小，对应的函数也就越光滑，也就是更加简单的函数，因此不容易发生过拟合问题。</li>
</ul></li>
<li>神经网络很容易过拟合，正则化项非常有用
<ul>
<li>如何选择几层隐藏层：
<ul>
<li>通过交叉验证集，测试1，2，…，l个隐藏层的误差，选择表现最好的一个</li>
</ul></li>
</ul></li>
</ul></li>
</ol>
<h3 id="build-a-spam-classifier">Build a Spam Classifier</h3>
<ol type="1">
<li>做法：
<ul>
<li>遍历整个训练集，然后在中间选出出现次数最多的n个单词，n一般介于10000和50000之间，作为特征</li>
<li>列出可能的做法，讨论可行性，然后选择一个方向</li>
</ul></li>
<li>误差分析
<ul>
<li>推荐做法：
<ul>
<li>用简单算法快速实现，然后测试CV集（避免过早优化）</li>
<li>画学习曲线，决定是否需要更多数据、更多特征或其他</li>
<li>误差分析：手动检验CV集中的错误分类样本，发现系统性的错误分类特征，构造更好的特征
<ul>
<li>手动对错误的部分分类</li>
<li>发现特征</li>
<li>用数字来量化表现误差（在CV集上，不能在test集上）</li>
</ul></li>
</ul></li>
</ul></li>
<li>stem词干提取（porter stemmer）</li>
<li>skewed classes 偏斜类问题
<ul>
<li>癌症的比例非常非常低</li>
<li>查准率（precision）：预测为1的病人里，多少是真正得癌症的
<ul>
<li><span class="math inline">\(=\frac{True\ positives}{predicted\
positives}=\frac{True\ positives}{True\ pos+False\ pos}\)</span></li>
</ul></li>
<li>召回率Recall：实际得癌症的病人里，多少是真正预测得癌症的
<ul>
<li><span class="math inline">\(=\frac{True\ positives}{actual\
positives}=\frac{True\ positives}{True\ pos+False\ neg}\)</span></li>
</ul></li>
<li><img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/113quiz1q1.png"></li>
<li>假设现在想要预测y=1，当非常自信的情况下（修改<span class="math inline">\(h_\theta(x)\)</span>分类临界值threshold0.5为0.9）：高查准，低召回</li>
<li>假设想要避免错过癌症案例，（修改<span class="math inline">\(h_\theta(x)\)</span>分类临界值0.5为0.3）：高召回，低查准</li>
<li>绘制查准率-召回率曲线</li>
<li><span class="math inline">\(F_1\)</span>Score <span class="math inline">\(=\frac{2PR}{P+R}\)</span>:
<ul>
<li>评估选择算法或者不同临界值的量化标准</li>
</ul></li>
</ul></li>
<li>Data For Machine Learning
<ul>
<li>有大量训练数据可以显著提升算法表现，可能得到低方差、低偏差的结果，test误差和train误差也相近</li>
<li>多项式参数对大训练集没有帮助</li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week08</title>
    <url>/Machine-Learning-Week08/</url>
    <content><![CDATA[<h1 id="unsupervised-learning-无监督学习">Unsupervised learning
无监督学习</h1>
<h2 id="clustering-聚类">Clustering 聚类</h2>
<p>无监督学习是不给分类标签y</p>
<h3 id="应用">1. 应用：</h3>
<ul>
<li>对消费者市场划分</li>
<li>社交网络分析</li>
<li>管理计算机</li>
<li>获取星系信息</li>
</ul>
<h3 id="k-means-algorithm-k均值算法">2. K-means algorithm K均值算法</h3>
<ul>
<li><p>随机选择两个点作为聚类中心(分两类)，然后进行下面两步</p>
<ul>
<li>簇分配，遍历每个点，和哪个cluster centroid更近</li>
<li>移动聚类中心到上面分配的簇的均值</li>
</ul></li>
<li><p>Input:</p>
<ul>
<li>K - number of clusters</li>
<li>training set</li>
</ul></li>
<li><p>algorithm</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"> repeat{</span><br><span class="line">	for i=1 to m</span><br><span class="line">		c_i := index (from 1 to K) of cluster centroid closest to x_i</span><br><span class="line">    for k=1 to K</span><br><span class="line">    	u_k := average (mean) of points assigned to cluster k</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></li>
<li><p>如果有中心没有被分配到任何点，可以删去这个分类，也可以reinitialize
the cluster centroid</p></li>
</ul>
<h3 id="k-means-for-non-sperated-clusters">3. K-means for non-sperated
clusters</h3>
<ul>
<li>根据身高体重分类T恤大小</li>
</ul>
<h3 id="optimization-objective">4. Optimization Objective</h3>
<ul>
<li><p><span class="math display">\[
\begin{array}{l}
J\left(c^{(1)}, \ldots, c^{(m)}, \mu_{1}, \ldots,
\mu_{K}\right)=\frac{1}{m}
\sum_{i=1}\left\|x^{(i)}-\mu_{c^{(i)}}\right\|^{2} \\
\min _{c^{(1)}, \ldots, c^{(m)},\mu_{1}, \ldots, \mu_{K}}
J\left(c^{(1)}, \ldots, c^{(m)}, \mu_{1}, \ldots, \mu_{K}\right)
\end{array}
\]</span></p></li>
<li><p>also called distortion of K-means algorithm</p></li>
<li><p>cost function 关于iteration的曲线</p></li>
</ul>
<h3 id="random-initialization">5. Random Initialization</h3>
<p>随机初始化聚类中心</p>
<p>随机选k个个例本身，作为中心，但是容易陷入Local
optima。解决方案：多次随机，选择cost function最小的</p>
<h3 id="choosing-the-number-of-clusters">6. Choosing the Number of
Clusters</h3>
<p>Elbow Method:</p>
<p>绘制J关于K的曲线，找到“Elbow”的节点，作为分类个数</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210331185002558.png" alt="image-20210331185002558">
<figcaption aria-hidden="true">image-20210331185002558</figcaption>
</figure>
<p>用途之一：later/downstream purpose</p>
<h2 id="motivation">Motivation</h2>
<h3 id="data-compression------dimensionality-reduction降维">1. Data
Compression ---- Dimensionality Reduction降维</h3>
<p>压缩数据，减少内存占用加快运算</p>
<h3 id="data-visualization">2. Data Visualization</h3>
<p>降维可以便于数据可视化</p>
<h2 id="principal-component-analysis主成分分析">Principal Component
Analysis主成分分析</h2>
<h3 id="problem-formulation">1. Problem formulation</h3>
<p>PCA：To find a surface onto which to project the data so as to
minimize the the projection error
(线性拟合中就是使得离某条直线的距离最小).</p>
<ul>
<li>先mean normalization（feature scaling）</li>
<li>Reduce from 2-dimension to 1-dimension: Find a direction (a vector
<span class="math inline">\(u_i\in \R^n\)</span>) onto which to project
the data so as to minimize the projection error.
该向量的正负没有关系。</li>
<li>Reduce from n-dimension to k-dimension: Find k vectors <span class="math inline">\(u^{(1)}\)</span>,<span class="math inline">\(u^{(2)}\)</span>, ... , <span class="math inline">\(u^{(k)}\)</span> onto which to project the data so
as to minimize the projection error.</li>
<li>注：PCA is not linear
regression。注意看下图区别，线性回归是和预期值的差最小，PCA是到直线距离最小</li>
</ul>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210331214953585.png" alt="image-20210331214953585">
<figcaption aria-hidden="true">image-20210331214953585</figcaption>
</figure>
<h3 id="pca-algorithm">2. PCA Algorithm</h3>
<p>PCA的目标：</p>
<ul>
<li>maximize variance perspective 最大投影方差</li>
<li>minimize error perspective 最小重构代价</li>
</ul>
<p>Step1: mean normalization.
根据数据(有不同的scales)，可能还需要feature
scaling。均值归一化是特征缩放的一种方法。（除以标准差）</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210331224012613.png" alt="image-20210331224012613">
<figcaption aria-hidden="true">image-20210331224012613</figcaption>
</figure>
<p>Step2: Reduce data from n-dimensions to k-dimensions：</p>
<ul>
<li><p>Compute "covariance matrix": <span class="math inline">\(\Sigma =
\frac{1}{m}\sum_{i=1}^{n}{(x^{(i)})(x^{(i)})^T}\)</span>，xi是n*1的列向量，symmetric
positive definite</p></li>
<li><p>Compute "eigenvectors" of matrix <span class="math inline">\(\Sigma\)</span>: [U, S, V] = svd(Sigma) ;或者
eig(Sigma)</p>
<p>svd: Singular value decomposition <a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解</a></p>
<p>其中U是我们需要的，
列向量为各个特征向量，压缩到k个向量只需要取前k个列向量（对称矩阵的特征向量是正交的）</p></li>
</ul>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210401221235384.png" alt="image-20210401221235384">
<figcaption aria-hidden="true">image-20210401221235384</figcaption>
</figure>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210401222501123.png" alt="image-20210401222501123">
<figcaption aria-hidden="true">image-20210401222501123</figcaption>
</figure>
<p>注：PCA算法中，没有x0=1这一项<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/v2-a00e6519ce24ea2b151c8bece4226d95_720w.jpg" alt="img"><img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/v2-ddbf215dce156564852941f7294c7431_720w.jpg" alt="img"></p>
<p><strong>疑问：</strong></p>
<ol type="1">
<li><p><strong>为什么不直接用X进行SVD，而要算协方差的SVD？</strong></p>
<p><strong>Answer：</strong><a href="https://www.coursera.org/learn/machine-learning/discussions/weeks/8/threads/kk41Fe5OEeqnzBL0ZMnGMw">协方差的U和X的V是相等的，其实都可以</a></p>
<p>理解：<span class="math inline">\(X=U'S'V'^T\)</span>,
<span class="math inline">\(X^TX=USV^T\)</span>，现要说明 <span class="math inline">\(V'=U\)</span>, 只需要 <span class="math inline">\(X^TX=V'S'^2V'^T=USV^T\)</span>，所以
<span class="math inline">\(V'=U\)</span>。</p></li>
<li><p><strong>为什么要取协方差矩阵SVD后的左奇异矩阵来压缩特征维度？</strong></p>
<p>首先易证一个SPD矩阵通过SVD得到的U和V是相等的。</p>
<p>其次，通常我们用XV=YU来表示坐标变换。</p>
<p><span class="math inline">\(\Sigma = X^TX\)</span>，又有 <span class="math inline">\(\Sigma = USV^T\)</span>，可以得到 <span class="math inline">\(\Sigma V =
US\)</span>，这里实际上是将covariance矩阵变化为对角线矩阵，即互不相关（其中U=V，即U为协方差矩阵的特征向量，S为其特征值）。那么这时，我们令XI=UZ，I是X坐标系下的正交基，即
<span class="math inline">\(Z=U^TX\)</span>。此时Z的协方差 <span class="math inline">\(Z^TZ=\Sigma\)</span>。</p></li>
</ol>
<h2 id="applying-pca">Applying PCA</h2>
<h3 id="reconstruction-from-compressed-representation">Reconstruction
from compressed representation</h3>
<p><span class="math display">\[
z=U_{reduce}^Tx
\]</span></p>
<p>可以得到： <span class="math display">\[
x_{approx}=U_{reduce}z
\]</span></p>
<h3 id="choosing-the-number-k-of-principal-component">Choosing the
number k of principal component</h3>
<ul>
<li>Average squared projection error：</li>
</ul>
<p><span class="math display">\[
\frac{1}{m}\Sigma_{i=1}^m\parallel x^{(i)}-x_{approx}^{(i)}\parallel ^2
\]</span></p>
<ul>
<li><p>Tptal variation in the data: <span class="math display">\[
\frac{1}{m}\Sigma_{i=1}^m\parallel x^{(i)}\parallel ^2
\]</span></p></li>
<li><p>Choose k to be smallest value so that <span class="math display">\[
\frac{\frac{1}{m}\Sigma_{i=1}^m\parallel
x^{(i)}-x_{approx}^{(i)}\parallel ^2}{\frac{1}{m}\Sigma_{i=1}^m\parallel
x^{(i)}\parallel ^2}\le0.01
\]</span> "99% of variance is
retained"。这个数值可以根据需求不同更换。</p></li>
<li><p>算法：</p>
<p>SVD函数返回的S矩阵为diagonal matrix。</p>
<p>For given k: k 递增，使得 <span class="math display">\[
1-\frac{\Sigma_{i=1}^{k}S_{ii}}{\Sigma_{i=1}^{n}S_{ii}}\le0.01
\]</span></p>
<p>选择最小的K满足上式。</p>
<p>根据性质：矩阵特征值之和等于主对角线元素之和，易证(6)式等价于(7)式。</p></li>
</ul>
<h3 id="advice-for-applying-pca">Advice for applying PCA</h3>
<ol type="1">
<li><p>Supervised learning speedup</p>
<p>压缩原始训练集数据的维度</p>
<p>Mapping <span class="math inline">\(x^{(i)}\rightarrow
z^{(i)}\)</span>should be defined by running PCA only on the trainning
set. This mapping can be applied as well to the examples <span class="math inline">\(x_{CV}^{(i)}\)</span> and <span class="math inline">\(x_{test}^{(i)}\)</span> in the cross validation
and test sets.</p></li>
<li><p>Compression</p>
<ul>
<li>Reduce memory/disk needed to store data</li>
<li>speed up learning algorithm</li>
</ul></li>
<li><p>Visualization</p></li>
<li><p>Bad use of PCA: To prevent overfitting</p>
<p>可能确实可以避免过拟合，但是不是解决过拟合的好方法。正确方法式加正则项(Use
regularization instead!)</p>
<p>For example: <img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210403175514854.png" alt="image-20210403175514854"></p></li>
<li><p>正确做法是先不用PCA，在原始数据跑一遍，如果不符合期望，再用PCA</p></li>
</ol>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week10</title>
    <url>/Machine-Learning-Week10/</url>
    <content><![CDATA[<h1 id="large-scale-machine-learning">Large Scale Machine Learning</h1>
<h2 id="gradient-descent-with-large-datasets">Gradient Descent with
Large Datasets</h2>
<h3 id="learning-with-large-datasets">Learning with large datasets</h3>
<p>如何知道小m（size of dataset）也能产生很好的结果？绘制learning
curve（error关于m的曲线）</p>
<p>左图为high variance，说明增加m有助于结果变好；右图为high
bias，说明增加m无助于结果变好</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210409182113679.png" alt="左：high variance；右: high bias">
<figcaption aria-hidden="true">左：high variance；右: high
bias</figcaption>
</figure>
<h3 id="stochastic-gradient-descent-随机梯度下降">Stochastic gradient
descent 随机梯度下降</h3>
<p>Batch gradient descent:
前述传统的梯度下降算法。batch意味着每次都要用到所有m个例子</p>
<p>Stochastic gradient descent：每次iteration用1个例子</p>
<ol type="1">
<li><p>Randomly shuffle dataset</p></li>
<li><p>Repeat：</p>
<p>for i =1,..., m { <span class="math inline">\(\theta_j=\theta_j-\alpha(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\)</span>
}</p>
<blockquote>
<p>Note: 和Batch gradient
descent不同的是，无需每一步都要计算全部的training example</p>
</blockquote></li>
</ol>
<p>并不一定会达到global minimum，最后可能在全局最优附近徘徊。</p>
<p>上述过程重复的次数取决于数据集的大小，一般为10次，若数据集特别大，则1次。</p>
<h3 id="mini-batch-gradient-descent">Mini-Batch gradient descent</h3>
<p>每次iteration用b（mini-batch size）个例子，常用2-100</p>
<p>第i个例子，直到第i+b-1个例子</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210415223239627.png" alt="mini-batch gradient descent">
<figcaption aria-hidden="true">mini-batch gradient descent</figcaption>
</figure>
<p>只有当have a good vetorized implementation
，mini-batch才能比stochastic表现得更好。（并行运算）</p>
<h3 id="stochastic-gradient-descent-convergence">Stochastic gradient
descent convergence</h3>
<p>收敛性：</p>
<ul>
<li><figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210416223947732.png" alt="Algorithm">
<figcaption aria-hidden="true">Algorithm</figcaption>
</figure></li>
<li><p>更小的学习速率学习曲线更平滑，有可能会得到slightly
better结果。（cost关于no. of iteration 的曲线）</p></li>
<li><p>增加1000这个数值，会使得曲线更加平滑</p></li>
<li><p>如果学习曲线在上升，就要用更小的α</p></li>
<li><p>如果学习曲线几乎在一个水平上，那么就需要增加特征等</p></li>
<li><p>关于学习速率的选择：</p>
<ul>
<li>Learning rate α is typically held constant 在随机梯度下降中</li>
<li>Can slowly decrease α over time if we want θ to converge.
（例如：<span class="math inline">\(\alpha=\frac{const1}{iteration
Number+const2}\)</span>）
<ul>
<li>但是这里，两个参数的调整tune会很费时间，所以很少采用这种方法</li>
</ul></li>
</ul></li>
</ul>
<h2 id="advanced-topics">Advanced Topics</h2>
<p>### Online Learning</p>
<ul>
<li><p>连续的数据流：用户进入离开</p></li>
<li><p><span class="math inline">\(p(y=1|x;\theta)\)</span>其中x表示价格，可以用logistic
regression或者神经网络</p></li>
<li><p>这里考虑逻辑回归：</p>
<p>Repeat forever {</p>
<p>​ Get (x,y) corresponding to user.</p>
<p>​ Update θ using (x,y):</p>
<p>​ <span class="math inline">\(\theta_j:=\theta_j-\alpha(h_{\theta}(x)-y)x_j\)</span>
(j=0, ..., n)</p>
<p>}</p></li>
<li><p>这个算法可以用于学习用户喜好</p>
<ul>
<li>Choosing special offers to show user</li>
<li>Customized selection of news articles</li>
<li>product recommendation/search</li>
</ul></li>
</ul>
<h3 id="map-reduce-and-data-parallelism">Map reduce and data
parallelism</h3>
<p>Map reduce 可以处理更多数据。思想来自于Jeffrey Dean和Sanjay
Ghemawat。summation over the training set。即并行处理</p>
<ul>
<li>神经网络也同样可以并行计算前向和后向propagation</li>
<li>有时只需要向量化实现算法，不需要多核并行</li>
<li>Hadoop是开源map reduce</li>
<li>由于并行的latency，速度会少于N倍 的单核心</li>
</ul>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Week09</title>
    <url>/Machine-Learning-Week09/</url>
    <content><![CDATA[<h1 id="anomaly-detection-异常检测">Anomaly Detection 异常检测</h1>
<h2 id="density-estimation">Density Estimation</h2>
<h3 id="problem-motivation">Problem Motivation</h3>
<p>异常检测主要用于unsupervised learning</p>
<p>Model：<span class="math inline">\(P(x_{test}&lt;\epsilon)\rightarrow
anomaly\)</span></p>
<p>​ <span class="math inline">\(P(x_{test}\ge \epsilon)\rightarrow
OK\)</span></p>
<p>也被用于检测账号被盗、生产、数据中心的电脑</p>
<h3 id="gaussian-distribution">Gaussian Distribution</h3>
<p>=Normal distribution</p>
<p>X ～ N (0, 1)，“distributed as”, 这里N的符号为"script N"</p>
<p>给定dataset，参数估计：via Maximum likelihood estimation
通常是m-1，对大数据集没影响 <span class="math display">\[
\mu = \frac{1}{m}\sum_{i=1}^mx^{(i)}\\
\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)^2
\]</span></p>
<h3 id="algorithm">Algorithm</h3>
<p>基本假设，各特征间相互独立 <span class="math display">\[
P(x)=\prod_{j=1}^np(x_j;\mu_j,\sigma_j^2)
\]</span> <img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210404220930233.png" alt="Anomaly detection algorithm"></p>
<h2 id="building-an-anomaly-detection-system">Building an Anomaly
Detection System</h2>
<h3 id="developing-and-evaluating-an-anomaly-detection-system">Developing
and Evaluating an Anomaly Detection System</h3>
<p>Training set: normal examples (可以有少量异常的混进来)</p>
<p>然后应用于cross calidation set 和 test set</p>
<ul>
<li><p>10000正常，20异常：</p>
<p>​ 训练集：6000正常</p>
<p>​ CV：2000正常，10异常</p>
<p>​ Test：2000正常，10异常</p></li>
</ul>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210405114004132.png" alt="Algorithm evaluation">
<figcaption aria-hidden="true">Algorithm evaluation</figcaption>
</figure>
<p>由于normal是0，所以预测0常常有更高的准确率，是skew的偏斜的。因此查准率accuracy不是很好的分析指标。</p>
<ul>
<li>选择ε时，可以试几个不同的，然后选择使得Cross
validation集中F1-score最大的，或者效果最好的。</li>
<li>有时还需要决定用什么特征</li>
</ul>
<h3 id="anomaly-detection-vs.-supervised-learning">Anomaly detection vs.
Supervised learning</h3>
<p>Anomaly detection:</p>
<ul>
<li>y=1非常少(0-20常见), y=0占比更大</li>
<li>有大量不同的异常类型，其他算法难于分辨各种类型</li>
<li>有未知异常类型</li>
</ul>
<p>supervised learning：</p>
<ul>
<li>大量的positive 和negative examples</li>
<li>有足够positive examples，并且未来的positive
example和训练集的相似</li>
<li>Spam</li>
</ul>
<h3 id="choosing-what-features-to-use">Choosing what features to
use</h3>
<ul>
<li>Non-gaussian feature</li>
</ul>
<p>绘制特征的直方图，看是否大致是个bell shaped
curve，如果不是，运用一些变换使得其看起来更加gaussian</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210405203939050.png" alt="take a log transformation">
<figcaption aria-hidden="true">take a log transformation</figcaption>
</figure>
<ul>
<li><p>Error analysis for anomaly detection</p>
<p>甄别失败时，new feature
可能有帮助，例如组合现有的特征形成新特征</p></li>
</ul>
<h2 id="multivariate-gaussian-distribution">Multivariate Gaussian
Distribution</h2>
<h3 id="multivariate-gaussian-distribution-1">Multivariate Gaussian
Distribution</h3>
<p>不同于之前的是，p(x)不再是各个特征概率的乘积。</p>
<p>Parameters: <span class="math inline">\(\mu \in \R^n\)</span>, <span class="math inline">\(\Sigma \in \R^{n\times n}\)</span> <span class="math display">\[
p(x;\mu,\Sigma)=
\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^2}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
\]</span> <span class="math inline">\(|\Sigma|\)</span>=determinant of
Σ</p>
<p><img src="https://cdn.jsdelivr.net/gh/xuJ14/ImageHost@latest/images/image-20210405214331278.png" alt="examples"> <span class="math display">\[
\mu = \frac{1}{m}\sum_{i=1}^mx^{(i)}\\
\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T
\]</span></p>
<h3 id="anomaly-detection-using-the-multivariate-gaussian-distribution">Anomaly
Detection using the multivariate Gaussian distribution</h3>
<p>首先计算μ和Σ，然后计算p(x)，最后根据ε判断是否异常</p>
<ul>
<li><p>优点：自动捕捉特征间的相关性。m&gt;10n用起来才比较好，必须要m&gt;n，否则协方差矩阵不可逆。</p></li>
<li><p>缺点：计算量比较大。如果某个特征是其他的线性组合，那么协方差矩阵也不可逆，要注意避免。</p></li>
</ul>
<p>前述的模型：优点在于计算量小，但是要人工创造一些新特征来更好地分辨。m&lt;n的时候也能用。</p>
<p>若出现协方差矩阵不可逆：</p>
<ul>
<li>首先检查m, n大小</li>
<li>检查是否有redundant features，就是特征间是否线性独立</li>
</ul>
<h2 id="predicting-movie-ratings-recommendation-system">Predicting Movie
Ratings ——Recommendation System</h2>
<h3 id="problem-formulation">Problem Formulation</h3>
<p>r(i,j) 表示第i个电影，j用户是否打分；</p>
<p>y(i,j) 表示该用户对该电影的打分值 ；</p>
<h3 id="content-based-recommendations">Content Based
Recommendations</h3>
<p>设电影特征n个</p>
<p>For each user j, learn a parameter <span class="math inline">\(\theta^{(j)}\in\R^{n+1}\)</span>. Predict user j
as rating movie i with <span class="math inline">\((\theta^{(j)})^Tx^{(i)}\)</span> stars. （注意
<span class="math inline">\(x_0=1\)</span>）。用线性回归</p>
<p>To learn <span class="math inline">\(\theta^{(j)}\)</span>： <span class="math display">\[
\min_{\Theta^{(j)}}\frac{1}{2m^{(j)}}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2m^{(j)}}\sum_{k=1}^{n}(\theta_k^{(j)})^2
\]</span> Note: 这里 <span class="math inline">\(m^{(j)}\)</span>可以去掉，对结果没有任何影响</p>
<p>优化目标也可以为： <span class="math display">\[
J(\Theta^{(1)},...,\Theta^{(n_u)})=\min_{\Theta^{(1)},...,\Theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2
\]</span> Gradient descent update: <span class="math display">\[
\theta_{k}^{(j)}:=\theta_{k}^{(j)}-\alpha \sum_{i: r(i,
j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)
x_{k}^{(i)}\ (\text { for } k=0)
\]</span></p>
<p><span class="math display">\[
\theta_{k}^{(j)}:=\theta_{k}^{(j)}-\alpha\left(\sum_{i: r(i,
j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i, j)}\right)
x_{k}^{(i)}+\lambda \theta_{k}^{(j)}\right)\ (\text { for } k \neq 0)
\]</span></p>
<p>k=0的时候不加正则项</p>
<h2 id="collaborative-filtering">Collaborative Filtering</h2>
<h3 id="collaborative-filtering-1">Collaborative Filtering</h3>
<ul>
<li><p>feature learning</p></li>
<li><p>Given <span class="math inline">\(\Theta^{(1)},...,\Theta^{(n_u)}\)</span>，to learn
<span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>;</p></li>
<li><p>算法类似7、8式</p></li>
<li><p><span class="math inline">\(用户的\Theta\rightarrow
电影特征x\rightarrow \Theta\rightarrow x\rightarrow...\)</span></p></li>
<li><p>每个用户都在帮助这个推荐系统更好地分类电影，从而更好地预测评分</p></li>
<li><p>算法：同时实现：</p>
<ol type="1">
<li><p>Given <span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>，estimate <span class="math inline">\(\theta^{(1)},...,\theta^{(n_u)}\)</span>： <span class="math display">\[
\min_{\Theta^{(j)}}\frac{1}{2m^{(j)}}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2m^{(j)}}\sum_{k=1}^{n}(\theta_k^{(j)})^2
\]</span></p></li>
<li><p>Given <span class="math inline">\(\theta^{(1)},...,\theta^{(n_u)}\)</span>，estimate
<span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>： <span class="math display">\[
\min _{x^{(1)}, \ldots, x^{\left(n_{m}\right)}} \frac{1}{2}
\sum_{i=1}^{n_{m}} \sum_{j: r(i,
j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i,
j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}}
\sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}
\]</span></p></li>
</ol>
<p>即：</p>
<p>Minimizing <span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>
and <span class="math inline">\(\theta^{(1)},...,\theta^{(n_u)}\)</span>
simultaneously： <span class="math display">\[
\min_{x^{(1)},...,x^{(n_m)}\\\theta^{(1)},...,\theta^{(n_u)}}J\left(x^{(1)},
\ldots, x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots,
\theta^{\left(n_{u}\right)}\right)
\]</span></p>
<p><span class="math display">\[
J\left(x^{(1)}, \ldots, x^{\left(n_{m}\right)}, \theta^{(1)}, \ldots,
\theta^{\left(n_{u}\right)}\right)=\frac{1}{2} \sum_{(i, j): r(i,
j)=1}\left(\left(\theta^{(j)}\right)^{T} x^{(i)}-y^{(i,
j)}\right)^{2}+\frac{\lambda}{2} \sum_{i=1}^{n_{m}}
\sum_{k=1}^{n}\left(x_{k}^{(i)}\right)^{2}+\frac{\lambda}{2}
\sum_{j=1}^{n_{u}} \sum_{k=1}^{n}\left(\theta_{k}^{(j)}\right)^{2}
\]</span></p>
<blockquote>
<p>该算法下，无需设定 <span class="math inline">\(x_0=1\)</span>,
也没有<span class="math inline">\(\theta_0\)</span>，因为算法会自行选择参数。该算法下：<span class="math inline">\(\theta\in\R^n\)</span>，<span class="math inline">\(x\in\R^n\)</span>，n是特征数</p>
</blockquote></li>
<li><p>算法实现：</p>
<ol type="1">
<li>Initialize <span class="math inline">\(x^{(1)},...,x^{(n_m)}\)</span>， <span class="math inline">\(\theta^{(1)},...,\theta^{(n_u)}\)</span> to small
random values</li>
<li>Minimize J using gradient descent (or an advanced optimization
algorithm)</li>
<li>For a user with parameters <span class="math inline">\(\theta\)</span> and a movie with (learned)
features x, predict a star rating of <span class="math inline">\(\theta^Tx\)</span></li>
</ol></li>
</ul>
<h2 id="low-rank-matrix-factorization">Low Rank Matrix
Factorization</h2>
<p>### Vectorization</p>
<ul>
<li><p>Given matrices X (each row containing features of a particular
movie) and Θ (each row containing the weights for those features for a
given user), then the full matrix Y of all predicted ratings of all
movies by all users is given simply by: <span class="math inline">\(Y =
X\Theta^T\)</span>.</p></li>
<li><p>Predicting how similar two movies i and j are can be done using
the distance between their respective feature vectors x. Specifically,
we are looking for a small value of <span class="math inline">\(||x^{(i)} - x^{(j)}||\)</span>.</p></li>
</ul>
<h3 id="mean-normalization">Mean Normalization</h3>
<p>If the ranking system for movies is used from the previous lectures,
then new users (who have watched no movies), will be assigned new movies
incorrectly. Specifically, they will be assigned θ with all components
equal to zero due to the minimization of the regularization term. That
is, we assume that the new user will rank all movies 0, which does not
seem intuitively correct. 新用户会被预测为0</p>
<p>We rectify this problem by normalizing the data relative to the mean.
First, we use a matrix Y to store the data from previous ratings, where
the ith row of Y is the ratings for the ith movie and the jth column
corresponds to the ratings for the jth user.</p>
<p>We can now define a vector <span class="math display">\[
\mu  = [\mu_1, \mu_2, \dots , \mu_{n_m}]
\]</span> such that <span class="math display">\[
\mu_i = \frac{\sum_{j:r(i,j)=1}{Y_{i,j}}}{\sum_{j}{r(i,j)}}
\]</span> Which is effectively the mean of the previous ratings for the
ith movie (where only movies that have been watched by users are
counted). We now can normalize the data by subtracting u, the mean
rating, from the actual ratings for each user (column in matrix Y):</p>
<p>As an example, consider the following matrix Y and mean ratings μ:
<span class="math display">\[
Y=\left[\begin{array}{cccc}
5 &amp; 5 &amp; 0 &amp; 0 \\
4 &amp; ? &amp; ? &amp; 0 \\
0 &amp; 0 &amp; 5 &amp; 4 \\
0 &amp; 0 &amp; 5 &amp; 0
\end{array}\right], \quad \mu=\left[\begin{array}{c}
2.5 \\
2 \\
2.25 \\
1.25
\end{array}\right]
\]</span> The resulting Y′ vector is: <span class="math display">\[
Y^{\prime}=\left[\begin{array}{cccc}
2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 \\
2 &amp; ? &amp; ? &amp; -2 \\
-2 .25 &amp; -2.25 &amp; 3.75 &amp; 1.25 \\
-1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25
\end{array}\right]
\]</span> Now we must slightly modify the linear regression prediction
to include the mean normalization term: <span class="math display">\[
(\theta^{(j)})^T x^{(i)} + \mu_i
\]</span> Now, for a new user, the initial predicted values will be
equal to the μ term instead of simply being initialized to zero, which
is more accurate.</p>
<blockquote>
<p>Note:
一般的均值归一化要除以range，即max-min，但是这里不需要，因为都有统一的scale</p>
</blockquote>
]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>This is my first blog!</title>
    <url>/This-is-my-first-bolg/</url>
    <content><![CDATA[<p>This is the first time for me to use Hexo to build my blog.</p>
<p>My major is <strong>Engineering Physics</strong> and
<strong>Financial Engineering</strong>.</p>
<p>Hope to share my experiences with you!</p>
<p><strong>Thank you</strong> for following my blog!</p>
]]></content>
      <tags>
        <tag>Essay</tag>
      </tags>
  </entry>
  <entry>
    <title>Reading List</title>
    <url>/Reading-List/</url>
    <content><![CDATA[<h1 id="machine-learning">1. Machine Learning</h1>
<h2 id="step-1">Step 1</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="">
Hands-On Machine learning with Scikit-Learn and Tensorflow</li>
<li><input type="checkbox" disabled="">
Python Machine Learning. Sebastian Raschka</li>
<li><input type="checkbox" disabled="">
Introduction to Statistical Learning with R</li>
<li><input type="checkbox" disabled="">
【参考书】机器学习. 周志华</li>
</ul>
<h2 id="step-2">Step 2</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="">
训练平台：Kaggle，天池大数据竞赛</li>
<li><input type="checkbox" disabled="">
Scikit-learn: machine learning in Python</li>
</ul>
<h2 id="step-3">Step 3</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="">
Deeplearning.ai —— Coursera</li>
<li><input type="checkbox" disabled="">
Deep learning - by Ian GoodFellow</li>
</ul>
<h2 id="step-4">Step 4</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="">
Elements of Statistical Learning</li>
<li><input type="checkbox" disabled="">
统计学习基础</li>
<li><input type="checkbox" disabled="">
订阅arxiv</li>
<li><input type="checkbox" disabled="">
关注顶会：ICML/NIPS/KDD</li>
</ul>
<h1 id="financial-engineering">2. Financial Engineering</h1>
<p><a href="https://www.zhihu.com/question/302531501/answer/534780501">金融工程专业需要哪些数学基础？
- 经管之家的回答 - 知乎</a></p>
]]></content>
      <tags>
        <tag>Learning Tips</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Algorithm and Data Structure</title>
    <url>/Python-Algorithm-and-Data-Structure/</url>
    <content><![CDATA[<h1 id="python-数据结构与算法分析">Python 数据结构与算法分析</h1>
<h2 id="stack">Stack</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Stack</span>: </span><br><span class="line">    <span class="string">'''Last in First out, insert to last place'''</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.items = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isEmpty</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items == []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, item</span>):</span><br><span class="line">        self.items.append(item)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pop</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items.pop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">peek</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.items)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Stack</span>: </span><br><span class="line">    <span class="string">'''Last in First out, insert first place'''</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.items = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isEmpty</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items == []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, item</span>):</span><br><span class="line">        self.items.insert(<span class="number">0</span>,item)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pop</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">peek</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.items)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parChecker</span>(<span class="params">symbolString</span>):</span><br><span class="line">    s = Stack()</span><br><span class="line">    balanced = <span class="literal">True</span></span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> index &lt; <span class="built_in">len</span>(symbolString) <span class="keyword">and</span> balanced:</span><br><span class="line">        symbol = symbolString[index]</span><br><span class="line">        <span class="keyword">if</span> symbol == <span class="string">'('</span>:</span><br><span class="line">            s.push(symbol)</span><br><span class="line">        <span class="keyword">elif</span> s.isEmpty():</span><br><span class="line">            balanced = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            s.pop()</span><br><span class="line">        index = index + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> balanced <span class="keyword">and</span> s.isEmpty():</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parChecker</span>(<span class="params">symbolString</span>):</span><br><span class="line">    s = Stack()</span><br><span class="line">    balanced = <span class="literal">True</span></span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> index &lt; <span class="built_in">len</span>(symbolString) <span class="keyword">and</span> balanced:</span><br><span class="line">        symbol = symbolString[index]</span><br><span class="line">        <span class="keyword">if</span> symbol <span class="keyword">in</span> <span class="string">'([{'</span>:</span><br><span class="line">            s.push(symbol)</span><br><span class="line">        <span class="keyword">elif</span> s.isEmpty():</span><br><span class="line">            balanced = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            top = s.pop()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> matches(top, symbol):</span><br><span class="line">                balanced = <span class="literal">False</span></span><br><span class="line">        index = index + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> balanced <span class="keyword">and</span> s.isEmpty():</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matches</span>(<span class="params"><span class="built_in">open</span>,close</span>):</span><br><span class="line">    opens = <span class="string">'([{'</span></span><br><span class="line">    closes = <span class="string">')]}'</span></span><br><span class="line">    <span class="keyword">return</span> opens.index(<span class="built_in">open</span>) == closes.index(close)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="queue">Queue</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Queue</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.items = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isEmpty</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items == []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">enqueue</span>(<span class="params">self, item</span>):</span><br><span class="line">        self.items.insert(<span class="number">0</span>,item)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dequeue</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items.pop()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.items)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hotPotato</span>(<span class="params">namelist, num</span>):</span><br><span class="line">    simqueue = Queue()</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> namelist:</span><br><span class="line">        simqueue.enqueue(name)</span><br><span class="line">    <span class="keyword">while</span> simqueue.size()&gt;<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">            simqueue.enqueue(simqueue.dequeue())</span><br><span class="line">        simqueue.dequeue()</span><br><span class="line">    <span class="keyword">return</span> simqueue.dequeue()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(hotPotato([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],<span class="number">7</span>))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>4</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Printer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,ppm</span>):</span><br><span class="line">        self.pagerate = ppm</span><br><span class="line">        self.currentTask = <span class="literal">None</span></span><br><span class="line">        self.timeRemaining = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tick</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.currentTask != <span class="literal">None</span>:</span><br><span class="line">            self.timeRemaining = self.timeRemaining - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> self.timeRemaining &lt;= <span class="number">0</span>:</span><br><span class="line">                self.currentTask = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">busy</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.currentTask != <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">startNext</span>(<span class="params">self, newtask</span>):</span><br><span class="line">        self.currentTask = newtask</span><br><span class="line">        self.timeRemaining = newtask.getPages() * <span class="number">60</span>/self.pagerate</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Task</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, time</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.timestamp = time</span><br><span class="line">        self.pages = random.randrange(<span class="number">1</span>,<span class="number">21</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getStamp</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.timestamp</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getPages</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.pages</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">waitTime</span>(<span class="params">self, currenttime</span>):</span><br><span class="line">        <span class="keyword">return</span> currenttime - self.timestamp</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newPrintTask</span>():</span><br><span class="line">    num = random.randrange(<span class="number">1</span>,<span class="number">181</span>)</span><br><span class="line">    <span class="keyword">if</span> num == <span class="number">180</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simulation</span>(<span class="params">numSeconds, pagesPerMinute</span>):</span><br><span class="line">    labprinter = Printer(pagesPerMinute)</span><br><span class="line">    printQueue = Queue()</span><br><span class="line">    waitingtimes = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> currentSecond <span class="keyword">in</span> <span class="built_in">range</span>(numSeconds):</span><br><span class="line">        <span class="keyword">if</span> newPrintTask():</span><br><span class="line">            task = Task(currentSecond)</span><br><span class="line">            printQueue.enqueue(task)</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> labprinter.busy()) <span class="keyword">and</span> (<span class="keyword">not</span> printQueue.isEmpty()):</span><br><span class="line">            nexttask = printQueue.dequeue()</span><br><span class="line">            waitingtimes.append(nexttask.waitTime(currentSecond))</span><br><span class="line">            labprinter.startNext(nexttask)</span><br><span class="line">        labprinter.tick()</span><br><span class="line">    averageWait = <span class="built_in">sum</span>(waitingtimes)/<span class="built_in">len</span>(waitingtimes)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Average wait <span class="subst">{averageWait:<span class="number">8.2</span>f}</span> secs <span class="subst">{printQueue.size():3d}</span> tasks remaining.'</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    simulation(<span class="number">3600</span>,<span class="number">5</span>)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>Average wait    49.06 secs   0 tasks remaining.
Average wait   129.56 secs   0 tasks remaining.
Average wait    29.47 secs   0 tasks remaining.
Average wait    76.53 secs   1 tasks remaining.
Average wait    57.75 secs   0 tasks remaining.
Average wait    17.36 secs   1 tasks remaining.
Average wait    99.14 secs   0 tasks remaining.
Average wait   253.56 secs   0 tasks remaining.
Average wait   115.83 secs   0 tasks remaining.
Average wait   265.00 secs   4 tasks remaining.</code></pre>
<h3 id="双端队列">双端队列</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Deque</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.items = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isEmpty</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items == []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addFront</span>(<span class="params">self, item</span>):</span><br><span class="line">        self.items.append(item)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addRear</span>(<span class="params">self, item</span>):</span><br><span class="line">        self.items.insert(<span class="number">0</span>,item)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeFront</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items.pop()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeRear</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.items.pop(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.items)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="无序表">无序表</h3>
<p>链表实现无序表</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, initdata</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.data = initdata</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getData</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getNext</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">next</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setData</span>(<span class="params">self, newdata</span>):</span><br><span class="line">        self.data = newdata</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setNext</span>(<span class="params">self,newnext</span>):</span><br><span class="line">        self.<span class="built_in">next</span> = newnext</span><br></pre></td></tr></tbody></table></figure>
<h2 id="递归">递归</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 调整sys最大递归深度</span></span><br><span class="line"><span class="comment"># import sys</span></span><br><span class="line"><span class="comment"># sys.getrecursionlimit()</span></span><br><span class="line"><span class="comment"># sys.setrecursionlimit()</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="无序表-1">无序表</h3>
<p>用链表实现无序表：基本元素：节点<br>
无序表本身不包含数据，只包含节点的引用<br>
添加新数据最快的位置是head</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, initdata</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.data = initdata</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getData</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getNext</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">next</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setData</span>(<span class="params">self,newdata</span>):</span><br><span class="line">        self.data = newdata</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setNext</span>(<span class="params">self, newnext</span>):</span><br><span class="line">        self.<span class="built_in">next</span> = newnext</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UnorderedList</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># head 对第一个节点的引用 </span></span><br><span class="line">        self.head = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, item</span>):</span><br><span class="line">        temp = Node(item)</span><br><span class="line">        temp.setNext(self.head)</span><br><span class="line">        self.head = temp</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">size</span>(<span class="params">self</span>):</span><br><span class="line">        current = self.head</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> current != <span class="literal">None</span>:</span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">            current = current.getNext()</span><br><span class="line">        <span class="keyword">return</span> count</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, item</span>):</span><br><span class="line">        current = self.head</span><br><span class="line">        found = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> current != <span class="literal">None</span> <span class="keyword">and</span> <span class="keyword">not</span> found:</span><br><span class="line">            <span class="keyword">if</span> current.getData() == item:</span><br><span class="line">                found = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current = current.getNext()</span><br><span class="line">        <span class="keyword">return</span> found</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">remove</span>(<span class="params">self,item</span>):</span><br><span class="line">        current = self.head</span><br><span class="line">        previous = <span class="literal">None</span></span><br><span class="line">        found = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> found:</span><br><span class="line">            <span class="keyword">if</span> current.getData() == item:</span><br><span class="line">                found = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                previous = current</span><br><span class="line">                current = current.getNext()</span><br><span class="line">        <span class="keyword">if</span> previous == <span class="literal">None</span>:</span><br><span class="line">            self.head = current.getNext()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            previous.setNext(current.getNext())   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="有序表">有序表</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OrderedList</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.head = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, item</span>):</span><br><span class="line">        current = self.head</span><br><span class="line">        found = <span class="literal">False</span></span><br><span class="line">        stop = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span> current != <span class="literal">None</span> <span class="keyword">and</span> <span class="keyword">not</span> found <span class="keyword">and</span> <span class="keyword">not</span> stop:</span><br><span class="line">            <span class="keyword">if</span> current.getData() == item:</span><br><span class="line">                found = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> current.getData() &gt; item:</span><br><span class="line">                    stop = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    current = current.getNext()</span><br><span class="line">        <span class="keyword">return</span> found</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, item</span>): </span><br><span class="line">        current = self.head </span><br><span class="line">        previous = <span class="literal">None</span> </span><br><span class="line">        stop = <span class="literal">False</span> </span><br><span class="line">        <span class="keyword">while</span> current != <span class="literal">None</span> <span class="keyword">and</span> <span class="keyword">not</span> stop: </span><br><span class="line">            <span class="keyword">if</span> current.getData() &gt; item: </span><br><span class="line">                stop = <span class="literal">True</span> </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                previous = current </span><br><span class="line">                current = current.getNext() </span><br><span class="line">        temp = Node(item) </span><br><span class="line">        <span class="keyword">if</span> previous == <span class="literal">None</span>: </span><br><span class="line">            temp.setNext(self.head) </span><br><span class="line">            self.head = temp </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            temp.setNext(current) </span><br><span class="line">            previous.setNext(temp)</span><br><span class="line">    <span class="comment"># 剩下的操作幼自己写： size, remove</span></span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure>
<h3 id="线性结构小结">线性结构小结</h3>
<ul>
<li>stack：后进先出</li>
<li>Queue：先进先出</li>
<li>双端队列：可以同时具备stack和queue的功能</li>
<li>链表List：不需要连续存储空间，head需要特别处理
<ul>
<li>无序表</li>
<li>有序表</li>
</ul></li>
</ul>
<h3 id="递归应用">递归应用</h3>
<p>重复计算太多， 可以记录中间结果</p>
<p>找零</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recMC</span>(<span class="params">coinValueList, change</span>):</span><br><span class="line">    minCoins = change</span><br><span class="line">    <span class="keyword">if</span> change <span class="keyword">in</span> coinValueList:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> [c <span class="keyword">for</span> c <span class="keyword">in</span> coinValueList <span class="keyword">if</span> c&lt;=change]:</span><br><span class="line">            numCoins = <span class="number">1</span> + recMC(coinValueList, change-i)</span><br><span class="line">            <span class="keyword">if</span> numCoins &lt; minCoins:</span><br><span class="line">                minCoins = numCoins</span><br><span class="line">    <span class="keyword">return</span> minCoins</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">recMC([<span class="number">1</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">25</span>],<span class="number">63</span>)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>6</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">recDC</span>(<span class="params">coinValueList, change, knownResults</span>):</span><br><span class="line">    minCoins = change</span><br><span class="line">    <span class="keyword">if</span> change <span class="keyword">in</span> coinValueList:</span><br><span class="line">        knownResults[change] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> knownResults[change] &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> knownResults[change]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> [c <span class="keyword">for</span> c <span class="keyword">in</span> coinValueList <span class="keyword">if</span> c&lt;=change]:</span><br><span class="line">            numCoins = <span class="number">1</span> + recMC(coinValueList, change-i, knownResults)</span><br><span class="line">            <span class="keyword">if</span> numCoins &lt; minCoins:</span><br><span class="line">                minCoins = numCoins</span><br><span class="line">                knownResults[change] = minCoins</span><br><span class="line">    <span class="keyword">return</span> minCoins</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">recDC([<span class="number">25</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">1</span>], <span class="number">63</span>, [<span class="number">0</span>]*<span class="number">64</span>)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>6</code></pre>
<p>动态规划<br>
从最简单的情况开始，递增寻找最优</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dpMakeChange</span>(<span class="params">coinValueList, change, minCoins</span>):</span><br><span class="line">    <span class="keyword">for</span> cents <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,change+<span class="number">1</span>):</span><br><span class="line">        coinCount = cents</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> [c <span class="keyword">for</span> c <span class="keyword">in</span> coinValueList <span class="keyword">if</span> c&lt;=cents]:</span><br><span class="line">            <span class="keyword">if</span> minCoins[cents-j]+<span class="number">1</span>&lt;coinCount:</span><br><span class="line">                coinCount = minCoins[cents-j]+<span class="number">1</span></span><br><span class="line">        minCoins[cents] = coinCount</span><br><span class="line">    <span class="keyword">return</span> minCoins[change]</span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">dpMakeChange([<span class="number">1</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">21</span>,<span class="number">25</span>],<span class="number">63</span>,[<span class="number">0</span>]*<span class="number">64</span>)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>3</code></pre>
<h2 id="查找">查找</h2>
<p>顺序查找</p>
<p>无序表查找</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sequentialSearch</span>(<span class="params">alist, item</span>):</span><br><span class="line">    pos = <span class="number">0</span></span><br><span class="line">    found = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> pos &lt; <span class="built_in">len</span>(alist) <span class="keyword">and</span> <span class="keyword">not</span> found:</span><br><span class="line">        <span class="keyword">if</span> alist[pos] == item:</span><br><span class="line">            found = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pos = pos + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> found</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>有序表查找</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">orderedSequentialSearch</span>(<span class="params">alist, item</span>):</span><br><span class="line">    pos = <span class="number">0</span></span><br><span class="line">    found = <span class="literal">False</span></span><br><span class="line">    stop = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> pos &lt; <span class="built_in">len</span>(alist) <span class="keyword">and</span> <span class="keyword">not</span> found <span class="keyword">and</span> <span class="keyword">not</span> stop:</span><br><span class="line">        <span class="keyword">if</span> alist[pos] == item:</span><br><span class="line">            found = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> alist[pos] &gt; item:</span><br><span class="line">                stop = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pos = pos + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> found</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">testlist = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(orderedSequentialSearch(testlist, <span class="number">3</span>))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>True</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(sequentialSearch(testlist,<span class="number">8</span>))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>False</code></pre>
<p>有序表，二分查找：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binarySearch</span>(<span class="params">alist, item</span>):</span><br><span class="line">    first = <span class="number">0</span></span><br><span class="line">    last = <span class="built_in">len</span>(alist) - <span class="number">1</span></span><br><span class="line">    found = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> first&lt;=last <span class="keyword">and</span> <span class="keyword">not</span> found:</span><br><span class="line">        midpoint = (first + last)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> alist[midpoint] == item:</span><br><span class="line">            found = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> item &lt; alist[midpoint]:</span><br><span class="line">                last = midpoint - <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                first = midpoint + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> found</span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(binarySearch(testlist,<span class="number">4</span>))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>True</code></pre>
<p>递归实现二分法</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binarySearch</span>(<span class="params">alist, item</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(alist) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        midpoint = <span class="built_in">len</span>(alist)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> alist[midpoint] == item:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> item&lt;alist[midpoint]:</span><br><span class="line">                <span class="keyword">return</span> binarySearch(alist[:midpoint], item)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> binarySearch(alist[midpoint+<span class="number">1</span>:], item)</span><br><span class="line">                </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">binarySearch(testlist,<span class="number">3</span>)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>True</code></pre>
<p>有序表查找也有排序的开销，有时候数据经常变动的话，不如用无序表和顺序查找更快</p>
<h3 id="排序算法">排序算法</h3>
<p>冒泡排序<br>
两两相邻交换，最大项交换到最后，经过n-1次</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bubbleSort</span>(<span class="params">alist</span>):</span><br><span class="line">    <span class="keyword">for</span> passnum <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(alist)-<span class="number">1</span>,<span class="number">0</span>,-<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(passnum):</span><br><span class="line">            <span class="keyword">if</span> alist[i]&gt;alist[i+<span class="number">1</span>]:</span><br><span class="line">                alist[i], alist[i+<span class="number">1</span>] = alist[i+<span class="number">1</span>], alist[i]</span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">bubbleSort(alist)</span><br><span class="line"><span class="built_in">print</span>(alist)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<pre><code>[17, 20, 26, 31, 44, 54, 55, 77, 93]</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">shortBubbleSort</span>(<span class="params">alist</span>):</span><br><span class="line">    exchanges = <span class="literal">True</span></span><br><span class="line">    passnum = <span class="built_in">len</span>(alist)-<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> passnum &gt; <span class="number">0</span> <span class="keyword">and</span> exchanges:</span><br><span class="line">        exchanges = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(passnum):</span><br><span class="line">            <span class="keyword">if</span> alist[i] &gt; alist[i+<span class="number">1</span>]:</span><br><span class="line">                exchanges = <span class="literal">True</span></span><br><span class="line">                alist[i],alist[i+<span class="number">1</span>] = alist[i+<span class="number">1</span>], alist[i]</span><br><span class="line">        passnum = passnum - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">alist = [<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">90</span>,<span class="number">50</span>,<span class="number">60</span>,<span class="number">70</span>,<span class="number">80</span>,<span class="number">100</span>,<span class="number">110</span>]</span><br><span class="line">shortBubbleSort(alist)</span><br><span class="line"><span class="built_in">print</span>(alist)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<pre><code>[20, 30, 40, 50, 60, 70, 80, 90, 100, 110]</code></pre>
<p>选择排序</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">selectionSort</span>(<span class="params">alist</span>):</span><br><span class="line">    <span class="keyword">for</span> fillslot <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(alist)-<span class="number">1</span>,<span class="number">0</span>,-<span class="number">1</span>):</span><br><span class="line">        positionOfMax = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> location <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,fillslot+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> alist[location]&gt;alist[positionOfMax]:</span><br><span class="line">                positionOfMax = location</span><br><span class="line">        alist[fillslot], alist[positionOfMax] = alist[positionOfMax],alist[fillslot]</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">selectionSort(alist)</span><br><span class="line"><span class="built_in">print</span>(alist)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[17, 20, 26, 31, 44, 54, 55, 77, 93]</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">insertionSort</span>(<span class="params">alist</span>):</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(alist)):</span><br><span class="line">        currentvalue = alist[index]</span><br><span class="line">        position = index</span><br><span class="line">        <span class="keyword">while</span> position&gt;<span class="number">0</span> <span class="keyword">and</span> alist[position-<span class="number">1</span>]&gt;currentvalue:</span><br><span class="line">            alist[position] = alist[position-<span class="number">1</span>]</span><br><span class="line">            position = position - <span class="number">1</span></span><br><span class="line">        alist[position] = currentvalue</span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">insertionSort(alist)</span><br><span class="line"><span class="built_in">print</span>(alist)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[17, 20, 26, 31, 44, 54, 55, 77, 93]</code></pre>
<p>谢尔排序</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">shellSort</span>(<span class="params">alist</span>):</span><br><span class="line">    sublistcount = <span class="built_in">len</span>(alist)//<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> sublistcount&gt;<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> startposition <span class="keyword">in</span> <span class="built_in">range</span>(sublistcount):</span><br><span class="line">            gapInsertionSort(alist, startposition, sublistcount)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'After increments of size'</span>,sublistcount,<span class="string">'The list is'</span>,alist)</span><br><span class="line">        sublistcount = sublistcount // <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gapInsertionSort</span>(<span class="params">alist,start,gap</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start+gap,<span class="built_in">len</span>(alist),gap):</span><br><span class="line">        currentvalue = alist[i]</span><br><span class="line">        position = i</span><br><span class="line">        <span class="keyword">while</span> position&gt;=gap <span class="keyword">and</span> alist[position-gap]&gt;currentvalue:</span><br><span class="line">            alist[position] = alist[position-gap]</span><br><span class="line">            position = position - gap</span><br><span class="line">        alist[position] = currentvalue</span><br><span class="line">    </span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">shellSort(alist)</span><br><span class="line"><span class="built_in">print</span>(alist)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>After increments of size 4 The list is [20, 26, 44, 17, 54, 31, 93, 55, 77]
After increments of size 2 The list is [20, 17, 44, 26, 54, 31, 77, 55, 93]
After increments of size 1 The list is [17, 20, 26, 31, 44, 54, 55, 77, 93]
[17, 20, 26, 31, 44, 54, 55, 77, 93]</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>,<span class="number">20</span>,<span class="number">4</span>))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[0, 4, 8, 12, 16]</code></pre>
<p>归并排序</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> left</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mergeSort</span>(<span class="params">alist</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(alist)&gt;<span class="number">1</span>:</span><br><span class="line">        mid = <span class="built_in">len</span>(alist)//<span class="number">2</span></span><br><span class="line">        lefthalf = alist[:mid]</span><br><span class="line">        righthalf = alist[mid:]</span><br><span class="line"></span><br><span class="line">        mergeSort(lefthalf)</span><br><span class="line">        mergeSort(righthalf)</span><br><span class="line"></span><br><span class="line">        i= j= k= <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i&lt;<span class="built_in">len</span>(lefthalf) <span class="keyword">and</span> j&lt;<span class="built_in">len</span>(righthalf):</span><br><span class="line">            <span class="keyword">if</span> lefthalf[i]&lt;righthalf[j]:</span><br><span class="line">                alist[k]=lefthalf[i]</span><br><span class="line">                i=i+<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                alist[k]=righthalf[j]</span><br><span class="line">                j=j+<span class="number">1</span></span><br><span class="line">            k=k+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> i&lt;<span class="built_in">len</span>(lefthalf):</span><br><span class="line">            alist[k] = lefthalf[i]</span><br><span class="line">            i=i+<span class="number">1</span></span><br><span class="line">            k=k+<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> j&lt;<span class="built_in">len</span>(righthalf):</span><br><span class="line">            alist[k]=righthalf[j]</span><br><span class="line">            j=j+<span class="number">1</span></span><br><span class="line">            k=k+<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">mergeSort(alist)</span><br><span class="line"><span class="built_in">print</span>(alist)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[17, 20, 26, 31, 44, 54, 55, 77, 93]</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mergeSort</span>(<span class="params">lst</span>):</span><br><span class="line">    <span class="comment"># 递归结束条件</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(lst)&lt;=<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> lst</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分解问题， 并递归调用</span></span><br><span class="line">    middle = <span class="built_in">len</span>(lst)//<span class="number">2</span></span><br><span class="line">    left = mergeSort(lst[:middle]) <span class="comment"># 左半部排好序</span></span><br><span class="line">    right  = mergeSort(lst[middle:]) <span class="comment"># 右半部排好序</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 合并左右半部，完成排序</span></span><br><span class="line">    merged = []</span><br><span class="line">    <span class="keyword">while</span> left <span class="keyword">and</span> right:</span><br><span class="line">        <span class="keyword">if</span> left[<span class="number">0</span>] &lt;= right[<span class="number">0</span>]:</span><br><span class="line">            merged.append(left.pop(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            merged.append(right.pop(<span class="number">0</span>))</span><br><span class="line">    merged.extend(right <span class="keyword">if</span> right <span class="keyword">else</span> left)</span><br><span class="line">    <span class="keyword">return</span> merged</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mergeSort(alist))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[17, 20, 26, 31, 44, 54, 55, 77, 93]</code></pre>
<p>快速排序</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span> right</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quickSort</span>(<span class="params">alist</span>):</span><br><span class="line">    quickSortHelper(alist,<span class="number">0</span>,<span class="built_in">len</span>(alist)-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quickSortHelper</span>(<span class="params">alist,first,last</span>):</span><br><span class="line">    <span class="keyword">if</span> first &lt; last:</span><br><span class="line">        splitpoint = partition(alist,first,last)</span><br><span class="line">        quickSortHelper(alist,first,splitpoint-<span class="number">1</span>)</span><br><span class="line">        quickSortHelper(alist,splitpoint+<span class="number">1</span>,last)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">partition</span>(<span class="params">alist,first,last</span>):</span><br><span class="line">    pivotvalue = alist[first]</span><br><span class="line"></span><br><span class="line">    leftmark = first + <span class="number">1</span></span><br><span class="line">    rightmark = last</span><br><span class="line"></span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="keyword">while</span> alist[leftmark] &lt;= pivotvalue <span class="keyword">and</span> leftmark &lt;= rightmark:</span><br><span class="line">            leftmark = leftmark +<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> alist[rightmark] &gt;= pivotvalue <span class="keyword">and</span> rightmark &gt;=leftmark:</span><br><span class="line">            rightmark = rightmark - <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> rightmark &lt; leftmark:</span><br><span class="line">            done = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            alist[leftmark], alist[rightmark] = alist[rightmark], alist[leftmark]</span><br><span class="line">    </span><br><span class="line">    alist[first], alist[rightmark] = alist[rightmark], alist[first]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rightmark</span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">quickSort(alist)</span><br><span class="line"><span class="built_in">print</span>(alist)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[17, 20, 26, 31, 44, 54, 55, 77, 93]</code></pre>
<h2 id="散列函数">散列函数</h2>
<p>MD5和SHA系列hash function: hashlib</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line">hashlib.md5(<span class="string">"hello world"</span>.encode(<span class="string">'utf-8'</span>)).hexdigest()</span><br></pre></td></tr></tbody></table></figure>
<pre><code>'5eb63bbbe01eeed093cb22bb8f5acdc3'</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">m = hashlib.md5()</span><br><span class="line">m.update(<span class="string">'hello world'</span>.encode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"><span class="built_in">print</span>(m.hexdigest())</span><br></pre></td></tr></tbody></table></figure>
<pre><code>&lt;md5 _hashlib.HASH object @ 0x000002345866C390&gt;
5eb63bbbe01eeed093cb22bb8f5acdc3</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">m.update(<span class="string">'nihao shijie'</span>.encode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"><span class="built_in">print</span>(m.hexdigest())</span><br></pre></td></tr></tbody></table></figure>
<pre><code>&lt;md5 _hashlib.HASH object @ 0x000002345866C390&gt;
215aa34b159e42f10d53a1187363b520</code></pre>
<p>冲突解决方案：开放定址（线性探测linear probing）</p>
<h2 id="树">树</h2>
<p>嵌套列表表示树</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">BinaryTree</span>(<span class="params">r</span>): </span><br><span class="line">    <span class="keyword">return</span> [r, [], []]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">insertLeft</span>(<span class="params">root, newBranch</span>): </span><br><span class="line">    t = root.pop(<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(t) &gt; <span class="number">1</span>: </span><br><span class="line">        root.insert(<span class="number">1</span>, [newBranch, t, []]) </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        root.insert(<span class="number">1</span>, [newBranch, [], []]) </span><br><span class="line">    <span class="keyword">return</span> root</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">insertRight</span>(<span class="params">root, newBranch</span>): </span><br><span class="line">    t = root.pop(<span class="number">2</span>) </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(t) &gt; <span class="number">1</span>: </span><br><span class="line">        root.insert(<span class="number">2</span>, [newBranch, [], t]) </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        root.insert(<span class="number">2</span>, [newBranch, [], []]) </span><br><span class="line">    <span class="keyword">return</span> root</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getRootVal</span>(<span class="params">root</span>): </span><br><span class="line">    <span class="keyword">return</span> root[<span class="number">0</span>] </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setRootVal</span>(<span class="params">root, newVal</span>): </span><br><span class="line">    root[<span class="number">0</span>] = newVal </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getLeftChild</span>(<span class="params">root</span>): </span><br><span class="line">    <span class="keyword">return</span> root[<span class="number">1</span>] </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getRightChild</span>(<span class="params">root</span>): </span><br><span class="line">    <span class="keyword">return</span> root[<span class="number">2</span>] </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">r = BinaryTree(<span class="number">3</span>)</span><br><span class="line">insertLeft(r,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line">insertLeft(r,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line">insertRight(r,<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line">l = getLeftChild(r)</span><br><span class="line"><span class="built_in">print</span>(l)</span><br><span class="line">setRootVal(l,<span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line">insertLeft(l,<span class="number">11</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line"><span class="built_in">print</span>(getRightChild(getRightChild(r)))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>[3, [4, [], []], []]
[3, [5, [4, [], []], []], []]
[3, [5, [4, [], []], []], [6, [], []]]
[5, [4, [], []], []]
[3, [9, [4, [], []], []], [6, [], []]]
[3, [9, [11, [4, [], []], []], []], [6, [], []]]
[]</code></pre>
<p>节点链接表示树：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BinaryTree</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rootObj</span>): </span><br><span class="line">        self.key = rootObj </span><br><span class="line">        self.leftChild = <span class="literal">None</span> </span><br><span class="line">        self.rightChild = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insertLeft</span>(<span class="params">self, newNode</span>): </span><br><span class="line">        <span class="keyword">if</span> self.leftChild == <span class="literal">None</span>: </span><br><span class="line">            self.leftChild = BinaryTree(newNode) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            t = BinaryTree(newNode) </span><br><span class="line">            t.left = self.leftChild </span><br><span class="line">            self.leftChild = t </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insertRight</span>(<span class="params">self, newNode</span>): </span><br><span class="line">        <span class="keyword">if</span> self.rightChild == <span class="literal">None</span>: </span><br><span class="line">            self.rightChild = BinaryTree(newNode) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            t = BinaryTree(newNode) </span><br><span class="line">            t.right = self.rightChild </span><br><span class="line">            self.rightChild = t</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getRightChild</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.rightChild </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getLeftChild</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.leftChild </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setRootVal</span>(<span class="params">self, obj</span>): </span><br><span class="line">        self.key = obj </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getRootVal</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.key</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">r = BinaryTree(<span class="string">'a'</span>)</span><br><span class="line"><span class="built_in">print</span>(r.getRootVal())</span><br><span class="line">r.insertLeft(<span class="string">'b'</span>)</span><br><span class="line"><span class="built_in">print</span>(r.getLeftChild().getRootVal())</span><br><span class="line">r.insertRight(<span class="string">'c'</span>)</span><br><span class="line">r.getRightChild().setRootVal(<span class="string">'hello'</span>)</span><br><span class="line"><span class="built_in">print</span>(r.getRightChild().getRootVal())</span><br></pre></td></tr></tbody></table></figure>
<pre><code>a
b
hello</code></pre>
<p>可以将表达式表示为树</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">buildParseTree</span>(<span class="params">fpexp</span>): </span><br><span class="line">    <span class="comment"># fplist = fpexp.split()</span></span><br><span class="line">    fplist = <span class="built_in">list</span>(fpexp)</span><br><span class="line">    pStack = Stack() </span><br><span class="line">    eTree = BinaryTree(<span class="string">''</span>) </span><br><span class="line">    pStack.push(eTree) </span><br><span class="line">    currentTree = eTree </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> fplist: </span><br><span class="line">        <span class="keyword">if</span> i == <span class="string">'('</span>: </span><br><span class="line">            currentTree.insertLeft(<span class="string">''</span>) </span><br><span class="line">            pStack.push(currentTree) </span><br><span class="line">            currentTree = currentTree.getLeftChild() </span><br><span class="line">        <span class="keyword">elif</span> i <span class="keyword">not</span> <span class="keyword">in</span> <span class="string">'+-*/)'</span>: </span><br><span class="line">            currentTree.setRootVal(<span class="built_in">eval</span>(i)) </span><br><span class="line">            parent = pStack.pop() </span><br><span class="line">            currentTree = parent </span><br><span class="line">        <span class="keyword">elif</span> i <span class="keyword">in</span> <span class="string">'+-*/'</span>: </span><br><span class="line">            currentTree.setRootVal(i) </span><br><span class="line">            currentTree.insertRight(<span class="string">''</span>) </span><br><span class="line">            pStack.push(currentTree) </span><br><span class="line">            currentTree = currentTree.getRightChild() </span><br><span class="line">        <span class="keyword">elif</span> i == <span class="string">')'</span>: </span><br><span class="line">            currentTree = pStack.pop() </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Unknown Operator: "</span> + i) </span><br><span class="line">    <span class="keyword">return</span> eTree </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">buildParseTree(<span class="string">'((3+5)+(5*6))'</span>)</span><br></pre></td></tr></tbody></table></figure>
<pre><code>&lt;__main__.BinaryTree at 0x23456a3b4f0&gt;</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">parseTree</span>): </span><br><span class="line">    opers = {<span class="string">'+'</span>:operator.add, <span class="string">'-'</span>:operator.sub, <span class="string">'*'</span>:operator.mul, <span class="string">'/'</span>:operator.truediv} </span><br><span class="line">    leftC = parseTree.getLeftChild() </span><br><span class="line">    rightC = parseTree.getRightChild() </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> leftC <span class="keyword">and</span> rightC: </span><br><span class="line">        fn = opers[parseTree.getRootVal()] </span><br><span class="line">        <span class="keyword">return</span> fn(evaluate(leftC), evaluate(rightC)) </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> parseTree.getRootVal() </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">evaluate(buildParseTree(<span class="string">'((3+5)+(5*6))'</span>))</span><br></pre></td></tr></tbody></table></figure>
<pre><code>38</code></pre>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tree = buildParseTree(<span class="string">'((3+5)+(5*6))'</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tree.getLeftChild().getLeftChild().getRootVal()</span><br></pre></td></tr></tbody></table></figure>
<pre><code>3</code></pre>
<p>树的遍历：<br>
前序遍历<br>
中序遍历<br>
后续遍历</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preorder</span>(<span class="params">tree</span>): </span><br><span class="line">    <span class="keyword">if</span> tree: </span><br><span class="line">        <span class="built_in">print</span>(tree.getRootVal()) </span><br><span class="line">        preorder(tree.getLeftChild()) </span><br><span class="line">        preorder(tree.getRightChild())</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postorder</span>(<span class="params">tree</span>): </span><br><span class="line">    <span class="keyword">if</span> tree != <span class="literal">None</span>: </span><br><span class="line">        postorder(tree.getLeftChild()) </span><br><span class="line">        postorder(tree.getRightChild()) </span><br><span class="line">        <span class="built_in">print</span>(tree.getRootVal()) </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">tree</span>): </span><br><span class="line">    <span class="keyword">if</span> tree != <span class="literal">None</span>: </span><br><span class="line">        inorder(tree.getLeftChild()) </span><br><span class="line">        <span class="built_in">print</span>(tree.getRootVal()) </span><br><span class="line">        inorder(tree.getRightChild())</span><br></pre></td></tr></tbody></table></figure>
<p>也可以写入binary tree class里</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preorder</span>(<span class="params">self</span>): </span><br><span class="line">    <span class="built_in">print</span>(self.key) </span><br><span class="line">    <span class="keyword">if</span> self.leftChild: </span><br><span class="line">        self.left.preorder() </span><br><span class="line">    <span class="keyword">if</span> self.rightChild: </span><br><span class="line">        self.right.preorder() </span><br></pre></td></tr></tbody></table></figure>
<h3 id="优先队列vip放在队首">优先队列（vip放在队首）</h3>
<p>按优先级排队：
二叉堆（能够将优先队列的入队和出队复杂度都保持在O（logn））<br>
完全二叉树保证平衡：如果节点的下标为p，那么左子节点下标为2p，右子节点下标为2p-1，父节点下标为p//2</p>
<p>堆次序：<br>
任何一个节点x，其父节点p中的key均小于x中的key，根节点的key最小</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BinHeap</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): </span><br><span class="line">        self.heapList = [<span class="number">0</span>] </span><br><span class="line">        self.currentSize = <span class="number">0</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">percUp</span>(<span class="params">self, i</span>): </span><br><span class="line">        <span class="comment"># 将新数据上浮到合适的位置（大小），以便执行插入操作</span></span><br><span class="line">        <span class="keyword">while</span> i // <span class="number">2</span> &gt; <span class="number">0</span>: </span><br><span class="line">            <span class="keyword">if</span> self.heapList[i] &lt; self.heapList[i // <span class="number">2</span>]: </span><br><span class="line">                self.heapList[i // <span class="number">2</span>], self.heapList[i] = self.heapList[i], self.heapList[i // <span class="number">2</span>]</span><br><span class="line">            i = i // <span class="number">2</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">insert</span>(<span class="params">self, k</span>): </span><br><span class="line">        self.heapList.append(k) </span><br><span class="line">        self.currentSize = self.currentSize + <span class="number">1</span> </span><br><span class="line">        self.percUp(self.currentSize)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">percDown</span>(<span class="params">self, i</span>): </span><br><span class="line">        <span class="comment"># 删除最小节点时，先将最后一个节点放到删除节点处，再下沉</span></span><br><span class="line">        <span class="keyword">while</span> (i * <span class="number">2</span>) &lt;= self.currentSize: </span><br><span class="line">            mc = self.minChild(i) </span><br><span class="line">            <span class="keyword">if</span> self.heapList[i] &gt; self.heapList[mc]: </span><br><span class="line">                self.heapList[i], self.heapList[mc] = self.heapList[mc], self.heapList[i]</span><br><span class="line">            i = mc </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minChild</span>(<span class="params">self, i</span>): </span><br><span class="line">        <span class="keyword">if</span> i * <span class="number">2</span> + <span class="number">1</span> &gt; self.currentSize: </span><br><span class="line">            <span class="keyword">return</span> i * <span class="number">2</span> </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">if</span> self.heapList[i*<span class="number">2</span>] &lt; self.heapList[i*<span class="number">2</span>+<span class="number">1</span>]: </span><br><span class="line">                <span class="keyword">return</span> i * <span class="number">2</span> </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">return</span> i * <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delMin</span>(<span class="params">self</span>): </span><br><span class="line">        retval = self.heapList[<span class="number">1</span>] </span><br><span class="line">        self.heapList[<span class="number">1</span>] = self.heapList[self.currentSize] </span><br><span class="line">        self.currentSize = self.currentSize - <span class="number">1</span> </span><br><span class="line">        self.heapList.pop() </span><br><span class="line">        self.percDown(<span class="number">1</span>) </span><br><span class="line">        <span class="keyword">return</span> retval </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buildHeap</span>(<span class="params">self, alist</span>): </span><br><span class="line">        <span class="comment"># 复杂度为O 1</span></span><br><span class="line">        i = <span class="built_in">len</span>(alist) // <span class="number">2</span></span><br><span class="line">        self.currentSize = <span class="built_in">len</span>(alist) </span><br><span class="line">        self.heapList = [<span class="number">0</span>] + alist[:] </span><br><span class="line">        <span class="keyword">while</span> (i &gt; <span class="number">0</span>): </span><br><span class="line">            self.percDown(i) </span><br><span class="line">            i = i - <span class="number">1</span> </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>堆排序复杂度O（nlogn）<br>
最大和最后一个叶子交换顺序后调整需要O(logn)， 总共n个节点。<br>
注：建堆的复杂度为O(n)</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">heapify</span>(<span class="params">arr, n, i</span>): </span><br><span class="line">    largest = i  </span><br><span class="line">    l = <span class="number">2</span> * i + <span class="number">1</span>     <span class="comment"># left = 2*i + 1 </span></span><br><span class="line">    r = <span class="number">2</span> * i + <span class="number">2</span>     <span class="comment"># right = 2*i + 2 </span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> l &lt; n <span class="keyword">and</span> arr[i] &lt; arr[l]: </span><br><span class="line">        largest = l </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> r &lt; n <span class="keyword">and</span> arr[largest] &lt; arr[r]: </span><br><span class="line">        largest = r </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> largest != i: </span><br><span class="line">        arr[i],arr[largest] = arr[largest],arr[i]  <span class="comment"># 交换</span></span><br><span class="line">  </span><br><span class="line">        heapify(arr, n, largest) </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">heapSort</span>(<span class="params">arr</span>): </span><br><span class="line">    n = <span class="built_in">len</span>(arr) </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Build a maxheap. </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n, -<span class="number">1</span>, -<span class="number">1</span>): </span><br><span class="line">        heapify(arr, n, i) </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 一个个交换元素</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n-<span class="number">1</span>, <span class="number">0</span>, -<span class="number">1</span>): </span><br><span class="line">        arr[i], arr[<span class="number">0</span>] = arr[<span class="number">0</span>], arr[i]   <span class="comment"># 交换</span></span><br><span class="line">        heapify(arr, i, <span class="number">0</span>) </span><br><span class="line">  </span><br><span class="line">arr = [ <span class="number">12</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>] </span><br><span class="line">heapSort(arr) </span><br><span class="line">n = <span class="built_in">len</span>(arr) </span><br><span class="line"><span class="built_in">print</span> (<span class="string">"排序后"</span>) </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n): </span><br><span class="line">    <span class="built_in">print</span> (<span class="string">"%d"</span> %arr[i]),</span><br></pre></td></tr></tbody></table></figure>
<pre><code>排序后
5
6
7
11
12
13</code></pre>
<h2 id="二叉查找树">二叉查找树</h2>
<p>比父节点小的key都出现在左子树，比父节点大的出现在右子树</p>
<p>ADT
Map（抽象数据类型映射）之前已经有两种实现方法：1.有序表+二分搜索。2.
hash表。下面用二叉查找树实现</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TreeNode</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key, val, left=<span class="literal">None</span>, right=<span class="literal">None</span>, </span></span><br><span class="line"><span class="params">        parent=<span class="literal">None</span></span>): </span><br><span class="line">        self.key = key </span><br><span class="line">        self.payload = val </span><br><span class="line">        self.leftChild = left </span><br><span class="line">        self.rightChild = right </span><br><span class="line">        self.parent = parent </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasLeftChild</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.leftChild </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasRightChild</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.rightChild </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isLeftChild</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.parent <span class="keyword">and</span> self.parent.leftChild == self</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isRightChild</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.parent <span class="keyword">and</span> self.parent.rightChild == self </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isRoot</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> self.parent </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isLeaf</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> (self.rightChild <span class="keyword">or</span> self.leftChild) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasAnyChildren</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.rightChild <span class="keyword">or</span> self.leftChild </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasBothChildren</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.rightChild <span class="keyword">and</span> self.leftChild </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">replaceNodeData</span>(<span class="params">self, key, value, lc, rc</span>): </span><br><span class="line">        self.key = key </span><br><span class="line">        self.payload = value </span><br><span class="line">        self.leftChild = lc </span><br><span class="line">        self.rightChild = rc </span><br><span class="line">        <span class="keyword">if</span> self.hasLeftChild(): </span><br><span class="line">            self.leftChild.parent = self </span><br><span class="line">        <span class="keyword">if</span> self.hasRightChild(): </span><br><span class="line">            self.rightChild.parent = self</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">if</span> self: </span><br><span class="line">            <span class="keyword">if</span> self.hasLeftChild(): </span><br><span class="line">                <span class="keyword">for</span> elem <span class="keyword">in</span> self.leftChild: </span><br><span class="line">                    <span class="keyword">yield</span> elem </span><br><span class="line">            <span class="keyword">yield</span> self.key </span><br><span class="line">            <span class="keyword">if</span> self.hasRightChild(): </span><br><span class="line">                <span class="keyword">for</span> elem <span class="keyword">in</span> self.rightChild: </span><br><span class="line">                    <span class="keyword">yield</span> elem</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findSuccessor</span>(<span class="params">self</span>): </span><br><span class="line">        succ = <span class="literal">None</span> </span><br><span class="line">        <span class="keyword">if</span> self.hasRightChild(): </span><br><span class="line">            succ = self.rightChild.findMin() </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">if</span> self.parent: </span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild(): </span><br><span class="line">                    succ = self.parent </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    self.parent.rightChild = <span class="literal">None</span> </span><br><span class="line">                    succ = self.parent.findSuccessor() </span><br><span class="line">                    self.parent.rightChild = self </span><br><span class="line">        <span class="keyword">return</span> succ</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMin</span>(<span class="params">self</span>): </span><br><span class="line">        current = self </span><br><span class="line">        <span class="keyword">while</span> current.hasLeftChild(): </span><br><span class="line">            current = current.leftChild </span><br><span class="line">        <span class="keyword">return</span> current</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spliceOut</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">if</span> self.isLeaf(): </span><br><span class="line">            <span class="keyword">if</span> self.isLeftChild(): </span><br><span class="line">                self.parent.leftChild = <span class="literal">None</span> </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                self.parent.rightChild = <span class="literal">None</span> </span><br><span class="line">        <span class="keyword">elif</span> self.hasAnyChildren(): </span><br><span class="line">            <span class="keyword">if</span> self.hasLeftChild(): </span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild(): </span><br><span class="line">                    self.parent.leftChild = self.leftChild </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    self.parent.rightChild = self.leftChild </span><br><span class="line">                self.leftChild.parent = self.parent </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">if</span> self.isLeftChild(): </span><br><span class="line">                    self.parent.leftChild = self.rightChild </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    self.parent.rightChild = self.rightChild </span><br><span class="line">                self.rightChild.parent = self.parent</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BinarySearchTree</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): </span><br><span class="line">        self.root = <span class="literal">None</span> </span><br><span class="line">        self.size = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">length</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.size </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.size </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="comment"># for 循环</span></span><br><span class="line">        <span class="keyword">return</span> self.root.__iter__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">put</span>(<span class="params">self, key, val</span>): </span><br><span class="line">        <span class="keyword">if</span> self.root: </span><br><span class="line">            self._put(key, val, self.root) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            self.root = TreeNode(key, val) </span><br><span class="line">            self.size = self.size + <span class="number">1</span> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_put</span>(<span class="params">self, key, val, currentNode</span>): </span><br><span class="line">        <span class="keyword">if</span> key &lt; currentNode.key: </span><br><span class="line">            <span class="keyword">if</span> currentNode.hasLeftChild(): </span><br><span class="line">                self._put(key, val, currentNode.leftChild) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                currentNode.leftChild = TreeNode(key, val, parent=currentNode) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">if</span> currentNode.hasRightChild(): </span><br><span class="line">                self._put(key, val, currentNode.rightChild) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                currentNode.rightChild = TreeNode(key, val, parent=currentNode)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setitem__</span>(<span class="params">self, k, v</span>): </span><br><span class="line">        <span class="comment"># m[k] = v</span></span><br><span class="line">        self.put(k, v)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, key</span>): </span><br><span class="line">        <span class="keyword">if</span> self.root: </span><br><span class="line">            res = self._get(key, self.root) </span><br><span class="line">            <span class="keyword">if</span> res: </span><br><span class="line">                <span class="keyword">return</span> res.payload </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span> </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get</span>(<span class="params">self, key, currentNode</span>): </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> currentNode: </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span> </span><br><span class="line">        <span class="keyword">elif</span> currentNode.key == key: </span><br><span class="line">            <span class="keyword">return</span> currentNode </span><br><span class="line">        <span class="keyword">elif</span> key &lt; currentNode.key: </span><br><span class="line">            <span class="keyword">return</span> self._get(key, currentNode.leftChild)</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> self._get(key, currentNode.rightChild) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, key</span>): </span><br><span class="line">        <span class="keyword">return</span> self.get(key)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__contains__</span>(<span class="params">self, key</span>): </span><br><span class="line">        <span class="comment"># k in m</span></span><br><span class="line">        <span class="keyword">if</span> self._get(key, self.root): </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span> </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delete</span>(<span class="params">self, key</span>): </span><br><span class="line">        <span class="keyword">if</span> self.size &gt; <span class="number">1</span>: </span><br><span class="line">            nodeToRemove = self._get(key, self.root) </span><br><span class="line">            <span class="keyword">if</span> nodeToRemove: </span><br><span class="line">                self.remove(nodeToRemove) </span><br><span class="line">                self.size = self.size - <span class="number">1</span> </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">raise</span> KeyError(<span class="string">'Error, key not in tree'</span>) </span><br><span class="line">        <span class="keyword">elif</span> self.size == <span class="number">1</span> <span class="keyword">and</span> self.root.key == key: </span><br><span class="line">            self.root = <span class="literal">None</span> </span><br><span class="line">            self.size = self.size - <span class="number">1</span> </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">'Error, key not in tree'</span>) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__delitem__</span>(<span class="params">self, key</span>): </span><br><span class="line">        self.delete(key)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">remove</span>(<span class="params">self, currentNode</span>): </span><br><span class="line">        <span class="keyword">if</span> currentNode.isLeaf(): <span class="comment"># 叶子节点</span></span><br><span class="line">            <span class="keyword">if</span> currentNode == currentNode.parent.leftChild: </span><br><span class="line">                currentNode.parent.leftChild = <span class="literal">None</span> </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                currentNode.parent.rightChild = <span class="literal">None</span> </span><br><span class="line">        <span class="keyword">elif</span> currentNode.hasBothChildren(): <span class="comment"># 内部 </span></span><br><span class="line">            succ = currentNode.findSuccessor() </span><br><span class="line">            succ.spliceOut() </span><br><span class="line">            currentNode.key = succ.key </span><br><span class="line">            currentNode.payload = succ.payload </span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 只有一个子节点</span></span><br><span class="line">            <span class="keyword">if</span> currentNode.hasLeftChild(): </span><br><span class="line">                <span class="keyword">if</span> currentNode.isLeftChild(): </span><br><span class="line">                    currentNode.leftChild.parent = currentNode.parent </span><br><span class="line">                    currentNode.parent.leftChild = currentNode.leftChild </span><br><span class="line">                <span class="keyword">elif</span> currentNode.isRightChild(): </span><br><span class="line">                    currentNode.leftChild.parent = currentNode.parent </span><br><span class="line">                    currentNode.parent.rightChild = currentNode.leftChild </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    currentNode.replaceNodeData(currentNode.leftChild.key, </span><br><span class="line">                                            currentNode.leftChild.payload, </span><br><span class="line">                                            currentNode.leftChild.leftChild, </span><br><span class="line">                                            currentNode.leftChild.rightChild) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">if</span> currentNode.isLeftChild(): </span><br><span class="line">                    currentNode.rightChild.parent = currentNode.parent </span><br><span class="line">                    currentNode.parent.leftChild = currentNode.rightChild </span><br><span class="line">                <span class="keyword">elif</span> currentNode.isRightChild(): </span><br><span class="line">                    currentNode.rightChild.parent = currentNode.parent </span><br><span class="line">                    currentNode.parent.rightChild = currentNode.rightChild </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    currentNode.replaceNodeData(currentNode.rightChild.key, </span><br><span class="line">                                            currentNode.rightChild.payload, </span><br><span class="line">                                            currentNode.rightChild.leftChild, </span><br><span class="line">                                            currentNode.rightChild.rightChild) </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>如果root是中位数，那就是比较平衡的，插入顺序影响很大。</p>
<h3 id="平衡二叉查找树-avl树">平衡二叉查找树： AVL树</h3>
<p>key插入的时候一直保持平衡，避免上述情况<br>
搜索时间复杂度为O（logn）<br>
左节点+1，右节点-1<br>
AVL树旋转的过程是核心</p>
<p>具体解析可以参见：《python数据结构与算法分析》第六章-平衡二叉搜索树</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BinarySearchTree</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): </span><br><span class="line">        self.root = <span class="literal">None</span> </span><br><span class="line">        self.size = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">length</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.size </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.size </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="comment"># for 循环</span></span><br><span class="line">        <span class="keyword">return</span> self.root.__iter__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">put</span>(<span class="params">self, key, val</span>): </span><br><span class="line">        <span class="keyword">if</span> self.root: </span><br><span class="line">            self._put(key, val, self.root) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            self.root = TreeNode(key, val) </span><br><span class="line">            self.size = self.size + <span class="number">1</span> </span><br><span class="line">    <span class="comment"># def _put(self, key, val, currentNode): </span></span><br><span class="line">    <span class="comment">#     if key &lt; currentNode.key: </span></span><br><span class="line">    <span class="comment">#         if currentNode.hasLeftChild(): </span></span><br><span class="line">    <span class="comment">#             self._put(key, val, currentNode.leftChild) </span></span><br><span class="line">    <span class="comment">#         else: </span></span><br><span class="line">    <span class="comment">#             currentNode.leftChild = TreeNode(key, val, parent=currentNode) </span></span><br><span class="line">    <span class="comment">#     else: </span></span><br><span class="line">    <span class="comment">#         if currentNode.hasRightChild(): </span></span><br><span class="line">    <span class="comment">#             self._put(key, val, currentNode.rightChild) </span></span><br><span class="line">    <span class="comment">#         else: </span></span><br><span class="line">    <span class="comment">#             currentNode.rightChild = TreeNode(key, val, parent=currentNode)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setitem__</span>(<span class="params">self, k, v</span>): </span><br><span class="line">        <span class="comment"># m[k] = v</span></span><br><span class="line">        self.put(k, v)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, key</span>): </span><br><span class="line">        <span class="keyword">if</span> self.root: </span><br><span class="line">            res = self._get(key, self.root) </span><br><span class="line">            <span class="keyword">if</span> res: </span><br><span class="line">                <span class="keyword">return</span> res.payload </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span> </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get</span>(<span class="params">self, key, currentNode</span>): </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> currentNode: </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span> </span><br><span class="line">        <span class="keyword">elif</span> currentNode.key == key: </span><br><span class="line">            <span class="keyword">return</span> currentNode </span><br><span class="line">        <span class="keyword">elif</span> key &lt; currentNode.key: </span><br><span class="line">            <span class="keyword">return</span> self._get(key, currentNode.leftChild)</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> self._get(key, currentNode.rightChild) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, key</span>): </span><br><span class="line">        <span class="keyword">return</span> self.get(key)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__contains__</span>(<span class="params">self, key</span>): </span><br><span class="line">        <span class="comment"># k in m</span></span><br><span class="line">        <span class="keyword">if</span> self._get(key, self.root): </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span> </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delete</span>(<span class="params">self, key</span>): </span><br><span class="line">        <span class="keyword">if</span> self.size &gt; <span class="number">1</span>: </span><br><span class="line">            nodeToRemove = self._get(key, self.root) </span><br><span class="line">            <span class="keyword">if</span> nodeToRemove: </span><br><span class="line">                self.remove(nodeToRemove) </span><br><span class="line">                self.size = self.size - <span class="number">1</span> </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">raise</span> KeyError(<span class="string">'Error, key not in tree'</span>) </span><br><span class="line">        <span class="keyword">elif</span> self.size == <span class="number">1</span> <span class="keyword">and</span> self.root.key == key: </span><br><span class="line">            self.root = <span class="literal">None</span> </span><br><span class="line">            self.size = self.size - <span class="number">1</span> </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">'Error, key not in tree'</span>) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__delitem__</span>(<span class="params">self, key</span>): </span><br><span class="line">        self.delete(key)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">remove</span>(<span class="params">self, currentNode</span>): </span><br><span class="line">        <span class="keyword">if</span> currentNode.isLeaf(): <span class="comment"># 叶子节点</span></span><br><span class="line">            <span class="keyword">if</span> currentNode == currentNode.parent.leftChild: </span><br><span class="line">                currentNode.parent.leftChild = <span class="literal">None</span> </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                currentNode.parent.rightChild = <span class="literal">None</span> </span><br><span class="line">        <span class="keyword">elif</span> currentNode.hasBothChildren(): <span class="comment"># 内部 </span></span><br><span class="line">            succ = currentNode.findSuccessor() </span><br><span class="line">            succ.spliceOut() </span><br><span class="line">            currentNode.key = succ.key </span><br><span class="line">            currentNode.payload = succ.payload </span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 只有一个子节点</span></span><br><span class="line">            <span class="keyword">if</span> currentNode.hasLeftChild(): </span><br><span class="line">                <span class="keyword">if</span> currentNode.isLeftChild(): </span><br><span class="line">                    currentNode.leftChild.parent = currentNode.parent </span><br><span class="line">                    currentNode.parent.leftChild = currentNode.leftChild </span><br><span class="line">                <span class="keyword">elif</span> currentNode.isRightChild(): </span><br><span class="line">                    currentNode.leftChild.parent = currentNode.parent </span><br><span class="line">                    currentNode.parent.rightChild = currentNode.leftChild </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    currentNode.replaceNodeData(currentNode.leftChild.key, </span><br><span class="line">                                            currentNode.leftChild.payload, </span><br><span class="line">                                            currentNode.leftChild.leftChild, </span><br><span class="line">                                            currentNode.leftChild.rightChild) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                <span class="keyword">if</span> currentNode.isLeftChild(): </span><br><span class="line">                    currentNode.rightChild.parent = currentNode.parent </span><br><span class="line">                    currentNode.parent.leftChild = currentNode.rightChild </span><br><span class="line">                <span class="keyword">elif</span> currentNode.isRightChild(): </span><br><span class="line">                    currentNode.rightChild.parent = currentNode.parent </span><br><span class="line">                    currentNode.parent.rightChild = currentNode.rightChild </span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    currentNode.replaceNodeData(currentNode.rightChild.key, </span><br><span class="line">                                            currentNode.rightChild.payload, </span><br><span class="line">                                            currentNode.rightChild.leftChild, </span><br><span class="line">                                            currentNode.rightChild.rightChild)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_put</span>(<span class="params">self, key, val, currentNode</span>): </span><br><span class="line">        <span class="keyword">if</span> key &lt; currentNode.key: </span><br><span class="line">            <span class="keyword">if</span> currentNode.hasLeftChild(): </span><br><span class="line">                self._put(key, val, currentNode.leftChild) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                currentNode.leftChild = TreeNode(key, val, parent=currentNode) </span><br><span class="line">                self.updateBalance(currentNode.leftChild) </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">if</span> currentNode.hasRightChild(): </span><br><span class="line">                self._put(key, val, currentNode.rightChild) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                currentNode.rightChild = TreeNode(key, val, parent=currentNode) </span><br><span class="line">                self.updateBalance(currentNode.rightChild) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">updateBalance</span>(<span class="params">self, node</span>): </span><br><span class="line">        <span class="keyword">if</span> node.balanceFactor &gt; <span class="number">1</span> <span class="keyword">or</span> node.balanceFactor &lt; -<span class="number">1</span>: </span><br><span class="line">            self.rebalance(node) </span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line">        <span class="keyword">if</span> node.parent != <span class="literal">None</span>: </span><br><span class="line">            <span class="keyword">if</span> node.isLeftChild(): </span><br><span class="line">                node.parent.balanceFactor += <span class="number">1</span> </span><br><span class="line">            <span class="keyword">elif</span> node.isRightChild(): </span><br><span class="line">                node.parent.balanceFactor -= <span class="number">1</span> </span><br><span class="line">            <span class="keyword">if</span> node.parent.balanceFactor != <span class="number">0</span>: </span><br><span class="line">                self.updateBalance(node.parent)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rotateLeft</span>(<span class="params">self, rotRoot</span>): </span><br><span class="line">        newRoot = rotRoot.rightChild </span><br><span class="line">        rotRoot.rightChild = newRoot.leftChild </span><br><span class="line">        <span class="keyword">if</span> newRoot.leftChild != <span class="literal">None</span>: </span><br><span class="line">            newRoot.leftChild.parent = rotRoot </span><br><span class="line">        newRoot.parent = rotRoot.parent </span><br><span class="line">        <span class="keyword">if</span> rotRoot.isRoot(): </span><br><span class="line">            self.root = newRoot </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">if</span> rotRoot.isLeftChild(): </span><br><span class="line">                rotRoot.parent.leftChild = newRoot </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                rotRoot.parent.rightChild = newRoot </span><br><span class="line">        newRoot.leftChild = rotRoot </span><br><span class="line">        rotRoot.parent = newRoot </span><br><span class="line">        rotRoot.balanceFactor = rotRoot.balanceFactor + <span class="number">1</span> - <span class="built_in">min</span>(newRoot.balanceFactor, <span class="number">0</span>) </span><br><span class="line">        newRoot.balanceFactor = newRoot.balanceFactor + <span class="number">1</span> + <span class="built_in">max</span>(rotRoot.balanceFactor, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rebalance</span>(<span class="params">self, node</span>): </span><br><span class="line">        <span class="keyword">if</span> node.balanceFactor &lt; <span class="number">0</span>: </span><br><span class="line">            <span class="keyword">if</span> node.rightChild.balanceFactor &gt; <span class="number">0</span>: </span><br><span class="line">                self.rotateRight(node.rightChild) </span><br><span class="line">                self.rotateLeft(node) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                self.rotateLeft(node) </span><br><span class="line">        <span class="keyword">elif</span> node.balanceFactor &gt; <span class="number">0</span>: </span><br><span class="line">            <span class="keyword">if</span> node.leftChild.balanceFactor &lt; <span class="number">0</span>: </span><br><span class="line">                self.rotateLeft(node.leftChild) </span><br><span class="line">                self.rotateRight(node) </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                self.rotateRight(node)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="图">图</h2>
<p>顶点vertex，边edge<br>
权重：给边赋权<br>
G = (V, E) 有向图，无向图<br>
有向无圈图：directed acyclic graph:DAG</p>
<p>ADT Graph两种实现方式：</p>
<ul>
<li>邻接矩阵adjacency matrix
<ul>
<li>无权边标注为1或0，有权边标注为相应权重</li>
</ul></li>
<li>邻接表adjacency list
<ul>
<li>类似字典</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vertex</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key</span>): </span><br><span class="line">        self.<span class="built_in">id</span> = key </span><br><span class="line">        self.connectedTo = {} </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addNeighbor</span>(<span class="params">self, nbr, weight=<span class="number">0</span></span>): </span><br><span class="line">        self.connectedTo[nbr] = weight </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(self.<span class="built_in">id</span>) + <span class="string">' connectedTo: '</span> + <span class="built_in">str</span>([x.<span class="built_in">id</span> <span class="keyword">for</span> x <span class="keyword">in</span> self.connectedTo])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getConnections</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.connectedTo.keys() </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getId</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">id</span> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getWeight</span>(<span class="params">self, nbr</span>): </span><br><span class="line">        <span class="keyword">return</span> self.connectedTo[nbr]</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Graph</span>: </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): </span><br><span class="line">        self.vertList = {} </span><br><span class="line">        self.numVertices = <span class="number">0</span> </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addVertex</span>(<span class="params">self, key</span>): </span><br><span class="line">        self.numVertices = self.numVertices + <span class="number">1</span> </span><br><span class="line">        newVertex = Vertex(key) </span><br><span class="line">        self.vertList[key] = newVertex </span><br><span class="line">        <span class="keyword">return</span> newVertex </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getVertex</span>(<span class="params">self, n</span>): </span><br><span class="line">        <span class="keyword">if</span> n <span class="keyword">in</span> self.vertList: </span><br><span class="line">            <span class="keyword">return</span> self.vertList[n] </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span> </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__contains__</span>(<span class="params">self, n</span>): </span><br><span class="line">        <span class="keyword">return</span> n <span class="keyword">in</span> self.vertList </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addEdge</span>(<span class="params">self, f, t, cost=<span class="number">0</span></span>): </span><br><span class="line">        <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> self.vertList: </span><br><span class="line">            nv = self.addVertex(f) </span><br><span class="line">        <span class="keyword">if</span> t <span class="keyword">not</span> <span class="keyword">in</span> self.vertList: </span><br><span class="line">            nv = self.addVertex(t) </span><br><span class="line">        self.vertList[f].addNeighbor(self.vertList[t], cost) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getVertices</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> self.vertList.keys() </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(self.vertList.values())</span><br></pre></td></tr></tbody></table></figure>
<h3 id="广度优先搜索ove">广度优先搜索:O(V+E)</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pythonds.graphs <span class="keyword">import</span> Graph, Vertex </span><br><span class="line"><span class="keyword">from</span> pythonds.basic <span class="keyword">import</span> Queue </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">g, start</span>): </span><br><span class="line">    start.setDistance(<span class="number">0</span>) </span><br><span class="line">    start.setPred(<span class="literal">None</span>) </span><br><span class="line">    vertQueue = Queue() </span><br><span class="line">    vertQueue.enqueue(start) </span><br><span class="line">    <span class="keyword">while</span> (vertQueue.size() &gt; <span class="number">0</span>): </span><br><span class="line">        currentVert = vertQueue.dequeue() </span><br><span class="line">        <span class="keyword">for</span> nbr <span class="keyword">in</span> currentVert.getConnections(): </span><br><span class="line">            <span class="keyword">if</span> (nbr.getColor() == <span class="string">'white'</span>): </span><br><span class="line">                nbr.setColor(<span class="string">'gray'</span>) </span><br><span class="line">                nbr.setDistance(currentVert.getDistance() + <span class="number">1</span>) </span><br><span class="line">                nbr.setPred(currentVert) </span><br><span class="line">                vertQueue.enqueue(nbr) </span><br><span class="line">        currentVert.setColor(<span class="string">'black'</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 回溯BFS</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">traverse</span>(<span class="params">y</span>): </span><br><span class="line">    x = y </span><br><span class="line">    <span class="keyword">while</span> (x.getPred()): </span><br><span class="line">        <span class="built_in">print</span>(x.getId()) </span><br><span class="line">        x = x.getPred() </span><br><span class="line">        <span class="built_in">print</span>(x.getId()) </span><br><span class="line"></span><br><span class="line"><span class="comment"># traverse(g.getVertex('sage'))</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="深度优先搜索ove">深度优先搜索：O（V+E）</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pythonds.graphs <span class="keyword">import</span> Graph </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DFSGraph</span>(<span class="title class_ inherited__">Graph</span>): </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="built_in">super</span>(). __init__() </span><br><span class="line">        self.time = <span class="number">0</span> </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="keyword">for</span> aVertex <span class="keyword">in</span> self: </span><br><span class="line">            aVertex.setColor(<span class="string">'white'</span>) </span><br><span class="line">            aVertex.setPred(-<span class="number">1</span>) </span><br><span class="line">        <span class="keyword">for</span> aVertex <span class="keyword">in</span> self: </span><br><span class="line">            <span class="keyword">if</span> aVertex.getColor() == <span class="string">'white'</span>: </span><br><span class="line">                self.dfsvisit(aVertex) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfsvisit</span>(<span class="params">self, startVertex</span>): </span><br><span class="line">        startVertex.setColor(<span class="string">'gray'</span>) </span><br><span class="line">        self.time += <span class="number">1</span> </span><br><span class="line">        startVertex.setDiscovery(self.time) </span><br><span class="line">        <span class="keyword">for</span> nextVertex <span class="keyword">in</span> startVertex.getConnections(): </span><br><span class="line">            <span class="keyword">if</span> nextVertex.getColor() == <span class="string">'white'</span>: </span><br><span class="line">                nextVertex.setPred(startVertex) </span><br><span class="line">                self.dfsvisit(nextVertex) </span><br><span class="line">        startVertex.setColor(<span class="string">'black'</span>) </span><br><span class="line">        self.time += <span class="number">1</span> </span><br><span class="line">        startVertex.setFinish(self.time)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="图的应用拓扑排序">图的应用：拓扑排序</h3>
<p>拓扑排序根据有向无环图生成一个包含所有顶点的线性序列，使得如果图 G
中有一条边为(v, w)，那么顶点 v 排在顶点 w
之前。在很多应用中，有向无环图被用于表明事件优先级。</p>
<h3 id="图的应用强连通分支">图的应用：强连通分支</h3>
<p>C中的任意两个顶点之间都有路径来回，这样的最大的子集就是强连通分支<br>
强连通分支的矩阵和转置矩阵相同</p>
<ul>
<li>kosaraju算法:
先用dfs得到每个顶点结束时间，然后按照结束时间最大的作为顶点，在转置图中用dfs，这样得到的所有顶点链接树就是强连通分支</li>
<li>Tarjan算法</li>
<li>Gabow算法（性能最高）</li>
</ul>
<h3 id="图的应用最短路径">图的应用：最短路径</h3>
<ul>
<li>Dijkstra算法：（只能处理大于0权重的图）O((V+E)logV)
<ul>
<li>利用优先队列：所有顶点建堆，然后出队优先点，更新邻接点的权值，并在优先队列中重排，直到优先队列中没有点</li>
<li>距离向量路由算法</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pythonds.graphs <span class="keyword">import</span> PriorityQueue, Graph, Vertex </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dijkstra</span>(<span class="params">aGraph, start</span>): </span><br><span class="line">    pq = PriorityQueue() </span><br><span class="line">    start.setDistance(<span class="number">0</span>) </span><br><span class="line">    pq.buildHeap([(v.getDistance(), v) <span class="keyword">for</span> v <span class="keyword">in</span> aGraph]) </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> pq.isEmpty(): </span><br><span class="line">        currentVert = pq.delMin() </span><br><span class="line">        <span class="keyword">for</span> nextVert <span class="keyword">in</span> currentVert.getConnections(): </span><br><span class="line">            newDist = currentVert.getDistance() + currentVert.getWeight(nextVert) </span><br><span class="line">        <span class="keyword">if</span> newDist &lt; nextVert.getDistance(): </span><br><span class="line">            nextVert.setDistance(newDist) </span><br><span class="line">            nextVert.setPred(currentVert) </span><br><span class="line">            pq.decreaseKey(nextVert, newDist) <span class="comment"># O(ElogV)</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="图的应用最小生成树minimum-weight-spanning-tree">图的应用：最小生成树(minimum
weight spanning tree)</h3>
<ul>
<li>拥有图中所有顶点和最小数量的边（权重和最小）以保持联通的子图（无圈）</li>
<li>prim算法（和dijkstra算法相似，只不过要采取贪心策略，加入所有的顶点）</li>
</ul>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Tree Traverse</title>
    <url>/Tree-Traverse/</url>
    <content><![CDATA[<h1 id="迭代法">迭代法</h1>
<p>Leetcode相关题目：94，144，145</p>
<h2 id="前序遍历">前序遍历</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preorderTraversal</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        result = []</span><br><span class="line">        st= []</span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            st.append(root)</span><br><span class="line">        <span class="keyword">while</span> st:</span><br><span class="line">            node = st.pop()</span><br><span class="line">            <span class="keyword">if</span> node != <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> node.right: <span class="comment">#右</span></span><br><span class="line">                    st.append(node.right)</span><br><span class="line">                <span class="keyword">if</span> node.left: <span class="comment">#左</span></span><br><span class="line">                    st.append(node.left)</span><br><span class="line">                st.append(node) <span class="comment">#中</span></span><br><span class="line">                st.append(<span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                node = st.pop()</span><br><span class="line">                result.append(node.val)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure>
<h2 id="中序遍历">中序遍历</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inorderTraversal</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        result = []</span><br><span class="line">        st = []</span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            st.append(root)</span><br><span class="line">        <span class="keyword">while</span> st:</span><br><span class="line">            node = st.pop()</span><br><span class="line">            <span class="keyword">if</span> node != <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> node.right: <span class="comment">#添加右节点（空节点不入栈）</span></span><br><span class="line">                    st.append(node.right)</span><br><span class="line">                </span><br><span class="line">                st.append(node) <span class="comment">#添加中节点</span></span><br><span class="line">                st.append(<span class="literal">None</span>) <span class="comment">#中节点访问过，但是还没有处理，加入空节点做为标记。</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> node.left: <span class="comment">#添加左节点（空节点不入栈）</span></span><br><span class="line">                    st.append(node.left)</span><br><span class="line">            <span class="keyword">else</span>: <span class="comment">#只有遇到空节点的时候，才将下一个节点放进结果集</span></span><br><span class="line">                node = st.pop() <span class="comment">#重新取出栈中元素</span></span><br><span class="line">                result.append(node.val) <span class="comment">#加入到结果集</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure>
<h2 id="后序遍历">后序遍历</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">postorderTraversal</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        result = []</span><br><span class="line">        st = []</span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            st.append(root)</span><br><span class="line">        <span class="keyword">while</span> st:</span><br><span class="line">            node = st.pop()</span><br><span class="line">            <span class="keyword">if</span> node != <span class="literal">None</span>:</span><br><span class="line">                st.append(node) <span class="comment">#中</span></span><br><span class="line">                st.append(<span class="literal">None</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> node.right: <span class="comment">#右</span></span><br><span class="line">                    st.append(node.right)</span><br><span class="line">                <span class="keyword">if</span> node.left: <span class="comment">#左</span></span><br><span class="line">                    st.append(node.left)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                node = st.pop()</span><br><span class="line">                result.append(node.val)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure>
<h1 id="递归法">递归法</h1>
<h2 id="前序遍历-1">前序遍历</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preorderTraversal</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 保存结果</span></span><br><span class="line">        result = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">traversal</span>(<span class="params">root: TreeNode</span>):</span><br><span class="line">            <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            result.append(root.val) <span class="comment"># 前序</span></span><br><span class="line">            traversal(root.left)    <span class="comment"># 左</span></span><br><span class="line">            traversal(root.right)   <span class="comment"># 右</span></span><br><span class="line"></span><br><span class="line">        traversal(root)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure>
<h2 id="中序遍历-1">中序遍历</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inorderTraversal</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        result = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">traversal</span>(<span class="params">root: TreeNode</span>):</span><br><span class="line">            <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            traversal(root.left)    <span class="comment"># 左</span></span><br><span class="line">            result.append(root.val) <span class="comment"># 中序</span></span><br><span class="line">            traversal(root.right)   <span class="comment"># 右</span></span><br><span class="line"></span><br><span class="line">        traversal(root)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure>
<h2 id="后序遍历-1">后序遍历</h2>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">postorderTraversal</span>(<span class="params">self, root: TreeNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        result = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">traversal</span>(<span class="params">root: TreeNode</span>):</span><br><span class="line">            <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            traversal(root.left)    <span class="comment"># 左</span></span><br><span class="line">            traversal(root.right)   <span class="comment"># 右</span></span><br><span class="line">            result.append(root.val) <span class="comment"># 后序</span></span><br><span class="line"></span><br><span class="line">        traversal(root)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
</search>
